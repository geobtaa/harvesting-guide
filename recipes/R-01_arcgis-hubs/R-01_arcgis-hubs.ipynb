{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a446268",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the [DCAT 1.1 API](https://resources.data.gov/resources/dcat-us/) of ArcGIS Hubs and return the metadata for all suitable items as a CSV file in the GeoBTAA Metadata Application Profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255ad2a",
   "metadata": {},
   "source": [
    "## View full recipe\n",
    "\n",
    "This Notebook is part of a workflow documented in the [Metadata Handbook Recipes section](https://geobtaa.github.io/metadata/recipes). This is the first recipe, called [ArcGIS](https://geobtaa.github.io/metadata/recipes/R-01_arcgis-hubs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7df424",
   "metadata": {},
   "source": [
    "## Prepare the list of active ArcGIS Hubs\n",
    "\n",
    "We maintain a list of active ArcGIS Hub sites in GEOMG. (Access to GEOMG requires a login account. External users can create their own list or use one provided in this repository)\n",
    "\n",
    "1. Filter for items with these parameters:\n",
    "   - Resource Class: Websites\n",
    "   - Accrual Method: DCAT US 1.1\n",
    "   - [Shortcut query](https://geomg.lib.umn.edu/documents?f%5Bb1g_dct_accrualMethod_s%5D%5B%5D=DCAT+US+1.1&f%5Bgbl_resourceClass_sm%5D%5B%5D=Websites&rows=20&sort=score+desc)\n",
    "   \n",
    "2. Rename the downloaded file `arcHubs.csv` and move it into the same directory as this Notebook.\n",
    "\n",
    "\n",
    "    \n",
    "Exporting from GEOMG will produce a CSV containing all of the metadata associated with each Hub. For this script, the only fields used are:\n",
    "\n",
    "* **ID**: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.\n",
    "* **Title**: The name of the Hub. This is transferred to the \"Provider\" field for each dataset\n",
    "* **Publisher**: The place or administration associated with the portal. This is applied to the title in each dataset in brackets\n",
    "* **Spatial Coverage**: A list of place names. These are transferred to the Spatial Coverage for each dataset\n",
    "* **Member Of**: a larger collection level record. Most of the Hubs are either part of our [Government Open Geospatial Data Collection](https://geo.btaa.org/catalog/ba5cc745-21c5-4ae9-954b-72dd8db6815a) or the [Research Institutes Geospatial Data Collection](https://geo.btaa.org/catalog/b0153110-e455-4ced-9114-9b13250a7093)\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8a6b2",
   "metadata": {},
   "source": [
    "## Define the module-level code\n",
    "\n",
    "This section includes the necessary imports, configuration settings, and function/class definitions that will be used by the rest of the code in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff38b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # Provides functionality to read from and write to CSV files.\n",
    "import json # Provides functionality to work with JSON data.\n",
    "import os # Provides a way of using operating system dependent functionality, like reading or writing the file system.\n",
    "import re # Provides regular expression matching operations.\n",
    "import time # Provides functions for working with time, including time conversion, sleep function and timers.\n",
    "import urllib.request # provides functions for working with URLs, like opening URLs, reading data from URLs, etc.\n",
    "from html.parser import HTMLParser # provides an HTML parsing library that can be used to extract data from HTML docs.\n",
    "from urllib.parse import urlparse, parse_qs # provides a way to parse URLs into their components.\n",
    "\n",
    "import numpy as np # Provides numerical operations and array manipulation tools.\n",
    "import pandas as pd # Provides data manipulation and analysis functionality.\n",
    "import requests # Provides HTTP library for sending requests to servers and receiving responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274c4a",
   "metadata": {},
   "source": [
    "**Set up paths and output CSV field names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38c9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \".\"  # Set to directory containing arcHubs.csv\n",
    "hubFile = \"arcHubs.csv\"  # the name of the CSV file with the list of ArcGIS Hubs\n",
    "fieldnames = [  # DCAT schema fields to be included in report\n",
    "    \"Title\",\n",
    "    \"Alternative Title\",\n",
    "    \"Description\",\n",
    "    \"Language\",\n",
    "    \"Creator\",\n",
    "    \"Title Source\",\n",
    "    \"Resource Class\",\n",
    "    \"Resource Type\",\n",
    "    \"Keyword\",\n",
    "    \"Date Issued\",\n",
    "    \"Temporal Coverage\",\n",
    "    \"Date Range\",\n",
    "    \"Spatial Coverage\",\n",
    "    \"Bounding Box\",\n",
    "    \"Format\",\n",
    "    \"Information\",\n",
    "    \"Download\",\n",
    "    \"MapServer\",\n",
    "    \"FeatureServer\",\n",
    "    \"ImageServer\",\n",
    "    \"ID\",\n",
    "    \"Identifier\",\n",
    "    \"Provider\",\n",
    "    \"Code\",\n",
    "    \"Member Of\",\n",
    "    \"Is Part Of\",\n",
    "    \"Rights\",\n",
    "    \"Accrual Method\",\n",
    "    \"Date Accessioned\",\n",
    "    \"Access Rights\",\n",
    "]\n",
    "\n",
    "ActionDate = time.strftime('%Y%m%d') # Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ba35c",
   "metadata": {},
   "source": [
    "**Function to remove HTML tags**\n",
    "\n",
    "Sometimes, the metadata fields we scrape contain HTML tags, such as links or formatting that do not work in the Geoportal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7616ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser): \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d): \n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self): # Returns a string of all the data in the list concatenated together.\n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html): # Defined by the MLS Stripper \n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value): # Calls strip_tags on the input value to remove any HTML tags present.\n",
    "    return strip_tags(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220f9c9",
   "metadata": {},
   "source": [
    "**Function to generate an output CSV**\n",
    "\n",
    "iterate over the keys and writes the corresponding values to the CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6391c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for hub in dictionary:\n",
    "            for keys in hub:\n",
    "                allvalues = hub[keys]\n",
    "                csvout.writerow(allvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba373d6",
   "metadata": {},
   "source": [
    "**Function to create a dictionary of metadata in the JSONs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3391b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the len function to get the number of datasets and the range function to loop through each dataset\n",
    "        \n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}  # Dictionary List\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a3aea",
   "metadata": {},
   "source": [
    "**Function to generate the title as: alternativeTitle [place name] {year}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1908624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function uses regular expressions to extract the year from the alternative title, and replaces it with an empty string to remove it from the title.\n",
    "\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    # find if year exist in alternativeTitle\n",
    "    year = ''\n",
    "    try:  \n",
    "      year_range = re.findall(r'(\\d{4})-(\\d{4})', alternativeTitle)\n",
    "    except:\n",
    "      year_range = ''\n",
    "    try: \n",
    "      single_year = re.match(r'.*(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})', alternativeTitle)\n",
    "    except:\n",
    "      single_year = ''    \n",
    "    if year_range:   # if a 'yyyy-yyyy' exists\n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  # or if a 'yyyy' exists\n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "     \n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'       \n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9df9",
   "metadata": {},
   "source": [
    "**Function to create a dictionary of scanned metadata**\n",
    "\n",
    "This code defines a function called `metadataNewItems()` which takes two arguments \n",
    "\n",
    "* `newdata` (a dictionary containing metadata information about new items)\n",
    "* `newitem_ids` (a dictionary containing information about the new items such as the position and the landing page URLs).\n",
    "\n",
    "The function processes the metadata information for each new item and creates a dictionary containing the formatted metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb10c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes blank fields '' for some columns\n",
    "\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    # y = position of the dataset in the DCAT metadata json, v = landing page URLs\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "        \n",
    "\n",
    "#ALTERNATIVE TITLE\n",
    "       \n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = str(cleanData(newdata[\"dataset\"][y]['title']))\n",
    "        except:\n",
    "            alternativeTitle = str(newdata[\"dataset\"][y]['title'])\n",
    "            \n",
    "# TITLE\n",
    "            \n",
    "        # call the format_title function\n",
    "#         title = format_title(alternativeTitle, titleSource)\n",
    "        title = alternativeTitle\n",
    "            \n",
    "#DESCRIPTION\n",
    "\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        description = description.replace(\"{{default.description}}\", \"\").replace(\"{{description}}\", \"\")\n",
    "        description = re.sub(r'[\\n]+|[\\r\\n]+', ' ', description, flags=re.S)\n",
    "        description = re.sub(r'\\s{2,}', ' ', description)\n",
    "        description = description.translate({8217: \"'\", 8220: '\"', 8221: '\"', 160: \"\", 183: \"\", 8226: \"\", 8211: \"-\", 8203: \"\"})\n",
    "\n",
    "\n",
    "# RESOURCE TYPE\n",
    "\n",
    "        # if 'LiDAR' exists in Title or Description, add it to Resource Type\n",
    "        if 'LiDAR' in title or 'LiDAR' in description:\n",
    "            resourceType = 'LiDAR'\n",
    "                            \n",
    "#CREATOR\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "\n",
    "\n",
    "# DISTRIBUTION\n",
    "\n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "\n",
    "        format_types = []\n",
    "        resourceClass = \"\"\n",
    "        formatElement = \"\"\n",
    "        downloadURL = \"\"\n",
    "        resourceType = \"\"\n",
    "        webService = \"\"\n",
    "        featureServer = \"\"\n",
    "        mapServer = \"\"\n",
    "        imageServer = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                # If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    resourceClass = \"Datasets|Web services\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"].split('?')[0]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"].split('?')[0]\n",
    "\n",
    "                    resourceType = \"Vector data\"\n",
    "\n",
    "                # If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"ArcGIS GeoService\":\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "\n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            resourceClass = \"Imagery|Web services\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            resourceType = \"Satellite imagery\"\n",
    "                    else:\n",
    "                        resourceClass = \"\"\n",
    "                        formatElement = \"\"\n",
    "                        downloadURL = \"\"\n",
    "\n",
    "            # If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                resourceClass = \"\"\n",
    "                formatElement = \"\"\n",
    "                downloadURL = \"\"\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            if \"FeatureServer\" in webService:\n",
    "                featureServer = webService\n",
    "            if \"MapServer\" in webService:\n",
    "                mapServer = webService\n",
    "            if \"ImageServer\" in webService:\n",
    "                imageServer = webService\n",
    "        except:\n",
    "            print(identifier)\n",
    "\n",
    "\n",
    "\n",
    "# BOUNDING BOX\n",
    "        \n",
    "        bbox = newdata[\"dataset\"][y][\"spatial\"]                \n",
    "    \n",
    "        try:\n",
    "            bboxList = []\n",
    "            bbox = ''\n",
    "            spatial = cleanData(newdata[\"dataset\"][y]['spatial'])\n",
    "            typeDmal = decimal.Decimal\n",
    "            fix4 = typeDmal(\"0.01\")\n",
    "            for coord in spatial.split(\",\"):\n",
    "                coordFix = typeDmal(coord).quantize(fix4)\n",
    "                bboxList.append(str(coordFix))\n",
    "            bbox = ','.join(bboxList)\n",
    "        except:\n",
    "            spatial = \"\"\n",
    "            \n",
    "# KEYWORDS\n",
    "\n",
    "        keyword = newdata[\"dataset\"][y][\"keyword\"]\n",
    "        keyword_list = []\n",
    "        keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "\n",
    "        \n",
    "# DATES\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "\n",
    "        # auto-generate Temporal Coverage and Date Range\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     # if title has {YYYY} or {YYYY-YYYY}\n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            dateRange = temporalCoverage[:4] + '-' + temporalCoverage[-4:]\n",
    "        else:\n",
    "            temporalCoverage = 'Continually updated resource'\n",
    "        \n",
    "#RIGHTS\n",
    "\n",
    "        rights = cleanData(newdata[\"dataset\"][y]['license']) if 'license' in newdata[\"dataset\"][y] else \"\"\n",
    "\n",
    "\n",
    "# IDENTIFIER\n",
    "        slug = identifier.split('=', 1)[-1].replace(\"&sublayer=\", \"_\")\n",
    "        querystring = parse_qs(urlparse(identifier).query)\n",
    "        identifier_new = \"https://hub.arcgis.com/datasets/\" + \"\" + querystring[\"id\"][0]\n",
    "\n",
    "            \n",
    "# Define full metadata list\n",
    "\n",
    "        metadataList = [\n",
    "            title, \n",
    "            alternativeTitle, \n",
    "            description, \n",
    "            language, \n",
    "            creator,\n",
    "            titleSource,\n",
    "            resourceClass, \n",
    "            resourceType, \n",
    "            keyword_list, \n",
    "            dateIssued, \n",
    "            temporalCoverage,\n",
    "            dateRange, \n",
    "            spatialCoverage, \n",
    "            bbox,\n",
    "            formatElement, \n",
    "            information, \n",
    "            downloadURL, \n",
    "            mapServer, \n",
    "            featureServer,\n",
    "            imageServer, \n",
    "            slug, \n",
    "            identifier_new, \n",
    "            provider, \n",
    "            hubCode, \n",
    "            memberOf, \n",
    "            isPartOf, \n",
    "            rights,\n",
    "            accrualMethod,\n",
    "            dateAccessioned, \n",
    "            accessRights\n",
    "        ]     \n",
    "\n",
    "        # deletes items where the resourceClass is empty\n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[6] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[slug] = metadata\n",
    "\n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef627b2b",
   "metadata": {},
   "source": [
    "## Run the executable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9c0be",
   "metadata": {},
   "source": [
    "**Declare a list to hold the scanned metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2ba8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRecords = []\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1069f4",
   "metadata": {},
   "source": [
    "**Scan the metadata for each Hub**\n",
    "\n",
    "This code reads data from `hubFile.csv` using the `csv.DictReader` function. It then iterates over each row in the file and extracts values from specific columns to be used later in the script.\n",
    "\n",
    "For each row, the script also defines default values for a set of metadata fields. It then checks if the URL provided in the CSV file exists and is a valid JSON response. If the response is not valid, the script prints an error message and continues to the next row. Otherwise, it extracts dataset identifiers from the JSON response and passes the response along with the identifiers to a function called metadataNewItems. The metadata for each row is then appended to a list called `allRecords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "908a93c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning  08b-42003 https://openac-alcogis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04b-24003 https://maps.aacounty.org//api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55003 https://data-ashlandcountywi.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39009 https://data-athgis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04b-24005 https://opendata.baltimorecountymd.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27011 https://data-bigstonecounty.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04b-24009 https://calvert-county-open-data-calvertgis.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55025-01 https://data-carpc.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04b-24013 https://data-carrollco-md.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27019 http://data-carver.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08b-42027 http://gisdata-centrecountygov.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08b-42029 https://chester-county-s-gis-hub-chesco.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27023 https://data-chippewa.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  13c-02 https://data-cityofgi.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10c-03 https://data-cityofmadison.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05c-02 https://information.stpaul.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27027 https://data-claycountymn.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  12b-17031 https://hub-cookcountyil.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39035 https://data-cuyahoga.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08b-42043 https://data-dauphinco.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39041 https://gisdata-delco.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04f-01 https://dvrpc-dvrpcgis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  07c-01 https://data.detroitmi.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55025 https://gis-countyofdane.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  12b-17043 https://gisdata-dupage.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55035-01 https://hub-eccounty.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  01b-18163 https://dev-evansvilleapc.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39049 https://auditor-fca.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04b-24021 https://gis-fcgmd.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55125-01 https://data-vilas.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11a-01 https://ogrip-geohio.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27051 https://hub-co-grant-mn-us.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04b-24025 https://harford-county-gis-hub-harfordgis.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27053 https://gis-hennepin.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  1000-0001 https://hudgis-hud.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  09a-04 https://www.indianamap.org/api/feed/dcat-us/1.1.json\n",
      "scanning  03a-03 https://geodata.iowa.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  03a-02 https://public-iowadot.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55059 https://dataportal.kenoshacounty.org/api/feed/dcat-us/1.1.json\n",
      "scanning  12b-17097 https://data-lakecountyil.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55021 https://opendata-cclid.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  13c-01 https://data2-lincolnne.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  03b-19113 http://opendata-linncounty-gis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39093 https://data-loraingis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04a-01 http://data.imap.maryland.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  02b-17113 http://www.mcgis.org//api/feed/dcat-us/1.1.json\n",
      "scanning  06a-02 https://gis-midnr.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55079 https://gis-mclio.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08b-42091 https://data-montcopa.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  13a-04 https://data-outdoornebraska.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  13a-01 https://www.nebraskamap.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  14a-02 https://gisdata-njdep.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  14a-01 https://njogis-newjersey.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  06b-26125 http://accessoakland.oakgov.com/api/feed/dcat-us/1.1.json\n",
      "scanning  13b-31055 https://data-dogis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  04c-01 https://opendata.dc.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  05c-01 https://opendata.minneapolismn.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  01c-02 http://data.indy.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  12c-02 https://opioid-awareness-peoriacountygis.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55087 https://data-ocgis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08a-02 https://newdata-dcnr.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  12b-17143 https://data-peoriacountygis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08c-01-2 http://data-phl.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55093 https://data-piercecounty-wi.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27119 https://hub-pcg.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27121 https://hub-popecounty.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55101 http://data.racinecounty.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27123 https://data-ramseygis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27129 https://hub-renvilleco.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27137 https://open-data-slcgis.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  13b-31153 https://data2-sarpy.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55111 https://data-saukgis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  06f-01 http://maps-semcog.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  09c-01 http://data-southbend.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55109 https://gis-scccdd.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39151 http://opendata.starkcountyohio.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  06a-01 http://gis-michigan.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27145 https://stearns-county-gis-stearns.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39153 https://data-summitgis.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  07d-02 https://mbgna-umich.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  12d-03 http://library-uchicago.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  07b-26161 https://data-washtenaw.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55133 https://data-waukeshacounty.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55135 https://data2017-04-05t135915451z-waupacacounty.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55137 https://data-waushara.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27167 https://hub-wilkinco.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10c-04 https://rapidsdata-wisconsinrapids.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55141 https://opendata.woodcogis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  05b-27173 https://hub-yellowmedicine.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  08b-42133 https://york-county-pa-gis-portal-yorkcountypa.hub.arcgis.com/api/feed/dcat-us/1.1.json\n"
     ]
    }
   ],
   "source": [
    "with open(hubFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Read in values from arcHubs.csv to be used within the script or as part of the metadata report\n",
    "        hubCode = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        isPartOf = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        \n",
    "        # Define default values for each record\n",
    "        accrualMethod = \"ArcGIS Hub\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        language = \"eng\"\n",
    "\n",
    "        print(\"scanning \", hubCode, url)\n",
    "        \n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        # check if the Hub's URL is broken\n",
    "        if response.headers['content-type'] != 'application/json; charset=utf-8':\n",
    "            print(\"\\n--------------------- Data hub URL does not exist --------------------\\n\",\n",
    "                  hubCode, url,  \"\\n--------------------------------------------------------------------------\\n\")\n",
    "            continue\n",
    "        else:\n",
    "            newdata = json.load(response)\n",
    "\n",
    "\n",
    "        # Makes a list of dataset identifiers\n",
    "        newjson_ids = getIdentifiers(newdata)\n",
    "\n",
    "\n",
    "        allRecords.append(metadataNewItems(newdata, newjson_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec019d73",
   "metadata": {},
   "source": [
    "**Write the scanned metadata to a CSV in the GeoBTAA Metadata Profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d787b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "newItemsReport = f\"{directory}/{ActionDate}_scannedRecords.csv\"\n",
    "printItemReport(newItemsReport, fieldnames, allRecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342edef9",
   "metadata": {},
   "source": [
    "**Remove Download links with \"tif.zip\"**\n",
    "\n",
    "A large number of items in the Download column are causing errors in the Geoportal. These items can be identified by the string \"tif.zip\". The following code removes Download links for those items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05237bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe from the file\n",
    "df_newitems = pd.read_csv(newItemsReport)\n",
    "\n",
    "# Replace missing values with an empty string\n",
    "df_newitems['Download'] = df_newitems['Download'].fillna('')\n",
    "\n",
    "# Remove \"tif.zip\" values in the \"Download\" column\n",
    "df_newitems.loc[df_newitems['Download'].str.contains('tif.zip', na=False), 'Download'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7cf85",
   "metadata": {},
   "source": [
    "**Drop duplicate items**\n",
    "\n",
    "ArcGIS Hub administrators can include datasets from other Hubs in their own site. As a result, some datasets are duplicated in other Hubs. However, they always have the same Identifier, so we can use pandas to detect and remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ce2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on the 'ID' column\n",
    "df_finalItems = df_newitems.drop_duplicates(subset=['ID'])\n",
    "\n",
    "# Save the modified dataframe back to the file\n",
    "df_finalItems.to_csv(newItemsReport, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
