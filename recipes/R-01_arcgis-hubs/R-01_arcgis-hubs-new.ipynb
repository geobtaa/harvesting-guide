{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c021fb51",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This script will scan the DCAT 1.1 APIs of ArcGIS Hubs and return the metadata for all suitable items. It will produce two CSV files\n",
    "1. A CSV with all metadata except the link fields\n",
    "2. A CSV with only the link fields (landing page, download, and REST services)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9928ee0",
   "metadata": {},
   "source": [
    "## Prepare the list of active ArcGIS Hubs\n",
    "\n",
    "We maintain a list of active ArcGIS Hub sites in GEOMG. (Access to GEOMG requires a login account. External users can create their own list or use one provided in this repository)\n",
    "\n",
    "1. Filter for items with these parameters:\n",
    "   - Resource Class: Websites\n",
    "   - Accrual Method: DCAT US 1.1\n",
    "   - [Shortcut query](https://geo.btaa.org/admin/documents?f%5Bb1g_dct_accrualMethod_s%5D%5B%5D=DCAT+US+1.1&f%5Bdct_format_s%5D%5B%5D=ArcGIS+Hub&f%5Bgbl_resourceClass_sm%5D%5B%5D=Websites&rows=20&sort=score+desc)\n",
    "   \n",
    "2. Rename the downloaded file `arcHubs.csv` and move it into the same directory as this Notebook.\n",
    "\n",
    "\n",
    "    \n",
    "Exporting from GEOMG will produce a CSV containing all of the metadata associated with each Hub. For this script, the only fields used are:\n",
    "\n",
    "* **ID**: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.\n",
    "* **Title**: The name of the Hub. This is transferred to the \"Provider\" field for each dataset\n",
    "* **Publisher**: The place or administration associated with the portal. This is applied to the title in each dataset in brackets\n",
    "* **Spatial Coverage**: A list of place names. These are transferred to the Spatial Coverage for each dataset\n",
    "* **Member Of**: The ID of a larger collection level record. Most of the Hubs are either part of our [Government Open Geospatial Data Collection](https://geo.btaa.org/catalog/ba5cc745-21c5-4ae9-954b-72dd8db6815a) or the [Research Institutes Geospatial Data Collection](https://geo.btaa.org/catalog/b0153110-e455-4ced-9114-9b13250a7093)\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a88a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \".\"  # Directory containing arcHubs.csv\n",
    "hubFile = \"arcHubs.csv\"  # the name of the CSV file with the list of ArcGIS Hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "ActionDate = time.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = [\n",
    "    \"Title\",\n",
    "    \"Alternative Title\",\n",
    "    \"Description\",\n",
    "    \"Language\",\n",
    "    \"Display Note\",\n",
    "    \"Creator\",\n",
    "    \"Title Source\",\n",
    "    \"Resource Class\",\n",
    "    \"Resource Type\",\n",
    "    \"Keyword\",\n",
    "    \"Date Issued\",\n",
    "    \"Temporal Coverage\",\n",
    "    \"Date Range\",\n",
    "    \"Spatial Coverage\",\n",
    "    \"Bounding Box\",\n",
    "    \"Format\",\n",
    "    \"documentation_external\",\n",
    "    \"download\",\n",
    "    \"arcgis_dynamic_map_layer\",\n",
    "    \"arcgis_feature_layer\",\n",
    "    \"arcgis_image_map_layer\",\n",
    "    \"arcgis_tiled_map_layer\",\n",
    "    \"ID\",\n",
    "    \"Identifier\",\n",
    "    \"Provider\",\n",
    "    \"Code\",\n",
    "    \"Member Of\",\n",
    "    \"Is Part Of\",\n",
    "    \"Rights\",\n",
    "    \"Accrual Method\",\n",
    "    \"Date Accessioned\",\n",
    "    \"Access Rights\",\n",
    "    \"Publication State\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d): \n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self): \n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html): \n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value):\n",
    "    return strip_tags(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f26afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdentifiers(data):\n",
    "    json_ids = {}\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n",
    "\n",
    "\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    year = ''\n",
    "    try:  \n",
    "        # Matches a range of years (yyyy-yyyy)\n",
    "        year_range = re.findall(r'\\b(\\d{4})-(\\d{4})\\b', alternativeTitle)\n",
    "    except:\n",
    "        year_range = ''\n",
    "    try: \n",
    "        # Matches standalone years or years adjacent to letters but not part of a longer sequence\n",
    "        single_year = re.match(r'.*(?<!\\d)(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})(?!\\d)', alternativeTitle)\n",
    "    except:\n",
    "        single_year = ''    \n",
    "\n",
    "    if year_range:   \n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  \n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "\n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadataNewItems(newdata, newitem_ids, language, displayNote, titleSource, spatialCoverage, \n",
    "                     provider, hubCode, memberOf, isPartOf, accrualMethod, dateAccessioned, accessRights, default_bbox):\n",
    "    newItemDict = {}\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "        \n",
    "        # Alternative Title\n",
    "        try:\n",
    "            alternativeTitle = str(cleanData(newdata[\"dataset\"][y]['title']))\n",
    "        except:\n",
    "            alternativeTitle = str(newdata[\"dataset\"][y]['title'])\n",
    "\n",
    "        # Format title\n",
    "        title = format_title(alternativeTitle, titleSource)\n",
    "\n",
    "        # Description\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        description = description.replace(\"{{default.description}}\", \"\").replace(\"{{description}}\", \"\")\n",
    "        description = re.sub(r'[\\n]+|[\\r\\n]+', ' ', description, flags=re.S)\n",
    "        description = re.sub(r'\\s{2,}', ' ', description)\n",
    "        description = description.translate({8217: \"'\", 8220: '\"', 8221: '\"', 160: \"\", 183: \"\", 8226: \"\", 8211: \"-\", 8203: \"\"})\n",
    "\n",
    "        # Creator\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "\n",
    "        # Initialize link-related variables\n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "        downloadURL = \"\"\n",
    "        mapServer = \"\"\n",
    "        featureServer = \"\"\n",
    "        imageServer = \"\"\n",
    "        tileServer = \"\"\n",
    "\n",
    "        # Resource properties\n",
    "        resourceClass = \"\"\n",
    "        formatElement = \"\"\n",
    "        resourceType = \"\"\n",
    "        keyword_list = '|'.join(newdata[\"dataset\"][y].get(\"keyword\", [])).replace(' ', '')\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        dateModified = cleanData(newdata[\"dataset\"][y]['modified']).split('T', 1)[0]\n",
    "\n",
    "        # Temporal Coverage\n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     \n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            # If temporalCoverage = YYYY or YYYY-YYYY, dateRange can be set accordingly\n",
    "            if '-' in temporalCoverage:\n",
    "                # format: YYYY-YYYY\n",
    "                dateRange = temporalCoverage\n",
    "            else:\n",
    "                # single year\n",
    "                dateRange = temporalCoverage + '-' + temporalCoverage\n",
    "        else:\n",
    "            temporalCoverage = 'Last modified ' + dateModified\n",
    "\n",
    "        # Rights\n",
    "        rights = cleanData(newdata[\"dataset\"][y].get('license', ''))\n",
    "\n",
    "        # Bounding Box\n",
    "        try:\n",
    "            scanned_bbox = newdata[\"dataset\"][y][\"spatial\"]\n",
    "\n",
    "            # Ensure it has the expected format\n",
    "            if scanned_bbox.get(\"type\") == \"envelope\" and \"coordinates\" in scanned_bbox:\n",
    "                coordinates = scanned_bbox[\"coordinates\"]\n",
    "                # Flatten the list of coordinates and round them\n",
    "                rounded_coordinates = [\n",
    "                    str(round(coord, 2)) for pair in coordinates for coord in pair\n",
    "                ]\n",
    "                # Format the bounding box as a comma-separated string\n",
    "                bbox = ','.join(rounded_coordinates)\n",
    "            else:\n",
    "                bbox = default_bbox  # Use the default bbox from the CSV\n",
    "        except Exception as e:\n",
    "            bbox = default_bbox  # Use the default bbox from the CSV in case of errors\n",
    "\n",
    "        # Determine Resource Class/Type and check for shapefile downloads\n",
    "        distribution = newdata[\"dataset\"][y].get(\"distribution\", [])\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                dist_title = dictionary.get(\"title\", \"\")\n",
    "                # If we find a shapefile distribution\n",
    "                if dist_title == \"Shapefile\":\n",
    "                    resourceClass = \"Datasets|Web services\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    downloadURL = dictionary.get(\"accessURL\", \"\")\n",
    "                # ArcGIS GeoService \n",
    "                if dist_title == \"ArcGIS GeoService\":\n",
    "                    webService = dictionary.get('accessURL', '')\n",
    "                    if \"FeatureServer\" in webService:\n",
    "                        featureServer = webService\n",
    "                        if resourceClass == \"\":\n",
    "                            resourceClass = \"Web services\"\n",
    "                            \n",
    "                    if \"MapServer\" in webService:\n",
    "                        mapServer = webService\n",
    "                        if resourceClass == \"\":\n",
    "                            resourceClass = \"Web services\"\n",
    "                            \n",
    "                    if \"ImageServer\" in webService:\n",
    "                        imageServer = webService\n",
    "                        resourceClass = \"Imagery|Web services\"\n",
    "                        formatElement = 'Imagery'\n",
    "                        resourceType = \"Raster data\"\n",
    "                    if \"TileServer\" in webService:\n",
    "                        tileServer = webService\n",
    "                        if resourceClass == \"\":\n",
    "                            resourceClass = \"Web services\"\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # If LiDAR found in title or description\n",
    "        if 'LiDAR' in title or 'LiDAR' in description:\n",
    "            resourceType = 'LiDAR'\n",
    "\n",
    "\n",
    "        # ID/Identifier\n",
    "        slug = identifier.split('=', 1)[-1].replace(\"&sublayer=\", \"_\")\n",
    "        querystring = parse_qs(urlparse(identifier).query)\n",
    "        if \"id\" in querystring:\n",
    "            identifier_new = \"https://hub.arcgis.com/datasets/\" + querystring[\"id\"][0]\n",
    "        else:\n",
    "            identifier_new = identifier\n",
    "\n",
    "        metadataList = [\n",
    "            title, \n",
    "            alternativeTitle, \n",
    "            description,\n",
    "            language,\n",
    "            displayNote,\n",
    "            creator,\n",
    "            titleSource,\n",
    "            resourceClass, \n",
    "            resourceType,\n",
    "            keyword_list, \n",
    "            dateIssued, \n",
    "            temporalCoverage,\n",
    "            dateRange, \n",
    "            spatialCoverage, \n",
    "            bbox,\n",
    "            formatElement, \n",
    "            information, \n",
    "            downloadURL,\n",
    "            mapServer, \n",
    "            featureServer,\n",
    "            imageServer, \n",
    "            tileServer,\n",
    "            slug, \n",
    "            identifier_new, \n",
    "            provider, \n",
    "            hubCode, \n",
    "            memberOf, \n",
    "            isPartOf, \n",
    "            rights,\n",
    "            accrualMethod,\n",
    "            dateAccessioned, \n",
    "            accessRights,\n",
    "            publicationState\n",
    "        ]     \n",
    "\n",
    "        newItemDict[slug] = metadataList\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8421ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "allRecords = []\n",
    "\n",
    "with open(hubFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        hubCode = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        isPartOf = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        accrualMethod = \"ArcGIS Hub\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        publicationState = \"published\"\n",
    "        language = \"eng\"\n",
    "        displayNote = (\"This dataset was automatically cataloged from the provider's ArcGIS Hub. \"\n",
    "                       \"In some cases, information shown here may be incorrect or out-of-date. \"\n",
    "                       \"Click the 'Visit Source' button to search for items on the original provider's website.\")\n",
    "\n",
    "        print(\"scanning \", hubCode, url)\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200 and 'application/json' in response.headers['Content-Type']:\n",
    "            newdata = response.json()\n",
    "        else:\n",
    "            print(\"Failed to fetch or incorrect content type: \", response.status_code, response.headers.get('Content-Type', ''))\n",
    "            continue\n",
    "\n",
    "        newjson_ids = getIdentifiers(newdata)\n",
    "        record_dict = metadataNewItems(newdata, newjson_ids, language, displayNote, titleSource, spatialCoverage, \n",
    "                               provider, hubCode, memberOf, isPartOf, accrualMethod, dateAccessioned, accessRights, row['Bounding Box'])\n",
    "\n",
    "        if record_dict:\n",
    "            allRecords.append(record_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2eb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten allRecords into a list of dicts suitable for DataFrame creation\n",
    "flat_data = []\n",
    "for rec in allRecords:\n",
    "    for slug, values in rec.items():\n",
    "        row_dict = dict(zip(fieldnames, values))\n",
    "        flat_data.append(row_dict)\n",
    "\n",
    "if not flat_data:\n",
    "    print(\"No records found, no CSV will be created.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "df = pd.DataFrame(flat_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71997c70",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by ID and Title\n",
    "df = df.drop_duplicates(subset=['ID'])\n",
    "df = df.drop_duplicates(subset=['Title'])\n",
    "\n",
    "# Fix Department of the Interior issue\n",
    "# Fix Creator field if it's a dict with 'name'\n",
    "def clean_creator(value):\n",
    "    if isinstance(value, dict) and 'name' in value:\n",
    "        return value['name']\n",
    "    elif isinstance(value, str):\n",
    "        # Handle string versions of the dict like \"{'name': 'Department of the Interior'}\"\n",
    "        match = re.match(r\"\\{\\s*'name'\\s*:\\s*'(.+?)'\\s*\\}\", value)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return value\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(clean_creator)\n",
    "#df['Creator'] = df['Creator'].replace(\"{'name': 'Department of the Interior'}\", \"Department of the Interior\")\n",
    "\n",
    "# Drop rows without a Resource Cl\n",
    "df['Resource Class'] = df['Resource Class'].replace(r'^\\s*$', np.nan, regex=True) #clear NaN values\n",
    "df = df.dropna(subset=['Resource Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f99f8",
   "metadata": {},
   "source": [
    "Notes about function \"clean_creator\":\n",
    "\n",
    "If the value is a dict with a 'name' key:\n",
    "→ It returns the 'name' (e.g., 'Department of the Interior').\n",
    "\n",
    "If the value is a string that looks like a dict (e.g., \"{'name': 'Department of the Interior'}\"):\n",
    "→ It uses a regular expression to extract the 'name' part only.\n",
    "\n",
    "If the value is a regular string like \"University of Iowa\":\n",
    "→ It returns it unchanged.\n",
    "\n",
    "If the value is NaN, None, or something else:\n",
    "→ It also returns it unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first CSV (all fields except links)\n",
    "# Updated fields for links\n",
    "link_fields = ['documentation_external', 'download', 'arcgis_dynamic_map_layer', 'arcgis_feature_layer', 'arcgis_image_map_layer', 'arcgis_tiled_map_layer']\n",
    "df_first_csv = df.drop(columns=link_fields)\n",
    "df_first_csv.to_csv(f'{ActionDate}_ArcHubs-metadata.csv', index=False)\n",
    "\n",
    "# Create the second CSV with friendlier_id, reference_type, distribution_url, and label\n",
    "rows = []\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    slug = r['ID']\n",
    "    for lf in link_fields:\n",
    "        if pd.notna(r[lf]) and r[lf] != \"\":\n",
    "            # Add Format as the label only if the reference_type is \"Download\"\n",
    "            label_value = r['Format'] if lf.lower() == 'download' else ''\n",
    "            rows.append({\n",
    "                'friendlier_id': slug, \n",
    "                'reference_type': lf, \n",
    "                'distribution_url': r[lf], \n",
    "                'label': label_value\n",
    "            })\n",
    "\n",
    "df_second_csv = pd.DataFrame(rows, columns=['friendlier_id', 'reference_type', 'distribution_url', 'label'])\n",
    "df_second_csv.to_csv(f'{ActionDate}_ArcHubs-links.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
