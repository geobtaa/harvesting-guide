{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f13543b",
   "metadata": {},
   "source": [
    "# Harvest from UMedia (05d-01)\n",
    "\n",
    "Purpose: This recipe is for harvesting items from the [University Of Minnesota's UMedia Digital Library](https://umedia.lib.umn.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41074e",
   "metadata": {},
   "source": [
    "## Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import csv \n",
    "import urllib.request \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_processing_utils import load_language_mapping, convert_languages_to_iso, clean_complex_fields, replace_with_pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b131780",
   "metadata": {},
   "source": [
    "## Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE IF NEEDED Specify the path to your JSON file\n",
    "json_file_path = 'east-asian.json'\n",
    "\n",
    "# Specify the path for the CSV file you want to create\n",
    "csv_file_path = 'east-asian.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and parameters for the API request\n",
    "base_url = 'https://umedia.lib.umn.edu/search.json'\n",
    "\n",
    "# Change these to match the facets you want to harvest\n",
    "params = {\n",
    "    'facets[contributing_organization_name_s][]': 'University of Minnesota Libraries, John R. Borchert Map Library.',\n",
    "#     'facets[super_collection_name_ss][]': 'Revealing Bound Maps'\n",
    "}\n",
    "\n",
    "# Change if needed \n",
    "max_items = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a2e6a",
   "metadata": {},
   "source": [
    "## Part 1: Download the metadata JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326acc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_metadata(base_url, params, max_items):\n",
    "    items = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        current_params = params.copy()\n",
    "        current_params['page'] = page\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(base_url, params=current_params)\n",
    "        response_json = response.json()\n",
    "\n",
    "        if not response_json or len(items) + len(response_json) > max_items:\n",
    "            break\n",
    "\n",
    "        items.extend(response_json)\n",
    "\n",
    "        if len(response_json) < 20:  # Assuming each page has 20 items; adjust based on actual pagination\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return items[:max_items]\n",
    "\n",
    "\n",
    "\n",
    "# Fetch the metadata\n",
    "metadata_items = fetch_metadata(base_url, params, max_items)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata_items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data successfully saved to {json_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559b39f",
   "metadata": {},
   "source": [
    "## Part 2: Parse the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateEnd = time.strftime('%Y-%m')\n",
    "\n",
    "fieldnames = ['Title', 'Alternative Title', 'Description', 'notes', 'dimensions', 'scale', 'Language', 'Creator', 'Publisher',\n",
    "              'Resource Type', 'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range',\n",
    "              'Information', 'Download', 'Image', 'Manifest', \n",
    "              'Identifier', 'ID', 'Access Rights', 'Provider', 'Code', 'Is Part Of', 'Member Of',\n",
    "              'Accrual Method', 'Date Accessioned', 'Rights', 'Resource Class', 'Format', 'Date Added', 'Collection Name', 'Set', 'City', 'State', 'Country', 'Continent', 'Region', 'coordinates'] \n",
    "\n",
    "actionDate = time.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ccdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON content into a DataFrame\n",
    "df = pd.read_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3684b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe and transfer values to it\n",
    "out_df = pd.DataFrame()\n",
    "\n",
    "## extract content from df\n",
    "out_df['Title'] = df['title']\n",
    "out_df['Alternative Title'] = df['title']\n",
    "out_df['Publisher'] = df['publisher']\n",
    "out_df['Keyword'] = df['subject'].str.join('|')\n",
    "out_df['Date Issued'] = df['date_created'].str.join(';')\n",
    "\n",
    "\n",
    "## extract content with variable presence\n",
    "        \n",
    "try:\n",
    "      out_df['Creator'] = df['creator'].str.join('|')\n",
    "except:\n",
    "      out_df['Creator'] = ''\n",
    "        \n",
    "try:\n",
    "      out_df['Provider'] = df['contributing_organization']\n",
    "except:\n",
    "      out_df['Provider'] = '' \n",
    "        \n",
    "try:\n",
    "      out_df['Rights'] = df['local_rights']\n",
    "except:\n",
    "      out_df['Rights'] = ''   \n",
    "\n",
    "try:\n",
    "      out_df['Identifier'] = df['persistent_url']\n",
    "except:\n",
    "      out_df['Identifier'] = '' \n",
    "        \n",
    "## These need to be manually checked\n",
    "try:\n",
    "      out_df['coordinates'] = df['coordinates'].str.join('|')\n",
    "except:\n",
    "      out_df['coordinates'] = '' \n",
    "        \n",
    "    \n",
    "## construct links\n",
    "out_df['Information'] = 'https://umedia.lib.umn.edu/item/' + df['id']\n",
    "out_df['Download'] = 'http://cdm16022.contentdm.oclc.org/utils/getfile/collection/' + df['set_spec'] + '/id/' + df['parent_id'].astype(str) + '/filename/print/page/download/fparams/forcedownload'\n",
    "out_df['B1G Image'] = df['thumb_url']\n",
    "out_df['Manifest'] = 'https://cdm16022.contentdm.oclc.org/iiif/info/' + df['set_spec'] + '/' + df['parent_id'].astype(str) + '/manifest.json'\n",
    "out_df['ID'] = df['id']\n",
    "\n",
    "\n",
    "## some hard-coded fields\n",
    "out_df['Resource Type'] = ''\n",
    "out_df['Code'] = '05d-01'\n",
    "out_df['Is Part Of'] = '05d-01'\n",
    "out_df['Member Of'] = '64bd8c4c-8e60-4956-b43d-bdc3f93db488'\n",
    "out_df['Accrual Method'] = 'JSON API'\n",
    "out_df['Access Rights'] = 'Public'\n",
    "out_df['Date Accessioned'] = actionDate\n",
    "out_df['Resource Class'] = 'Maps'\n",
    "out_df['Format'] = 'JPEG'\n",
    "\n",
    "\n",
    "## useful info that is not part of the BTAA Metadata Profile\n",
    "out_df['Date Added'] = df['date_added']\n",
    "try:\n",
    "      out_df['Collection Name'] = df['collection_name']\n",
    "except:\n",
    "      out_df['Collection Name'] = '' \n",
    "\n",
    "try:\n",
    "      out_df['Set'] = df['set_spec']\n",
    "except:\n",
    "      out_df['Set'] = ''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adafca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_range(date_str):\n",
    "    # Check if date_str is not a string (e.g., NaN represented as float)\n",
    "    if not isinstance(date_str, str):\n",
    "        # Handle non-string input (e.g., NaN) as you see fit; here, we return a placeholder\n",
    "        return \"\"\n",
    "    \n",
    "    # Split the date string into a list of years\n",
    "    years = date_str.split()\n",
    "    # Convert each year to an integer to sort them correctly\n",
    "    years = [int(year) for year in years]\n",
    "    \n",
    "    # Use the first and last years to create the range if there are multiple years\n",
    "    if len(years) > 1:\n",
    "        return f\"{min(years)}-{max(years)}\"\n",
    "    # If only one year is present, use it for both start and end of the range\n",
    "    else:\n",
    "        year = years[0]\n",
    "        return f\"{year}-{year}\"\n",
    "\n",
    "\n",
    "# Apply the function to create the 'Date Range' column\n",
    "out_df['Date Range'] = df['date_created_sort'].apply(create_date_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns\n",
    "complex_columns = ['language']\n",
    "\n",
    "# Apply clean_complex_fields to complex columns\n",
    "for col in complex_columns:\n",
    "    df[col] = df[col].apply(clean_complex_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_pipe = ['country', 'state', 'city', 'region', 'continent']\n",
    "\n",
    "# Apply the function to each column in the list\n",
    "for col in columns_to_pipe:\n",
    "    df[col] = replace_with_pipes(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "## update values for cities to match FAST format\n",
    "\n",
    "# Step 1: Prepare 'state' by using the first state if available, otherwise use 'country'\n",
    "df['state_or_country'] = df['state'].str.split('|').str[0].fillna('')\n",
    "df['state_or_country'] = df.apply(lambda row: row['country'] if row['state_or_country'] == '' else row['state_or_country'], axis=1)\n",
    "\n",
    "# Step 2: Concatenate 'state_or_country' with each city in the 'city' column\n",
    "def concatenate_location_city(row):\n",
    "    # Check if 'city' is not null or empty\n",
    "    if pd.notnull(row['city']) and row['city'] != '':\n",
    "        # Split the cities on pipe, concatenate with state_or_country, and join back with pipe\n",
    "        cities_augmented = '|'.join([f\"{row['state_or_country']}--{city.strip()}\" for city in row['city'].split('|')])\n",
    "        return cities_augmented\n",
    "    else:\n",
    "        # If 'city' is null or empty, just return an empty string or some default value\n",
    "        return ''\n",
    "\n",
    "# Apply the function to each row to augment 'city' values\n",
    "df['city'] = df.apply(concatenate_location_city, axis=1)\n",
    "        \n",
    "out_df['City'] = df['city']\n",
    " \n",
    "\n",
    "try:\n",
    "      out_df['State'] = df['state']\n",
    "except:\n",
    "      out_df['State'] = ''   \n",
    "        \n",
    "\n",
    "try:\n",
    "      out_df['Country'] = df['country']\n",
    "except:\n",
    "      out_df['Country'] = '' \n",
    "\n",
    "try:\n",
    "      out_df['Region'] = df['region']\n",
    "except:\n",
    "      out_df['Region'] = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb7736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "\n",
    "def concatenate_fields(row, fields):\n",
    "    # Initial concatenation of the fields if they exist in the row, ignoring empty or NaN values\n",
    "    concatenated_values = '|'.join(filter(None, [str(row[field]).strip() if (field in row.index and pd.notnull(row[field])) else '' for field in fields]))\n",
    "    # Additional step to remove whitespace around pipe separators\n",
    "    concatenated_values = '|'.join([value.strip() for value in concatenated_values.split('|')])\n",
    "    return concatenated_values\n",
    "\n",
    "# List of fields to concatenate for Description\n",
    "fields_for_description = ['description', 'notes', 'dimension', 'scale']\n",
    "\n",
    "# List of fields to concatenate for Spatial Coverage\n",
    "fields_for_spatial_coverage = ['state', 'city', 'region', 'country', 'continent']\n",
    "\n",
    "# Apply the function to each row of df to create the 'Description' column in out_df\n",
    "out_df['Description'] = df.apply(lambda row: concatenate_fields(row, fields_for_description), axis=1)\n",
    "\n",
    "# Apply the function to each row of df to create the 'Spatial Coverage' column\n",
    "out_df['Spatial Coverage'] = df.apply(lambda row: concatenate_fields(row, fields_for_spatial_coverage), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping\n",
    "lang_to_iso = load_language_mapping('../language-vocabulary.csv')\n",
    "\n",
    "out_df['Text Language'] = df['language']\n",
    "out_df['Language'] = df['language'].apply(lambda x: convert_languages_to_iso(x, lang_to_iso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811ff9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "out_df.to_csv(csv_file_path, index=False)  # Set index=False to avoid adding the DataFrame index as a column in the CSV\n",
    "\n",
    "print(f\"Data successfully saved to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
