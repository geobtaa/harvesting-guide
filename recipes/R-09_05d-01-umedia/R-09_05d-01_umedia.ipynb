{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f13543b",
   "metadata": {},
   "source": [
    "# Harvest from UMedia (05d-01)\n",
    "\n",
    "Purpose: This recipe is for harvesting items from the [University Of Minnesota's UMedia Digital Library](https://umedia.lib.umn.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41074e",
   "metadata": {},
   "source": [
    "## Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa84578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import csv \n",
    "import urllib.request \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f385e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_processing_utils import load_language_mapping, convert_languages_to_iso, clean_complex_fields, replace_with_pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b131780",
   "metadata": {},
   "source": [
    "## Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dced9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE IF NEEDED Specify the path to your JSON file\n",
    "json_file_path = 'umedia.json'\n",
    "\n",
    "# Specify the path for the CSV file you want to create\n",
    "csv_file_path = 'umedia.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f3e72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and parameters for the API request\n",
    "base_url = 'https://umedia.lib.umn.edu/search.json'\n",
    "\n",
    "# Change these to match the facets you want to harvest\n",
    "params = {\n",
    "    'facets[contributing_organization_name_s][]': 'University of Minnesota Libraries, John R. Borchert Map Library.',\n",
    "#     'facets[super_collection_name_ss][]': 'Revealing Bound Maps'\n",
    "}\n",
    "\n",
    "# Change if needed \n",
    "max_items = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a2e6a",
   "metadata": {},
   "source": [
    "## Part 1: Download the metadata JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326acc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to umedia.json\n"
     ]
    }
   ],
   "source": [
    "def fetch_metadata(base_url, params, max_items):\n",
    "    items = []\n",
    "    page = 0\n",
    "    while True:\n",
    "        current_params = params.copy()\n",
    "        current_params['page'] = page\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(base_url, params=current_params)\n",
    "        response_json = response.json()\n",
    "\n",
    "        if not response_json or len(items) + len(response_json) > max_items:\n",
    "            break\n",
    "\n",
    "        items.extend(response_json)\n",
    "\n",
    "        if len(response_json) < 20:  # Assuming each page has 20 items; adjust based on actual pagination\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return items[:max_items]\n",
    "\n",
    "\n",
    "\n",
    "# Fetch the metadata\n",
    "metadata_items = fetch_metadata(base_url, params, max_items)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata_items, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data successfully saved to {json_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559b39f",
   "metadata": {},
   "source": [
    "## Part 2: Parse the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a23789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dateEnd = time.strftime('%Y-%m')\n",
    "\n",
    "fieldnames = ['Title', 'Alternative Title', 'Description', 'notes', 'dimensions', 'scale', 'Language', 'Creator', 'Publisher',\n",
    "              'Resource Type', 'Keyword', 'Date Issued', 'Temporal Coverage', 'Date Range',\n",
    "              'Information', 'Download', 'Image', 'Manifest', \n",
    "              'Identifier', 'ID', 'Access Rights', 'Provider', 'Code', 'Is Part Of', 'Member Of',\n",
    "              'Accrual Method', 'Date Accessioned', 'Rights', 'Resource Class', 'Format', 'Date Added', 'Local Collection', 'Set', 'City', 'State', 'Country', 'Continent', 'Region', 'coordinates', 'Spatial Resolution as Text'] \n",
    "\n",
    "actionDate = time.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17ccdfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON content into a DataFrame\n",
    "df = pd.read_json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3684b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe and transfer values to it\n",
    "out_df = pd.DataFrame()\n",
    "\n",
    "## extract content from df\n",
    "out_df['Title'] = df['title']\n",
    "out_df['Alternative Title'] = df['title']\n",
    "out_df['Publisher'] = df['publisher']\n",
    "out_df['Keyword'] = df['subject'].str.join('|')\n",
    "out_df['Date Issued'] = df['date_created'].str.join(';')\n",
    "\n",
    "\n",
    "## extract content with variable presence\n",
    "        \n",
    "try:\n",
    "      out_df['Creator'] = df['creator'].str.join('|')\n",
    "except:\n",
    "      out_df['Creator'] = ''\n",
    "        \n",
    "try:\n",
    "      out_df['Provider'] = df['contributing_organization']\n",
    "except:\n",
    "      out_df['Provider'] = '' \n",
    "        \n",
    "try:\n",
    "      out_df['Rights'] = df['local_rights']\n",
    "except:\n",
    "      out_df['Rights'] = ''   \n",
    "\n",
    "try:\n",
    "      out_df['Identifier'] = df['persistent_url']\n",
    "except:\n",
    "      out_df['Identifier'] = '' \n",
    "        \n",
    "## These need to be manually checked\n",
    "try:\n",
    "      out_df['coordinates'] = df['coordinates'].str.join('|')\n",
    "except:\n",
    "      out_df['coordinates'] = '' \n",
    "        \n",
    "        \n",
    "try:\n",
    "      out_df['Spatial Resolution as Text'] = df['scale']\n",
    "except:\n",
    "      out_df['scale'] = ''       \n",
    "        \n",
    "    \n",
    "## construct links\n",
    "out_df['Information'] = 'https://umedia.lib.umn.edu/item/' + df['id']\n",
    "out_df['Download'] = 'http://cdm16022.contentdm.oclc.org/utils/getfile/collection/' + df['set_spec'] + '/id/' + df['parent_id'].astype(str) + '/filename/print/page/download/fparams/forcedownload'\n",
    "out_df['B1G Image'] = df['thumb_url']\n",
    "out_df['Manifest'] = 'https://cdm16022.contentdm.oclc.org/iiif/info/' + df['set_spec'] + '/' + df['parent_id'].astype(str) + '/manifest.json'\n",
    "out_df['ID'] = df['id']\n",
    "\n",
    "\n",
    "## some hard-coded fields\n",
    "out_df['Resource Type'] = ''\n",
    "out_df['Code'] = '05d-01'\n",
    "out_df['Is Part Of'] = '05d-01'\n",
    "out_df['Member Of'] = '64bd8c4c-8e60-4956-b43d-bdc3f93db488'\n",
    "out_df['Accrual Method'] = 'JSON API'\n",
    "out_df['Access Rights'] = 'Public'\n",
    "out_df['Date Accessioned'] = actionDate\n",
    "out_df['Resource Class'] = 'Maps'\n",
    "out_df['Format'] = 'JPEG'\n",
    "\n",
    "\n",
    "## useful info that is not part of the BTAA Metadata Profile\n",
    "out_df['Date Added'] = df['date_added']\n",
    "try:\n",
    "      out_df['Local Collection'] = df['collection_name']\n",
    "except:\n",
    "      out_df['Local Collection'] = '' \n",
    "\n",
    "try:\n",
    "      out_df['Set'] = df['set_spec']\n",
    "except:\n",
    "      out_df['Set'] = ''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5adafca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_range(date_str):\n",
    "    # Check if date_str is not a string (e.g., NaN represented as float)\n",
    "    if not isinstance(date_str, str):\n",
    "        # Handle non-string input (e.g., NaN) as you see fit; here, we return a placeholder\n",
    "        return \"\"\n",
    "    \n",
    "    # Split the date string into a list of years\n",
    "    years = date_str.split()\n",
    "    # Convert each year to an integer to sort them correctly\n",
    "    years = [int(year) for year in years]\n",
    "    \n",
    "    # Use the first and last years to create the range if there are multiple years\n",
    "    if len(years) > 1:\n",
    "        return f\"{min(years)}-{max(years)}\"\n",
    "    # If only one year is present, use it for both start and end of the range\n",
    "    else:\n",
    "        year = years[0]\n",
    "        return f\"{year}-{year}\"\n",
    "\n",
    "\n",
    "# Apply the function to create the 'Date Range' column\n",
    "out_df['Date Range'] = df['date_created_sort'].apply(create_date_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ed3ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns\n",
    "complex_columns = ['language']\n",
    "\n",
    "# Apply clean_complex_fields to complex columns\n",
    "for col in complex_columns:\n",
    "    df[col] = df[col].apply(clean_complex_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d6d91f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_pipe = ['country', 'state', 'city', 'region', 'continent']\n",
    "\n",
    "# Apply the function to each column in the list\n",
    "for col in columns_to_pipe:\n",
    "    df[col] = replace_with_pipes(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2539f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## update values for cities to match FAST format\n",
    "\n",
    "# # Step 1: Prepare 'state' by using the first state if available, otherwise use 'country'\n",
    "# df['state_or_country'] = df['state'].str.split('|').str[0].fillna('')\n",
    "# df['state_or_country'] = df.apply(lambda row: row['country'] if row['state_or_country'] == '' else row['state_or_country'], axis=1)\n",
    "\n",
    "# # Step 2: Concatenate 'state_or_country' with each city in the 'city' column\n",
    "# def concatenate_location_city(row):\n",
    "#     # Check if 'city' is not null or empty\n",
    "#     if pd.notnull(row['city']) and row['city'] != '':\n",
    "#         # Split the cities on pipe, concatenate with state_or_country, and join back with pipe\n",
    "#         cities_augmented = '|'.join([f\"{row['state_or_country']}--{city.strip()}\" for city in row['city'].split('|')])\n",
    "#         return cities_augmented\n",
    "#     else:\n",
    "#         # If 'city' is null or empty, just return an empty string or some default value\n",
    "#         return ''\n",
    "\n",
    "# # Apply the function to each row to augment 'city' values\n",
    "# df['city'] = df.apply(concatenate_location_city, axis=1)\n",
    "        \n",
    "# out_df['City'] = df['city']\n",
    " \n",
    "\n",
    "try:\n",
    "      out_df['City'] = df['city']\n",
    "except:\n",
    "      out_df['City'] = ''  \n",
    "    \n",
    "try:\n",
    "      out_df['State'] = df['state']\n",
    "except:\n",
    "      out_df['State'] = ''   \n",
    "        \n",
    "\n",
    "try:\n",
    "      out_df['Country'] = df['country']\n",
    "except:\n",
    "      out_df['Country'] = '' \n",
    "\n",
    "try:\n",
    "      out_df['Region'] = df['region']\n",
    "except:\n",
    "      out_df['Region'] = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cb7736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "\n",
    "def concatenate_fields(row, fields):\n",
    "    # Initial concatenation of the fields if they exist in the row, ignoring empty or NaN values\n",
    "    concatenated_values = '|'.join(filter(None, [str(row[field]).strip() if (field in row.index and pd.notnull(row[field])) else '' for field in fields]))\n",
    "    # Additional step to remove whitespace around pipe separators\n",
    "    concatenated_values = '|'.join([value.strip() for value in concatenated_values.split('|')])\n",
    "    return concatenated_values\n",
    "\n",
    "# List of fields to concatenate for Description\n",
    "fields_for_description = ['description', 'notes', 'dimension']\n",
    "\n",
    "# List of fields to concatenate for Spatial Coverage\n",
    "fields_for_spatial_coverage = ['state', 'city', 'region', 'country', 'continent']\n",
    "\n",
    "# Apply the function to each row of df to create the 'Description' column in out_df\n",
    "out_df['Description'] = df.apply(lambda row: concatenate_fields(row, fields_for_description), axis=1)\n",
    "\n",
    "# Apply the function to each row of df to create the 'Spatial Coverage' column\n",
    "out_df['Spatial Coverage'] = df.apply(lambda row: concatenate_fields(row, fields_for_spatial_coverage), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c71e6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mapping\n",
    "lang_to_iso = load_language_mapping('../language-vocabulary.csv')\n",
    "\n",
    "out_df['Text Language'] = df['language']\n",
    "out_df['Language'] = df['language'].apply(lambda x: convert_languages_to_iso(x, lang_to_iso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "811ff9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to umedia.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "out_df.to_csv(csv_file_path, index=False)  # Set index=False to avoid adding the DataFrame index as a column in the CSV\n",
    "\n",
    "print(f\"Data successfully saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d1117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
