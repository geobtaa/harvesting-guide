{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edaebb0",
   "metadata": {},
   "source": [
    "## Transform a batch GBL 1.0 JSON files from UW-Madison\n",
    "\n",
    "**Purpose: This script will read a batch of GBL-1.0 metadata JSON files and tranform them into a single CSV.** \n",
    "\n",
    "This is a version that is specific for UW-Madison records\n",
    "\n",
    "Metadata records in the [GeoBlacklight](https://opengeometadata.org/docs/gbl-1.0) or [OpenGeoMetadata](https://opengeometadata.org/docs/ogm-aardvark) standards are frequently shared as batches of JSON files. The entire [OpenGeoMetadata organization](https://github.com/OpenGeoMetadata) contains repositories full of hundreds of thousands of GeoBlacklight JSONs.\n",
    "\n",
    "In order to ingest these into the BTAA Geoportal, we need to transform them into a CSV.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b91c0",
   "metadata": {},
   "source": [
    "## Part 1: Load the modules and JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a33c67",
   "metadata": {},
   "source": [
    "### Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf654768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8083b1",
   "metadata": {},
   "source": [
    "### Declare the paths and file names\n",
    "\n",
    "First, move a folder of the JSONs into this directory. Files in the folder can be nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r\"/Users/majew030/GitHub/OGM/edu.wisc/HeldBy_RML\" # enter the name of the folder\n",
    "csv_name = \"10d-03\" # create a name for the output CSV without the .csv extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e4301",
   "metadata": {},
   "source": [
    "### Load the files into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7454688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [] # empty list\n",
    "\n",
    "# through all items, format and append to dataset list\n",
    "for path, dir, files in os.walk(json_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            json_file_open = open(file_path, 'rb')\n",
    "            data = json_file_open.read().decode('utf-8', errors='ignore')\n",
    "            loaded = json.loads(data)\n",
    "            dataset.append(loaded)\n",
    "            \n",
    "df = pd.DataFrame(dataset) # convert dataset into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: check all the field names\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0c3b1",
   "metadata": {},
   "source": [
    "### Transform multivalued fields for CSV template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split arrays and turn them into multivalued fields separated by pipes '|'\n",
    "\n",
    "df['dc_creator_sm']=df['dc_creator_sm'].str.join('|')\n",
    "df['dc_subject_sm']=df['dc_subject_sm'].str.join('|')\n",
    "df['dct_spatial_sm']=df['dct_spatial_sm'].str.join('|')\n",
    "df['dct_isPartOf_sm']=df['dct_isPartOf_sm'].str.join('|')\n",
    "df['dct_temporal_sm']=df['dct_temporal_sm'].str.join('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split solr_geom coordinates and reorder from WENS to WSEN\n",
    "df[['w','e','n','s']] = df['solr_geom'].str.strip('ENVELOPE()').str.split(',', expand=True)\n",
    "df['Bounding Box'] = df[['w', 's','e','n']].agg(','.join, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename columns to match GeoBTAA Template\n",
    "\n",
    "df = df.rename(columns={ \n",
    "    'dc_title_s' : 'Title',\n",
    "    'dc_description_s' : 'Description',\n",
    "    'dc_creator_sm' : 'Creator',\n",
    "    'dct_issued_s' : 'Date Issued',\n",
    "    'dc_rights_s' : 'Access Rights',\n",
    "    'dc_format_s' : 'Format',\n",
    "    'layer_slug_s' : 'ID',\n",
    "    'layer_id_s' : 'WxS Identifier', \n",
    "    'dct_provenance_s' : 'Provider',\n",
    "    'dc_publisher_s' : 'Publisher',\n",
    "    'dc_publisher_sm' : 'Publisher',\n",
    "    'dct_temporal_sm' : 'Temporal Coverage',\n",
    "    'dct_isPartOf_sm' : 'Local Collection',\n",
    "    'dc_subject_sm': 'Subject',\n",
    "    'uw_deprioritize_item_b' : 'Child Record',\n",
    "    'thumbnail_path_ss' : 'B1G Image'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc3ee8",
   "metadata": {},
   "source": [
    "## Add new fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43055315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Date Range field\n",
    "\n",
    "def format_temporal_coverage(row):\n",
    "    temporal_coverage = row['Temporal Coverage']\n",
    "    \n",
    "    # Check if the value is already a valid date range in yyyy-yyyy format\n",
    "    if pd.notna(temporal_coverage) and re.match(r'\\d{4}-\\d{4}', temporal_coverage):\n",
    "        return temporal_coverage  # Value is already formatted, so no change needed\n",
    "    \n",
    "    # Apply your existing logic to duplicate and format the value\n",
    "    return f\"{temporal_coverage}-{temporal_coverage}\" if pd.notna(temporal_coverage) else ''\n",
    "\n",
    "# Apply the function to create or update the \"Date Range\" column\n",
    "df['Date Range'] = df.apply(format_temporal_coverage, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Georeferenced field\n",
    "\n",
    "def check_geotiff(value):\n",
    "    if pd.notna(value) and \"GeoTIFF\" in value:\n",
    "        return \"true\"\n",
    "    else:\n",
    "        return \"false\"\n",
    "\n",
    "# Create the \"Georeferenced\" column using the check_geotiff function\n",
    "df[\"Georeferenced\"] = df[\"Format\"].apply(check_geotiff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dadcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Identifier field\n",
    "\n",
    "df['Identifier'] = \"https://geodata.wisc.edu/catalog/\" + df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75231a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Theme field\n",
    "\n",
    "theme_map = {\n",
    "    \"Farming\": \"Agriculture\",\n",
    "    \"Biota\": \"Biology\",\n",
    "    \"Atmospheric Sciences\": \"Climate\",\n",
    "    \"Geoscientific Information\": \"Geology\",\n",
    "    \"Imagery and Base Maps\": \"Imagery\",\n",
    "    \"Planning and Cadastral\": \"Property\",\n",
    "    \"Utilities and Communication\": \"Utilities\"\n",
    "}\n",
    "\n",
    "def map_theme_multivalued(subject):\n",
    "    \"\"\"\n",
    "    Split a pipe-separated Subject field into terms,\n",
    "    remap each term via theme_map if present,\n",
    "    then rejoin with pipes.\n",
    "    \"\"\"\n",
    "    # handle missing or empty\n",
    "    if not isinstance(subject, str) or subject.strip() == \"\":\n",
    "        return subject\n",
    "\n",
    "    parts = subject.split(\"|\")\n",
    "    mapped = [ theme_map.get(p.strip(), p.strip()) for p in parts ]\n",
    "    return \"|\".join(mapped)\n",
    "\n",
    "df['Theme'] = df['Subject'].apply(map_theme_multivalued)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f39dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add administrative fields with default values\n",
    "\n",
    "# Get the current date in yyyy-mm-dd format\n",
    "today_date = datetime.date.today().isoformat()\n",
    "\n",
    "# Add the \"Date Accessioned\" column with the today's date value to the DataFrame\n",
    "df['Date Accessioned'] = today_date\n",
    "df['Code'] = \"10\"\n",
    "df['Is Part Of'] = \"10d-03\"\n",
    "df['Member Of'] = \"dc8c18df-7d64-4ff4-a754-d18d0891187d\"\n",
    "df['Accrual Method'] = \"GBL-1.0\"\n",
    "df['Language'] = \"eng\"\n",
    "df['Spatial Coverage'] = \"Wisconsin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492f52e",
   "metadata": {},
   "source": [
    "### Transform values for fields without a straight crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Type to Resource Class value\n",
    "df['Resource Class'] = df['dc_type_s'].apply(lambda x: 'Imagery' if x == 'Image' else 'Datasets')\n",
    "\n",
    "\n",
    "#Convert Geometry Type to Resource Type value\n",
    "df['Resource Type'] = df['layer_geom_type_s'].astype(str) + ' data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b3c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Display Note from uw_notice_s and uw_supplemental_s\n",
    "\n",
    "def map_display_note(notice, supplemental):\n",
    "    \"\"\"    - uw_notice_s goes first (if nonblank)\n",
    "    - uw_supplemental_s goes second, prepended with \"Info: \" (if nonblank)\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    # add notice if present\n",
    "    if isinstance(notice, str) and notice.strip():\n",
    "        parts.append(notice.strip())\n",
    "    # add supplemental if present\n",
    "    if isinstance(supplemental, str) and supplemental.strip():\n",
    "        parts.append(f\"Info: {supplemental.strip()}\")\n",
    "    # join or return empty string\n",
    "    return \"|\".join(parts) if parts else \"\"\n",
    "\n",
    "df['Display Note'] = [\n",
    "    map_display_note(n, s)\n",
    "    for n, s in zip(df['uw_notice_s'], df['uw_supplemental_s'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7487c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Creator field to FAST format\n",
    "\n",
    "counties_in_wisconsin = [\n",
    "    'Adams', 'Ashland', 'Barron', 'Bayfield', 'Brown', 'Buffalo',\n",
    "    'Burnett', 'Calumet', 'Chippewa', 'Clark', 'Columbia', 'Crawford',\n",
    "    'Dane', 'Dodge', 'Door', 'Douglas', 'Dunn', 'Eau Claire',\n",
    "    'Florence', 'Fond du Lac', 'Fond Du Lac', 'Forest', 'Grant', 'Green', 'Green Lake',\n",
    "    'Iowa', 'Iron', 'Jackson', 'Jefferson', 'Juneau', 'Kenosha',\n",
    "    'Kewaunee', 'La Crosse', 'Lacrosse', 'Lafayette', 'Langlade', 'Lincoln',\n",
    "    'Manitowoc', 'Marathon', 'Marinette', 'Marquette', 'Menominee',\n",
    "    'Milwaukee', 'Monroe', 'Oconto', 'Oneida', 'Outagamie', 'Ozaukee',\n",
    "    'Pepin', 'Pierce', 'Polk', 'Portage', 'Price', 'Racine',\n",
    "    'Richland', 'Rock', 'Rusk', 'Sauk', 'Sawyer', 'Shawano',\n",
    "    'Sheboygan', 'St. Croix', 'St Croix', 'Taylor', 'Trempealeau', 'Vernon', 'Vilas',\n",
    "    'Walworth', 'Washburn', 'Washington', 'Waukesha', 'Waupaca',\n",
    "    'Waushara', 'Winnebago', 'Wood'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e17cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_wisconsin(name, counties):\n",
    "    \"\"\"\n",
    "    If `name` is a Wisconsin county (endswith \" County\" and base name is in counties),\n",
    "    or if it starts with \"City of \", prepend 'Wisconsin--'.\n",
    "    Otherwise, return as-is.\n",
    "    \"\"\"\n",
    "    # County check\n",
    "    if name.endswith(\" County\"):\n",
    "        base = name[:-len(\" County\")]\n",
    "        if base in counties:\n",
    "            return f\"Wisconsin--{name}\"\n",
    "    # City check\n",
    "    if name.startswith(\"City of \"):\n",
    "        return f\"Wisconsin--{name}\"\n",
    "    return name\n",
    "\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(\n",
    "    prepend_wisconsin, \n",
    "    args=(counties_in_wisconsin,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cafc80",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_pipes_and_spaces(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove leading and trailing pipes and spaces\n",
    "        value = value.strip('| ').strip('| ')\n",
    "        # Replace double or more spaces with a single space\n",
    "        value = re.sub(r'\\s{2,}', ' ', value)\n",
    "        return value\n",
    "    return value\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "df = df.map(trim_pipes_and_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by ID\n",
    "df = df.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38345bf",
   "metadata": {},
   "source": [
    "### Write the DataFrame to a CSV file with Aardvark labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Publisher',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Local Collection',\n",
    "'Theme',\n",
    "'Subject',\n",
    "'Keyword',\n",
    "'Temporal Coverage',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Geometry',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'Format',\n",
    "'WxS Identifier',\n",
    "'Georeferenced',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Access Rights',\n",
    "'Child Record',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method',\n",
    "'B1G Image'\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "primary_df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5842da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_df.to_csv(\"{}.csv\".format(csv_name), index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487bcf0",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d3b",
   "metadata": {},
   "source": [
    "### Split the References into columns and write to a separate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='processing.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def extract_values(row):\n",
    "    try:\n",
    "        dct_references_s = json.loads(row['dct_references_s'].replace('\"\"', '\"'))\n",
    "        return dct_references_s\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Log the error and the record causing it\n",
    "        logging.error(f'Error processing record with ID: {row[\"layer_slug_s\"]}, Error: {str(e)}')\n",
    "        return None\n",
    "\n",
    "# Apply the function to split the column and expand into separate columns\n",
    "df = pd.concat([df, df.apply(extract_values, axis=1).apply(pd.Series)], axis=1)\n",
    "\n",
    "# Rename columns based on keys in the JSON\n",
    "df = df.rename(columns={\n",
    "    'http://schema.org/downloadUrl': 'download',\n",
    "    'http://schema.org/url': 'full_layer_description',\n",
    "    'http://www.isotc211.org/schemas/2005/gmd/': 'metadata_iso',\n",
    "    'http://www.opengis.net/cat/csw/csdgm': 'metadata_fgdc',\n",
    "    'http://www.w3.org/1999/xhtml': 'metadata_html',\n",
    "    'http://lccn.loc.gov/sh85035852': 'documentation_download',\n",
    "    'http://iiif.io/api/image': 'iiif_image',\n",
    "    'http://iiif.io/api/presentation#manifest': 'iiif_manifest',\n",
    "    'http://www.loc.gov/mods/v3': 'metadata_mods',\n",
    "    'https://openindexmaps.org': 'open_index_map',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wms': 'wms',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wfs': 'wfs',\n",
    "    'urn:x-esri:serviceType:ArcGIS#FeatureLayer': 'arcgis_feature_layer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#TiledMapLayer': 'arcgis_tiled_map_layer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#DynamicMapLayer': 'arcgis_dynamic_map_layer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#ImageMapLayer': 'arcgis_image_map_layer'\n",
    "})\n",
    "\n",
    "# Define the columns in the DataFrame that correspond to distribution types\n",
    "distribution_columns = [\n",
    "    'download', 'documentation_external', 'metadata_iso', 'metadata_fgdc', 'metadata_html',\n",
    "    'documentation_download', 'iiif_image', 'iiif_manifest', 'metadata_mods',\n",
    "    'open_index_map', 'wms', 'wfs', 'arcgis_feature_layer',\n",
    "    'arcgis_tiled_map_layer', 'arcgis_dynamic_map_layer', 'arcgis_image_map_layer'\n",
    "]\n",
    "\n",
    "# Function to check if the value is an array\n",
    "def is_array_type(value):\n",
    "    return isinstance(value, list)\n",
    "\n",
    "# Function to extract the download information for rows with multiple downloads\n",
    "def extract_multiple_downloads(row):\n",
    "    friendlier_id = row[\"ID\"]\n",
    "    downloads = row.get(\"download\", None)\n",
    "    extracted_downloads = []\n",
    "    if is_array_type(downloads):\n",
    "        for download in downloads:\n",
    "            if isinstance(download, dict):\n",
    "                label = download.get(\"label\", \"\")  # Use the label from the array\n",
    "                url = download.get(\"url\", \"\")\n",
    "                extracted_downloads.append({\n",
    "                    \"friendlier_id\": friendlier_id,\n",
    "                    \"label\": label,\n",
    "                    \"reference_type\": \"download\",\n",
    "                    \"distribution_url\": url\n",
    "                })\n",
    "    return extracted_downloads\n",
    "\n",
    "# Prepare a list to store the rows for the new CSV\n",
    "distribution_rows = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    friendlier_id = row['ID']\n",
    "    label = row.get('Format', \"\")  # Default label for single download\n",
    "    \n",
    "    # Handle array-type download values\n",
    "    if is_array_type(row.get('download', None)):\n",
    "        # Extract multiple download links\n",
    "        distribution_rows.extend(extract_multiple_downloads(row))\n",
    "    elif pd.notnull(row.get('download', None)):  # Single download\n",
    "        distribution_rows.append({\n",
    "            'friendlier_id': friendlier_id,\n",
    "            'label': label,  # Use dc_format_s for single download\n",
    "            'reference_type': 'download',\n",
    "            'distribution_url': row['download']\n",
    "        })\n",
    "    \n",
    "    # Process other distribution columns\n",
    "    for col in distribution_columns:\n",
    "        if col != \"download\" and col in df.columns and pd.notnull(row.get(col, None)):\n",
    "            distribution_rows.append({\n",
    "                'friendlier_id': friendlier_id,\n",
    "                'label': \"\",  # Leave blank for non-download rows\n",
    "                'reference_type': col,\n",
    "                'distribution_url': row[col]\n",
    "            })\n",
    "\n",
    "# Create a new DataFrame for the distribution links\n",
    "distribution_df = pd.DataFrame(distribution_rows)\n",
    "\n",
    "# Save the distribution DataFrame to a CSV file\n",
    "distribution_df.to_csv('distributions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
