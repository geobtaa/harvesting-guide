{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edaebb0",
   "metadata": {},
   "source": [
    "## Transform a batch GBL 1.0 JSON files from UW-Madison\n",
    "\n",
    "**Purpose: This script will read a batch of GBL-1.0 metadata JSON files and tranform them into a single CSV.** \n",
    "\n",
    "This is a version that is specific for UW-Madison records\n",
    "\n",
    "Metadata records in the [GeoBlacklight](https://opengeometadata.org/docs/gbl-1.0) or [OpenGeoMetadata](https://opengeometadata.org/docs/ogm-aardvark) standards are frequently shared as batches of JSON files. The entire [OpenGeoMetadata organization](https://github.com/OpenGeoMetadata) contains repositories full of hundreds of thousands of GeoBlacklight JSONs.\n",
    "\n",
    "In order to ingest these into the BTAA Geoportal, we need to transform them into a CSV.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b91c0",
   "metadata": {},
   "source": [
    "## Part 1: Load the modules and JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a33c67",
   "metadata": {},
   "source": [
    "### Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf654768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8083b1",
   "metadata": {},
   "source": [
    "### Declare the paths and file names\n",
    "\n",
    "First, move a folder of the JSONs into this directory. Files in the folder can be nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042cb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r\"/Users/majew030/GitHub/OGM/edu.wisc\" # enter the name of the folder\n",
    "csv_name = \"10d-03\" # create a name for the output CSV without the .csv extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e4301",
   "metadata": {},
   "source": [
    "### Load the files into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7454688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [] # empty list\n",
    "\n",
    "# through all items, format and append to dataset list\n",
    "for path, dir, files in os.walk(json_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            json_file_open = open(file_path, 'rb')\n",
    "            data = json_file_open.read().decode('utf-8', errors='ignore')\n",
    "            loaded = json.loads(data)\n",
    "            dataset.append(loaded)\n",
    "            \n",
    "df = pd.DataFrame(dataset) # convert dataset into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e04a8f",
   "metadata": {},
   "source": [
    "## Part 2: Split multivalued and compound fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b57ef",
   "metadata": {},
   "source": [
    "### Split multivalued fields (arrays)\n",
    "\n",
    "This will remove the punctuation from fields that are formatted as arrays and separate them with pipes ('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dc6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .str.join('') takes each item, whether a list or a single character, and joins them with a pipe\n",
    "\n",
    "df['dc_creator_sm']=df['dc_creator_sm'].str.join('|')\n",
    "df['dc_subject_sm']=df['dc_subject_sm'].str.join('|')\n",
    "df['dct_spatial_sm']=df['dct_spatial_sm'].str.join('|')\n",
    "df['dct_isPartOf_sm']=df['dct_isPartOf_sm'].str.join('|')\n",
    "df['dct_temporal_sm']=df['dct_temporal_sm'].str.join('|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de4050",
   "metadata": {},
   "source": [
    "### Reorder coordinates\n",
    "\n",
    "This will reorder the 4 bbox coordinates into W,S,E,N, which is what the Klokan Bounding Box tool produces on the CSV export option. The BTAA metadata editor uses this order as well when ingesting items. However, Aardvark ultimately uses W,E,N,S, so these would need to be reordered before converting back to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split solr_geom coordinates and reorder from WENS to WSEN\n",
    "df[['w','e','n','s']] = df['solr_geom'].str.strip('ENVELOPE()').str.split(',', expand=True)\n",
    "df['Bounding Box'] = df[['w', 's','e','n']].agg(','.join, axis=1) \n",
    "\n",
    "\n",
    "df = df.drop(columns=['w','e','n','s','solr_geom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487bcf0",
   "metadata": {},
   "source": [
    "# Part 2a Distributions\n",
    "\n",
    "Parse the distribution links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d3b",
   "metadata": {},
   "source": [
    "### Split the References into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb81ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='processing.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def extract_values(row):\n",
    "    try:\n",
    "        dct_references_s = json.loads(row['dct_references_s'].replace('\"\"', '\"'))\n",
    "        return dct_references_s\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Log the error and the record causing it\n",
    "        logging.error(f'Error processing record with ID: {row[\"layer_slug_s\"]}, Error: {str(e)}')\n",
    "        return None\n",
    "\n",
    "# Apply the function to split the column and expand into separate columns\n",
    "df = pd.concat([df, df.apply(extract_values, axis=1).apply(pd.Series)], axis=1)\n",
    "\n",
    "# Rename columns based on keys in the JSON\n",
    "df = df.rename(columns={\n",
    "    'http://schema.org/downloadUrl': 'download',\n",
    "    'http://schema.org/url': 'full_layer_description',\n",
    "    'http://www.isotc211.org/schemas/2005/gmd/': 'metadata_iso',\n",
    "    'http://www.opengis.net/cat/csw/csdgm': 'metadata_fgdc',\n",
    "    'http://www.w3.org/1999/xhtml': 'metadata_html',\n",
    "    'http://lccn.loc.gov/sh85035852': 'documentation_download',\n",
    "    'http://iiif.io/api/image': 'iiif_image',\n",
    "    'http://iiif.io/api/presentation#manifest': 'iiif_manifest',\n",
    "    'http://www.loc.gov/mods/v3': 'metadata_mods',\n",
    "    'https://openindexmaps.org': 'open_index_map',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wms': 'wms',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wfs': 'wfs',\n",
    "    'urn:x-esri:serviceType:ArcGIS#FeatureLayer': 'arcgis_feature_layer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#TiledMapLayer': 'arcgis_tiled_map_layer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#DynamicMapLayer': 'arcgis_dynamic_map_layer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#ImageMapLayer': 'arcgis_image_map_layer'\n",
    "})\n",
    "\n",
    "# df = df.drop(columns=['dct_references_s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1156b29a",
   "metadata": {},
   "source": [
    "### Write the links to a distributions spreadsheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b9fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns in the DataFrame that correspond to distribution types\n",
    "distribution_columns = [\n",
    "    'download', 'full_layer_description', 'metadata_iso', 'metadata_fgdc', 'metadata_html',\n",
    "    'documentation_download', 'iiif_image', 'iiif_manifest', 'metadata_mods',\n",
    "    'open_index_map', 'wms', 'wfs', 'arcgis_feature_layer',\n",
    "    'arcgis_tiled_map_layer', 'arcgis_dynamic_map_layer', 'arcgis_image_map_layer'\n",
    "]\n",
    "\n",
    "# Function to check if the value is an array\n",
    "def is_array_type(value):\n",
    "    return isinstance(value, list)\n",
    "\n",
    "# Function to extract the download information for rows with multiple downloads\n",
    "def extract_multiple_downloads(row):\n",
    "    friendlier_id = row[\"layer_slug_s\"]\n",
    "    downloads = row.get(\"download\", None)\n",
    "    extracted_downloads = []\n",
    "    if is_array_type(downloads):\n",
    "        for download in downloads:\n",
    "            if isinstance(download, dict):\n",
    "                label = download.get(\"label\", \"\")  # Use the label from the array\n",
    "                url = download.get(\"url\", \"\")\n",
    "                extracted_downloads.append({\n",
    "                    \"friendlier_id\": friendlier_id,\n",
    "                    \"label\": label,\n",
    "                    \"reference_type\": \"download\",\n",
    "                    \"distribution_url\": url\n",
    "                })\n",
    "    return extracted_downloads\n",
    "\n",
    "# Prepare a list to store the rows for the new CSV\n",
    "distribution_rows = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    friendlier_id = row['layer_slug_s']\n",
    "    label = row.get('dc_format_s', \"\")  # Default label for single download\n",
    "    \n",
    "    # Handle array-type download values\n",
    "    if is_array_type(row.get('download', None)):\n",
    "        # Extract multiple download links\n",
    "        distribution_rows.extend(extract_multiple_downloads(row))\n",
    "    elif pd.notnull(row.get('download', None)):  # Single download\n",
    "        distribution_rows.append({\n",
    "            'friendlier_id': friendlier_id,\n",
    "            'label': label,  # Use dc_format_s for single download\n",
    "            'reference_type': 'download',\n",
    "            'distribution_url': row['download']\n",
    "        })\n",
    "    \n",
    "    # Process other distribution columns\n",
    "    for col in distribution_columns:\n",
    "        if col != \"download\" and col in df.columns and pd.notnull(row.get(col, None)):\n",
    "            distribution_rows.append({\n",
    "                'friendlier_id': friendlier_id,\n",
    "                'label': \"\",  # Leave blank for non-download rows\n",
    "                'reference_type': col,\n",
    "                'distribution_url': row[col]\n",
    "            })\n",
    "\n",
    "# Create a new DataFrame for the distribution links\n",
    "distribution_df = pd.DataFrame(distribution_rows)\n",
    "\n",
    "# Save the distribution DataFrame to a CSV file\n",
    "distribution_df.to_csv('distribution_links.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492f52e",
   "metadata": {},
   "source": [
    "## Part 3: Transform values for fields without a straight crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43055315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Data Type to Resource Class value\n",
    "df['Resource Class'] = df['dc_type_s'].apply(lambda x: 'Imagery' if x == 'Image' else 'Datasets')\n",
    "\n",
    "\n",
    "#Convert Geometry Type to Resource Type value\n",
    "df['Resource Type'] = df['layer_geom_type_s'].astype(str) + ' data'\n",
    "\n",
    "# Create Date Range field\n",
    "# 10. Handle the \"Date Range\" field\n",
    "# df['Date Range'] = df.apply(lambda row: f\"{row['dct_temporal_sm']}-{row['dct_temporal_sm']}\" if pd.notna(row['dct_temporal_sm']) else '', axis=1)\n",
    "\n",
    "def format_temporal_coverage(row):\n",
    "    temporal_coverage = row['dct_temporal_sm']\n",
    "    \n",
    "    # Check if the value is already a valid date range in yyyy-yyyy format\n",
    "    if pd.notna(temporal_coverage) and re.match(r'\\d{4}-\\d{4}', temporal_coverage):\n",
    "        return temporal_coverage  # Value is already formatted, so no change needed\n",
    "    \n",
    "    # Apply your existing logic to duplicate and format the value\n",
    "    return f\"{temporal_coverage}-{temporal_coverage}\" if pd.notna(temporal_coverage) else ''\n",
    "\n",
    "# Apply the function to create or update the \"Date Range\" column\n",
    "df['Date Range'] = df.apply(format_temporal_coverage, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069cc7b",
   "metadata": {},
   "source": [
    "### Check for GeoTIFFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "579a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if \"GeoTIFF\" is present in the \"dc_format_s\" column\n",
    "def check_geotiff(value):\n",
    "    if pd.notna(value) and \"GeoTIFF\" in value:\n",
    "        return \"true\"\n",
    "    else:\n",
    "        return \"false\"\n",
    "\n",
    "# Create the \"Georeferenced\" column using the check_geotiff function\n",
    "df[\"Georeferenced\"] = df[\"dc_format_s\"].apply(check_geotiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04cfe2a",
   "metadata": {},
   "source": [
    "## Part 4: Custom actions for UW-Madison records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeff38c",
   "metadata": {},
   "source": [
    "### Concatenate custom field 'uw_supplemental_s' to Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fc68b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty (NaN) values in the 'uw_supplemental_s' column with empty strings\n",
    "df['uw_supplemental_s'] = df['uw_supplemental_s'].fillna('')\n",
    "\n",
    "# Concatenate the 'Description' and 'uw_supplemental_s' columns with the pipe separator\n",
    "df['dc_description_s'] = df['dc_description_s'] + '|' + df['uw_supplemental_s']\n",
    "\n",
    "# Drop 'uw_supplemental_s' column\n",
    "df = df.drop(columns=['uw_supplemental_s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77865294",
   "metadata": {},
   "source": [
    "### Convert values in \"dc_subject_sm\" and create a new \"Theme\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75231a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the conversion mappings from old values to new values\n",
    "subject_sm_mapping = {\n",
    "    \"farming\": \"Agriculture\",\n",
    "    \"biota\": \"Biology\",\n",
    "    \"boundaries\": \"Boundaries\",\n",
    "    \"climatologymeteorologyatmosphere\": \"Climate\",\n",
    "    \"economy\": \"Economy\",\n",
    "    \"elevation\": \"Elevation\",\n",
    "    \"environment\": \"Environment\",\n",
    "    \"society; climatologyMeteorologyAtmosphere\": \"Events\",\n",
    "    \"geoscientificinformation\": \"Geology\",\n",
    "    \"health\": \"Health\",\n",
    "    \"imagerybasemapsearthcover\": \"Imagery\",\n",
    "    \"inlandwaters\": \"Inland Waters\",\n",
    "    \"location\": \"Location\",\n",
    "    \"intelligencemilitary\": \"Military\",\n",
    "    \"oceans\": \"Oceans\",\n",
    "    \"planningcadastre\": \"Property\",\n",
    "    \"society\": \"Society\",\n",
    "    \"structure\": \"Structure\",\n",
    "    \"transportation\": \"Transportation\",\n",
    "    \"utilitiescommunication\": \"Utilities\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    subject_values = row['dc_subject_sm']\n",
    "    if pd.notna(subject_values):  # Check for NaN before splitting\n",
    "        subject_values = subject_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in subject_values:\n",
    "            value_lower = value.lower()\n",
    "            if value_lower in subject_sm_mapping:\n",
    "                converted_values.append(subject_sm_mapping[value_lower])\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "df['Theme'] = df.apply(convert_and_join, axis=1)\n",
    "\n",
    "# Drop duplicates from the \"Theme\" column\n",
    "df['Theme'] = df['Theme'].str.split('|').apply(lambda x: '|'.join(sorted(set(x), key=x.index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ede73a",
   "metadata": {},
   "source": [
    "### Define County and City names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85c953c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_in_wisconsin = [\n",
    "    'Adams', 'Ashland', 'Barron', 'Bayfield', 'Brown', 'Buffalo',\n",
    "    'Burnett', 'Calumet', 'Chippewa', 'Clark', 'Columbia', 'Crawford',\n",
    "    'Dane', 'Dodge', 'Door', 'Douglas', 'Dunn', 'Eau Claire',\n",
    "    'Florence', 'Fond du Lac', 'Fond Du Lac', 'Forest', 'Grant', 'Green', 'Green Lake',\n",
    "    'Iowa', 'Iron', 'Jackson', 'Jefferson', 'Juneau', 'Kenosha',\n",
    "    'Kewaunee', 'La Crosse', 'Lacrosse', 'Lafayette', 'Langlade', 'Lincoln',\n",
    "    'Manitowoc', 'Marathon', 'Marinette', 'Marquette', 'Menominee',\n",
    "    'Milwaukee', 'Monroe', 'Oconto', 'Oneida', 'Outagamie', 'Ozaukee',\n",
    "    'Pepin', 'Pierce', 'Polk', 'Portage', 'Price', 'Racine',\n",
    "    'Richland', 'Rock', 'Rusk', 'Sauk', 'Sawyer', 'Shawano',\n",
    "    'Sheboygan', 'St. Croix', 'St Croix', 'Taylor', 'Trempealeau', 'Vernon', 'Vilas',\n",
    "    'Walworth', 'Washburn', 'Washington', 'Waukesha', 'Waupaca',\n",
    "    'Waushara', 'Winnebago', 'Wood'\n",
    "]\n",
    "\n",
    "cities_in_wisconsin = [\n",
    "    'Milwaukee', 'Washington', 'Waukesha', 'Appleton', 'Outagamie', \n",
    "    'Winnebago', 'Eau Claire', 'Fitchburg', 'Fond du Lac', 'Fond Du Lac','Green Bay', 'Janesville', \n",
    "    'Kenosha', 'La Crosse', 'Lacrosse', 'Madison', 'Oshkosh', 'Racine', 'Rhinelander', \n",
    "    'Sheboygan', 'Superior',\n",
    "    'Waukesha', 'Wausau', 'Wauwatosa', 'West Allis', 'Wisconsin Rapids'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb424e6",
   "metadata": {},
   "source": [
    "### Transform the title into \"theme [place] {date}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7bfd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_title(alt_title):\n",
    "    # Function to check if a word is an acronym (three or more capital letters)\n",
    "    def is_acronym(word):\n",
    "        return len(word) >= 3 and word.isupper()\n",
    "\n",
    "    # Split the title into words and apply title casing selectively\n",
    "    words = alt_title.split()\n",
    "    alt_title = ' '.join(word if is_acronym(word) else word.title() for word in words)\n",
    "    \n",
    "    # Sort counties by length (longest first) to match longer names before shorter ones\n",
    "    sorted_counties = sorted(counties_in_wisconsin, key=len, reverse=True)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Search for a city name in the title\n",
    "    city_found = False\n",
    "    for city in cities_in_wisconsin:\n",
    "        if f\"City of {city}\" in alt_title or f\"City Of {city}\" in alt_title:\n",
    "            alt_title = re.sub(f\"City Of {city}, Wi\", f\"[Wisconsin--{city}]\", alt_title, 1)\n",
    "            alt_title = re.sub(f\"City of {city}, Wi\", f\"[Wisconsin--{city}]\", alt_title, 1)\n",
    "            city_found = True\n",
    "            break\n",
    "\n",
    "    # If no city was found, search for a county name\n",
    "    \n",
    "    for county in sorted_counties:\n",
    "        # Using a more flexible regex pattern to match different formats\n",
    "        regex_pattern = fr\"{county} County,?\\s*WI\"\n",
    "        if re.search(regex_pattern, alt_title, re.IGNORECASE):\n",
    "            alt_title = re.sub(regex_pattern, f\"[Wisconsin--{county} County]\", alt_title, flags=re.IGNORECASE)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    alt_title = re.sub(\"Wi \", \"[Wisconsin] \", alt_title, 1)\n",
    "    alt_title = re.sub(\"Wiscsonsin \", \"[Wisconsin] \", alt_title, 1)\n",
    "    alt_title = re.sub(\" Wisconsin \", \" [Wisconsin] \", alt_title, 1)\n",
    "    alt_title = re.sub(\" Wisconsin, \", \" [Wisconsin] \", alt_title, 1)\n",
    "\n",
    "            \n",
    "# Replace a single year or a year range with curly brackets\n",
    "\n",
    "    date_pattern = r\"\\b\\d{4}(?:-\\d{4})?\\b\"\n",
    "    dates = re.findall(date_pattern, alt_title)\n",
    "    if dates:\n",
    "        # Replace the first occurrence of a year or year range with curly brackets\n",
    "        alt_title = re.sub(dates[0], \"{\" + dates[0] + \"}\", alt_title)\n",
    "\n",
    "    # Cleanup phrases post-transformation\n",
    "    alt_title = re.sub(r\",\\s*\\[\", \" [\", alt_title)\n",
    "    alt_title = re.sub(r\",\\s*\\{\", \" {\", alt_title)\n",
    "    alt_title = re.sub(r\":\\s*\\[\", \" [\", alt_title)\n",
    "    alt_title = re.sub(r\"For \\[\", \"[\", alt_title)\n",
    "    alt_title = re.sub(r\"For The \\[\", \"[\", alt_title)\n",
    "    alt_title = re.sub(r\"For The City Of \\[\", \"[\", alt_title)\n",
    "    alt_title = re.sub(r\"Doqqs\", \"DOQQs\", alt_title)\n",
    "    alt_title = re.sub(r\"Lidar\", \"LiDAR\", alt_title)\n",
    "    alt_title = re.sub(r\"For City Of Superior \\(Douglas County\\) \\[Wisconsin\\]\", \"[Wisconsin--Superior]\", alt_title)\n",
    "    return alt_title\n",
    "\n",
    "def move_place_and_date(title):\n",
    "    # Move content in square brackets to the end\n",
    "    square_bracket_pattern = r\"\\[.*?\\]\"\n",
    "    match = re.search(square_bracket_pattern, title)\n",
    "    if match:\n",
    "        content_in_square_brackets = match.group()\n",
    "        title = re.sub(square_bracket_pattern, \"\", title).strip()\n",
    "        title = f\"{title} {content_in_square_brackets}\"\n",
    "\n",
    "    # Move content in curly brackets to the end\n",
    "    curly_bracket_pattern = r\"\\{.*?\\}\"\n",
    "    match = re.search(curly_bracket_pattern, title)\n",
    "    if match:\n",
    "        content_in_curly_brackets = match.group()\n",
    "        title = re.sub(curly_bracket_pattern, \"\", title).strip()\n",
    "        title = f\"{title} {content_in_curly_brackets}\"\n",
    "\n",
    "    return title\n",
    "\n",
    "\n",
    "df['Title'] = df['dc_title_s'].apply(transform_title)\n",
    "df['Title'] = df['Title'].apply(move_place_and_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8026e01",
   "metadata": {},
   "source": [
    "### Use the newly formated to extract place names for the Spatial Coverage field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11d63c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spatial_coverage(title):\n",
    "    coverage = re.search(r'\\[(.*?)\\]', title)\n",
    "    if coverage:\n",
    "        coverage = coverage.group(1)\n",
    "        return coverage if coverage.endswith('Wisconsin') else coverage + \"|Wisconsin\"\n",
    "    return \"Wisconsin\"\n",
    "\n",
    "# Apply the function to the \"Title\" column and assign the results to the \"Spatial Coverage\" column.\n",
    "df['Spatial Coverage'] = df['Title'].apply(extract_spatial_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c17cbd",
   "metadata": {},
   "source": [
    "### Retain the original title in the Alternative Title field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d1d3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'dc_title_s' : 'Alternative Title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda6958",
   "metadata": {},
   "source": [
    "### Update the Creator field into the FAST format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fb35b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_creator(creator):\n",
    "    # Dictionary mapping of creators for direct transformation\n",
    "    creator_mappings = {\n",
    "        \"United States General Land Office\" : \"United States. General Land Office\",\n",
    "        \"U.S. Geological Survey\" : \"Geological Survey (U.S.)\",\n",
    "        \"Wisconsin Department of Transportation\" : \"Wisconsin. Department of Transportation\",\n",
    "        \"U.S. Department of Agriculture\" : \"United States. Department of Agriculture\",\n",
    "        \"Wisconsin Department of Natural Resources\" : \"Wisconsin. Department of Natural Resources\",\n",
    "        \"Wisconsin Department of Administration\" : \"Wisconsin. Department of Administration\"\n",
    "    }\n",
    "    \n",
    "    # If a direct mapping is found, return the transformed value\n",
    "    if creator in creator_mappings:\n",
    "        return creator_mappings[creator]\n",
    "    \n",
    "    # Check for mixed values that start with the same string\n",
    "#     if creator.startswith(\"Wisconsin Department of Tra\"):\n",
    "#         return \"Wisconsin. Department of Transportation\"\n",
    "    \n",
    "    # Search for a county name in the publisher string.\n",
    "    for county in counties_in_wisconsin:\n",
    "        if county + \" County\" in creator:\n",
    "            return f\"Wisconsin--{county} County\"\n",
    "    else:\n",
    "        for city in cities_in_wisconsin:\n",
    "            if f\"City of {city}\" in creator or city == creator:\n",
    "                return f\"Wisconsin--{city}\"\n",
    "    \n",
    "    # If no match found, return the original publisher string.\n",
    "    return creator\n",
    "\n",
    "df['Creator'] = df['dc_creator_sm'].apply(transform_creator)\n",
    "df = df.drop(columns=['dc_creator_sm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e084b1",
   "metadata": {},
   "source": [
    "## Part 4: Export to a new CSV with Aardvark labels as headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080a2f7",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02c55c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'geoblacklight_version',\n",
    "    'layer_modified_dt',  \n",
    "    'solr_year_i',\n",
    "    'layer_geom_type_s',\n",
    "    'dc_language_s',\n",
    "    'dc_identifier_s'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d5e72",
   "metadata": {},
   "source": [
    "### Add some fields with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50f39dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date in yyyy-mm-dd format\n",
    "today_date = datetime.date.today().isoformat()\n",
    "\n",
    "# Add the \"Date Accessioned\" column with the today's date value to the DataFrame\n",
    "df['Date Accessioned'] = today_date\n",
    "df['Code'] = \"10\"\n",
    "df['Is Part Of'] = \"10d-03\"\n",
    "df['Member Of'] = \"dc8c18df-7d64-4ff4-a754-d18d0891187d\"\n",
    "df['Accrual Method'] = \"GBL-1.0\"\n",
    "df['Language'] = \"eng\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06042ee7",
   "metadata": {},
   "source": [
    "### Rename the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5c8d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={ \n",
    "    'dc_description_s': 'Description',\n",
    "    'dct_issued_s': 'Date Issued',\n",
    "    'dc_rights_s' : 'Access Rights',\n",
    "    'dc_format_s': 'Format',\n",
    "    'layer_slug_s' : 'ID',\n",
    "    'layer_id_s' : 'WxS Identifier', \n",
    "    'dct_provenance_s' : 'Provider',\n",
    "    'dc_publisher_s' : 'Publisher',\n",
    "    'dc_publisher_sm' : 'Publisher',\n",
    "    'dc_source_sm' : 'Source',\n",
    "    'dct_temporal_sm' : 'Temporal Coverage',\n",
    "    'dct_isPartOf_sm' : 'Keyword',\n",
    "    'uw_notice_s' : 'Display Note'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2dadcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Identifier'] = \"https://geodata.wisc.edu/catalog/\" + df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bfc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_pipes_and_spaces(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove leading and trailing pipes and spaces\n",
    "        value = value.strip('| ').strip('| ')\n",
    "        # Replace double or more spaces with a single space\n",
    "        value = re.sub(r'\\s{2,}', ' ', value)\n",
    "        return value\n",
    "    return value\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "df = df.map(trim_pipes_and_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38345bf",
   "metadata": {},
   "source": [
    "### Write the DataFrame to a CSV file with Aardvark labels\n",
    "This can be uploaded to GBL Admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "79bd3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Publisher',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Theme',\n",
    "'Subject',\n",
    "'Keyword',\n",
    "'Temporal Coverage',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Geometry',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'Source',\n",
    "'Format',\n",
    "'WxS Identifier',\n",
    "'Georeferenced',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Rights Holder',\n",
    "'License',\n",
    "'Access Rights',\n",
    "'Suppressed',\n",
    "'Child Record',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method'\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5842da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}.csv\".format(csv_name), index=False, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
