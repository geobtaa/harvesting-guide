{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edaebb0",
   "metadata": {},
   "source": [
    "## Transform a batch GBL 1.0 JSON files from UW-Madison\n",
    "\n",
    "**Purpose: This script will read a batch of GBL-1.0 metadata JSON files and tranform them into a single CSV.** \n",
    "\n",
    "This is a version that is specific for UW-Madison records\n",
    "\n",
    "Metadata records in the [GeoBlacklight](https://opengeometadata.org/docs/gbl-1.0) or [OpenGeoMetadata](https://opengeometadata.org/docs/ogm-aardvark) standards are frequently shared as batches of JSON files. The entire [OpenGeoMetadata organization](https://github.com/OpenGeoMetadata) contains repositories full of hundreds of thousands of GeoBlacklight JSONs.\n",
    "\n",
    "In order to ingest these into the BTAA Geoportal, we need to transform them into a CSV.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b91c0",
   "metadata": {},
   "source": [
    "## Part 1: Load the modules and JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a33c67",
   "metadata": {},
   "source": [
    "### Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "cf654768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8083b1",
   "metadata": {},
   "source": [
    "### Declare the paths and file names\n",
    "\n",
    "First, move a folder of the JSONs into this directory. Files in the folder can be nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "042cb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r\"edu.wisc\" # enter the name of the folder\n",
    "csv_name = \"wiscview\" # create a name for the output CSV without the .csv extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e4301",
   "metadata": {},
   "source": [
    "### Load the files into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "d7454688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [] # empty list\n",
    "\n",
    "# through all items, format and append to dataset list\n",
    "for path, dir, files in os.walk(json_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            json_file_open = open(file_path, 'rb')\n",
    "            data = json_file_open.read().decode('utf-8', errors='ignore')\n",
    "            loaded = json.loads(data)\n",
    "            dataset.append(loaded)\n",
    "            \n",
    "df = pd.DataFrame(dataset) # convert dataset into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e04a8f",
   "metadata": {},
   "source": [
    "## Part 2: Split multivalued and compound fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b57ef",
   "metadata": {},
   "source": [
    "### Split multivalued fields (arrays)\n",
    "\n",
    "This will remove the punctuation from fields that are formatted as arrays and separate them with pipes ('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "b9dc6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .str.join('') takes each item, whether a list or a single character, and joins them with a pipe\n",
    "\n",
    "df['dc_creator_sm']=df['dc_creator_sm'].str.join('|')\n",
    "df['dc_subject_sm']=df['dc_subject_sm'].str.join('|')\n",
    "df['dct_spatial_sm']=df['dct_spatial_sm'].str.join('|')\n",
    "df['dct_isPartOf_sm']=df['dct_isPartOf_sm'].str.join('|')\n",
    "df['dct_temporal_sm']=df['dct_temporal_sm'].str.join('|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d3b",
   "metadata": {},
   "source": [
    "### Split the References into separate columns\n",
    "\n",
    "This step makes it easier to edit individual links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "eb81ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='processing.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def extract_values(row):\n",
    "#     dct_references_s = json.loads(row['dct_references_s'].replace('\"\"', '\"'))\n",
    "# #     dct_references_s = json.loads(row['dct_references_s'])\n",
    "#     return dct_references_s\n",
    "\n",
    "    try:\n",
    "        dct_references_s = json.loads(row['dct_references_s'].replace('\"\"', '\"'))\n",
    "        return dct_references_s\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Log the error and the record causing it\n",
    "        logging.error(f'Error processing record with ID: {row[\"layer_slug_s\"]}, Error: {str(e)}')\n",
    "        return None\n",
    "\n",
    "# Apply the function to split the column and expand into separate columns\n",
    "df = pd.concat([df, df.apply(extract_values, axis=1).apply(pd.Series)], axis=1)\n",
    "\n",
    "# Rename columns based on keys in the JSON\n",
    "df = df.rename(columns={\n",
    "    'http://schema.org/downloadUrl': 'Download',\n",
    "    'http://schema.org/url': 'Information',\n",
    "    'http://www.isotc211.org/schemas/2005/gmd/': 'ISO19139',\n",
    "    'http://www.opengis.net/cat/csw/csdgm': 'FGDC',\n",
    "    'http://www.w3.org/1999/xhtml': 'HTML',\n",
    "    'http://lccn.loc.gov/sh85035852': 'Documentation',\n",
    "    'http://iiif.io/api/image': 'IIIF',\n",
    "    'http://iiif.io/api/presentation#manifest': 'Manifest',\n",
    "    'http://www.loc.gov/mods/v3': 'MODS',\n",
    "    'https://openindexmaps.org': 'Index Map',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wms': 'WMS',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wfs': 'WFS',\n",
    "    'urn:x-esri:serviceType:ArcGIS#FeatureLayer': 'FeatureServer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#TiledMapLayer': 'TileServer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#DynamicMapLayer': 'MapServer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#ImageMapLayer': 'ImageServer',\n",
    "    'http://schema.org/DownloadAction': 'Harvard Download'\n",
    "    # Add more key-value pairs for renaming columns as needed\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de4050",
   "metadata": {},
   "source": [
    "### Reorder coordinates\n",
    "\n",
    "This will reorder the 4 bbox coordinates into W,S,E,N, which is what the Klokan Bounding Box tool produces on the CSV export option. The BTAA metadata editor uses this order as well when ingesting items. However, Aardvark ultimately uses W,E,N,S, so these would need to be reordered before converting back to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "ae0dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split solr_geom coordinates and reorder from WENS to WSEN\n",
    "df[['w','e','n','s']] = df['solr_geom'].str.strip('ENVELOPE()').str.split(',', expand=True)\n",
    "df['Bounding Box'] = df[['w', 's','e','n']].agg(','.join, axis=1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492f52e",
   "metadata": {},
   "source": [
    "## Part 3: Transform values for fields without a straight crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "43055315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Data Type to Resource Class value\n",
    "df['Resource Class'] = df['dc_type_s'].apply(lambda x: 'Imagery' if x == 'Image' else 'Datasets')\n",
    "\n",
    "\n",
    "#Convert Geometry Type to Resource Type value\n",
    "df['Resource Type'] = df['layer_geom_type_s'].astype(str) + ' data'\n",
    "\n",
    "# Create Date Range field\n",
    "# 10. Handle the \"Date Range\" field\n",
    "# df['Date Range'] = df.apply(lambda row: f\"{row['dct_temporal_sm']}-{row['dct_temporal_sm']}\" if pd.notna(row['dct_temporal_sm']) else '', axis=1)\n",
    "\n",
    "def format_temporal_coverage(row):\n",
    "    temporal_coverage = row['dct_temporal_sm']\n",
    "    \n",
    "    # Check if the value is already a valid date range in yyyy-yyyy format\n",
    "    if pd.notna(temporal_coverage) and re.match(r'\\d{4}-\\d{4}', temporal_coverage):\n",
    "        return temporal_coverage  # Value is already formatted, so no change needed\n",
    "    \n",
    "    # Apply your existing logic to duplicate and format the value\n",
    "    return f\"{temporal_coverage}-{temporal_coverage}\" if pd.notna(temporal_coverage) else ''\n",
    "\n",
    "# Apply the function to create or update the \"Date Range\" column\n",
    "df['Date Range'] = df.apply(format_temporal_coverage, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069cc7b",
   "metadata": {},
   "source": [
    "### Check for GeoTIFFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "579a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if \"GeoTIFF\" is present in the \"dc_format_s\" column\n",
    "def check_geotiff(value):\n",
    "    if pd.notna(value) and \"GeoTIFF\" in value:\n",
    "        return \"true\"\n",
    "    else:\n",
    "        return \"false\"\n",
    "\n",
    "# Create the \"Georeferenced\" column using the check_geotiff function\n",
    "df[\"Georeferenced\"] = df[\"dc_format_s\"].apply(check_geotiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeff38c",
   "metadata": {},
   "source": [
    "### Concatenate custom field 'uw_supplemental_s' to Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "2fc68b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty (NaN) values in the 'uw_supplemental_s' column with empty strings\n",
    "df['uw_supplemental_s'] = df['uw_supplemental_s'].fillna('')\n",
    "\n",
    "# Concatenate the 'Description' and 'uw_supplemental_s' columns with the pipe separator\n",
    "df['dc_description_s'] = df['dc_description_s'] + '|' + df['uw_supplemental_s']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77865294",
   "metadata": {},
   "source": [
    "### Convert values in \"dc_subject_sm\" and create a new \"Theme\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "d75231a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the conversion mappings from old values to new values\n",
    "subject_sm_mapping = {\n",
    "    \"farming\": \"Agriculture\",\n",
    "    \"biota\": \"Biology\",\n",
    "    \"boundaries\": \"Boundaries\",\n",
    "    \"climatologymeteorologyatmosphere\": \"Climate\",\n",
    "    \"economy\": \"Economy\",\n",
    "    \"elevation\": \"Elevation\",\n",
    "    \"environment\": \"Environment\",\n",
    "    \"society; climatologyMeteorologyAtmosphere\": \"Events\",\n",
    "    \"geoscientificinformation\": \"Geology\",\n",
    "    \"health\": \"Health\",\n",
    "    \"imagerybasemapsearthcover\": \"Imagery\",\n",
    "    \"inlandwaters\": \"Inland Waters\",\n",
    "    \"location\": \"Location\",\n",
    "    \"intelligencemilitary\": \"Military\",\n",
    "    \"oceans\": \"Oceans\",\n",
    "    \"planningcadastre\": \"Property\",\n",
    "    \"society\": \"Society\",\n",
    "    \"structure\": \"Structure\",\n",
    "    \"transportation\": \"Transportation\",\n",
    "    \"utilitiescommunication\": \"Utilities\"\n",
    "    \n",
    "    # Add more key-value pairs for other conversions as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    subject_values = row['dc_subject_sm']\n",
    "    if pd.notna(subject_values):  # Check for NaN before splitting\n",
    "        subject_values = subject_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in subject_values:\n",
    "            value_lower = value.lower()\n",
    "            if value_lower in subject_sm_mapping:\n",
    "                converted_values.append(subject_sm_mapping[value_lower])\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "df['Theme'] = df.apply(convert_and_join, axis=1)\n",
    "\n",
    "# Drop duplicates from the \"Theme\" column\n",
    "df['Theme'] = df['Theme'].str.split('|').apply(lambda x: '|'.join(sorted(set(x), key=x.index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e084b1",
   "metadata": {},
   "source": [
    "## Part 4: Export to a new CSV with Aardvark labels as headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080a2f7",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "02c55c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'geoblacklight_version',\n",
    "    'layer_modified_dt', \n",
    "#     'thumbnail_path_ss',\n",
    "    'w','e','n','s', \n",
    "    'solr_year_i',\n",
    "    'layer_geom_type_s',\n",
    "    'solr_geom',\n",
    "    'dct_references_s',\n",
    "    'uw_supplemental_s'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d5e72",
   "metadata": {},
   "source": [
    "### Add some fields with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "50f39dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date in yyyy-mm-dd format\n",
    "today_date = datetime.date.today().isoformat()\n",
    "\n",
    "# Add the \"Date Accessioned\" column with the today's date value to the DataFrame\n",
    "df['Date Accessioned'] = today_date\n",
    "df['Code'] = \"10\"\n",
    "df['Is Part Of'] = \"10d-03\"\n",
    "df['Member Of'] = \"dc8c18df-7d64-4ff4-a754-d18d0891187d\"\n",
    "df['Accrual Method'] = \"GBL-1.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06042ee7",
   "metadata": {},
   "source": [
    "### Rename the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "b5c8d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'dc_title_s': 'Alternative Title', \n",
    "    'dc_description_s': 'Description',\n",
    "    'dc_creator_sm': 'Creator',\n",
    "    'dct_issued_s': 'Date Issued',\n",
    "    'dct_issued_dt': 'Date Issued',\n",
    "    'dc_rights_s' : 'Access Rights',\n",
    "    'dc_format_s': 'Format',\n",
    "    'layer_slug_s' : 'ID',\n",
    "    'layer_id_s' : 'WxS Identifier', \n",
    "#     'dc_identifier_s' : 'Identifier',\n",
    "    'dc_language_s' : 'Language',\n",
    "    'dct_provenance_s' : 'Provider',\n",
    "    'dc_publisher_s' : 'Publisher',\n",
    "    'dc_publisher_sm' : 'Publisher',\n",
    "    'dc_source_sm' : 'Source',\n",
    "    'dct_spatial_sm' : 'Spatial Coverage',\n",
    "    'dct_temporal_sm' : 'Temporal Coverage',\n",
    "    'dct_isPartOf_sm' : 'Keyword',\n",
    "    'uw_notice_s' : 'Display Note'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "e2dadcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Identifier'] = \"https://geodata.wisc.edu/catalog/\" + df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "7d7bfd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_in_wisconsin = [\n",
    "    'Adams', 'Ashland', 'Barron', 'Bayfield', 'Brown', 'Buffalo',\n",
    "    'Burnett', 'Calumet', 'Chippewa', 'Clark', 'Columbia', 'Crawford',\n",
    "    'Dane', 'Dodge', 'Door', 'Douglas', 'Dunn', 'Eau Claire',\n",
    "    'Florence', 'Fond du Lac', 'Fond Du Lac', 'Forest', 'Grant', 'Green', 'Green Lake',\n",
    "    'Iowa', 'Iron', 'Jackson', 'Jefferson', 'Juneau', 'Kenosha',\n",
    "    'Kewaunee', 'La Crosse', 'Lafayette', 'Langlade', 'Lincoln',\n",
    "    'Manitowoc', 'Marathon', 'Marinette', 'Marquette', 'Menominee',\n",
    "    'Milwaukee', 'Monroe', 'Oconto', 'Oneida', 'Outagamie', 'Ozaukee',\n",
    "    'Pepin', 'Pierce', 'Polk', 'Portage', 'Price', 'Racine',\n",
    "    'Richland', 'Rock', 'Rusk', 'Sauk', 'Sawyer', 'Shawano',\n",
    "    'Sheboygan', 'St. Croix', 'Taylor', 'Trempealeau', 'Vernon', 'Vilas',\n",
    "    'Walworth', 'Washburn', 'Washington', 'Waukesha', 'Waupaca',\n",
    "    'Waushara', 'Winnebago', 'Wood'\n",
    "]\n",
    "\n",
    "cities_in_wisconsin = [\n",
    "    'Milwaukee', 'Washington', 'Waukesha', 'Appleton', 'Outagamie', \n",
    "    'Winnebago', 'Eau Claire', 'Fond du Lac', 'Green Bay', 'Janesville', \n",
    "    'Kenosha', 'La Crosse', 'Madison', 'Oshkosh', 'Racine', 'Sheboygan', \n",
    "    'Waukesha', 'Wausau', 'Wauwatosa', 'West Allis'\n",
    "]\n",
    "\n",
    "\n",
    "def transform_title(alt_title):\n",
    "    # Function to check if a word is an acronym (three or more capital letters)\n",
    "    def is_acronym(word):\n",
    "        return len(word) >= 3 and word.isupper()\n",
    "\n",
    "    # Split the title into words and apply title casing selectively\n",
    "    words = alt_title.split()\n",
    "    alt_title = ' '.join(word if is_acronym(word) else word.title() for word in words)\n",
    "\n",
    "    # Search for a city or county name in the title.\n",
    "    for county in counties_in_wisconsin:\n",
    "        if county in alt_title:\n",
    "            alt_title = re.sub(f\"{county} County, Wi\", f\"[Wisconsin--{county} County]\", alt_title, 1)\n",
    "            break\n",
    "    else:\n",
    "        for city in cities_in_wisconsin:\n",
    "            if city in alt_title:\n",
    "                alt_title = re.sub(f\"City Of {city}, Wi\", f\"[Wisconsin--{city}]\", alt_title, 1)\n",
    "                break\n",
    "        else:\n",
    "            alt_title = re.sub(\"Wi \", \"[Wisconsin] \", alt_title, 1)\n",
    "\n",
    "    # Replace the year.\n",
    "    year = re.findall(r\"\\b\\d{4}\\b\", alt_title)\n",
    "    if year:\n",
    "        alt_title = re.sub(year[0], \"{\"+year[0]+\"}\", alt_title)\n",
    "\n",
    "    # Cleanup phrases post-transformation\n",
    "    alt_title = re.sub(r\",\\s*\\[\", \" [\", alt_title)\n",
    "    alt_title = re.sub(r\"For \\[\", \"[\", alt_title)\n",
    "    alt_title = re.sub(r\"For The \\[\", \"[\", alt_title)\n",
    "    alt_title = re.sub(r\"For The City Of \\[\", \"[\", alt_title)\n",
    "#     alt_title = re.sub(r\"Plss\", \"PLSS\", alt_title)\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "df['Title'] = df['Alternative Title'].apply(transform_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "11d63c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spatial_coverage(title):\n",
    "    coverage = re.search(r'\\[(.*?)\\]', title)\n",
    "    if coverage:\n",
    "        coverage = coverage.group(1)\n",
    "        return coverage if coverage.endswith('Wisconsin') else coverage + \"|Wisconsin\"\n",
    "    return \"Wisconsin\"\n",
    "\n",
    "# Apply the function to the \"Title\" column and assign the results to the \"Spatial Coverage\" column.\n",
    "df['Spatial Coverage'] = df['Title'].apply(extract_spatial_coverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "58b85f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_pipes_and_spaces(value):\n",
    "    if isinstance(value, str):\n",
    "        return value.strip('| ').strip('| ')\n",
    "    return value\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "df = df.applymap(trim_pipes_and_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "79bd3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Publisher',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Theme',\n",
    "'Subject',\n",
    "'Keyword',\n",
    "'Temporal Coverage',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Geometry',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'Source',\n",
    "'Format',\n",
    "'WxS Identifier',\n",
    "'Georeferenced',\n",
    "'Documentation',\n",
    "'Download',\n",
    "'FeatureServer',\n",
    "'FGDC',\n",
    "'Harvard Download',\n",
    "'HTML',\n",
    "'IIIF',\n",
    "'ImageServer',\n",
    "'Information',\n",
    "'ISO19139',\n",
    "'Manifest',\n",
    "'MapServer',\n",
    "'MODS',\n",
    "'oEmbed',\n",
    "'Index Map',\n",
    "'TileServer',\n",
    "'WCS',\n",
    "'WFS',\n",
    "'WMS',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Rights Holder',\n",
    "'License',\n",
    "'Access Rights',\n",
    "'Suppressed',\n",
    "'Child Record',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method'\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487bcf0",
   "metadata": {},
   "source": [
    "### Check for multiple downloads and create a secondary CSV called \"multiple-downloads.csv\"\n",
    "\n",
    "See https://geobtaa.github.io/metadata/recipes/secondary-tables/ for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "25826b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if the value is an array\n",
    "def is_array_type(value):\n",
    "    return isinstance(value, list)\n",
    "\n",
    "# Function to extract the download information and write to \"multiple-downloads.csv\"\n",
    "def extract_downloads(row):\n",
    "    friendlier_id = row[\"ID\"]\n",
    "    downloads = row[\"Download\"]\n",
    "    extracted_downloads = []\n",
    "    if is_array_type(downloads):\n",
    "        for download in downloads:\n",
    "            if isinstance(download, dict):\n",
    "                label = download.get(\"label\", \"\")\n",
    "                value = download.get(\"url\", \"\")\n",
    "                extracted_downloads.append({\"friendlier_id\": friendlier_id, \"label\": label, \"value\": value})\n",
    "\n",
    "    return extracted_downloads\n",
    "\n",
    "# Apply the function to each row in the DataFrame where \"Download\" is an array\n",
    "download_list = df[df[\"Download\"].apply(is_array_type)].apply(extract_downloads, axis=1).explode().tolist()\n",
    "\n",
    "# Write the extracted downloads to \"multiple-downloads.csv\"\n",
    "with open(\"multiple-downloads.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"friendlier_id\", \"label\", \"value\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(download_list)\n",
    "\n",
    "# Update the \"Download\" column in the main DataFrame (df) to remove array-type values\n",
    "df.loc[df[\"Download\"].apply(is_array_type), \"Download\"] = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38345bf",
   "metadata": {},
   "source": [
    "### Write the DataFrame to a CSV file with Aardvark labels\n",
    "This can be uploaded to GEOMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5842da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}.csv\".format(csv_name), index=False, na_rep='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
