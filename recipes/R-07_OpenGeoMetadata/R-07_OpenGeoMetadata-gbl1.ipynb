{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edaebb0",
   "metadata": {},
   "source": [
    "## Transform a batch OpenGeoMetadata JSON files\n",
    "\n",
    "**Purpose: This script will read a batch of GeoBlacklight metadata JSON files in the GBL 1.0 schema and tranform them into a single CSV.** \n",
    "\n",
    "Metadata records in the [GBL-1.0](https://opengeometadata.org/docs/gbl-1.0) schema are frequently shared as batches of JSON files. The entire [OpenGeoMetadata organization](https://github.com/OpenGeoMetadata) contains repositories full of hundreds of thousands of GeoBlacklight JSONs.\n",
    "\n",
    "In order to ingest these into the BTAA Geoportal, we need to transform them into a CSV.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443b91c0",
   "metadata": {},
   "source": [
    "## Part 1: Load the modules and JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a33c67",
   "metadata": {},
   "source": [
    "### Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "cf654768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8083b1",
   "metadata": {},
   "source": [
    "### Declare the paths and file names\n",
    "\n",
    "First, move a folder of the JSONs into this directory. Files in the folder can be nested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "042cb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r\"solr\" # enter the name of the folder\n",
    "csv_name = \"fixtures\" # create a name for the output CSV without the .csv extension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e4301",
   "metadata": {},
   "source": [
    "### Load the files into a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "d7454688",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [] # empty list\n",
    "\n",
    "# through all items, format and append to dataset list\n",
    "for path, dir, files in os.walk(json_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            json_file_open = open(file_path, 'rb')\n",
    "            data = json_file_open.read().decode('utf-8', errors='ignore')\n",
    "            loaded = json.loads(data)\n",
    "            dataset.append(loaded)\n",
    "            \n",
    "df = pd.DataFrame(dataset) # convert dataset into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e04a8f",
   "metadata": {},
   "source": [
    "## Part 2: Split multivalued and compound fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b57ef",
   "metadata": {},
   "source": [
    "### Split multivalued fields (arrays)\n",
    "\n",
    "This will remove the punctuation from fields that are formatted as arrays and separate them with pipes ('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "b9dc6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .str.join('') takes each item, whether a list or a single character, and joins them with a pipe\n",
    "\n",
    "df['dc_creator_sm']=df['dc_creator_sm'].str.join('|')\n",
    "df['dc_subject_sm']=df['dc_subject_sm'].str.join('|')\n",
    "df['dct_spatial_sm']=df['dct_spatial_sm'].str.join('|')\n",
    "df['dct_isPartOf_sm']=df['dct_isPartOf_sm'].str.join('|')\n",
    "df['dct_temporal_sm']=df['dct_temporal_sm'].str.join('|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c5d3b",
   "metadata": {},
   "source": [
    "### Split the References into separate columns\n",
    "\n",
    "This step makes it easier to edit individual links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "eb81ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(row):\n",
    "    dct_references_s = json.loads(row['dct_references_s'].replace('\"\"', '\"'))\n",
    "    return dct_references_s\n",
    "\n",
    "# Apply the function to split the column and expand into separate columns\n",
    "df = pd.concat([df, df.apply(extract_values, axis=1).apply(pd.Series)], axis=1)\n",
    "\n",
    "# Rename columns based on keys in the JSON\n",
    "df = df.rename(columns={\n",
    "    'http://schema.org/downloadUrl': 'Download',\n",
    "    'http://schema.org/url': 'Information',\n",
    "    'http://www.isotc211.org/schemas/2005/gmd/': 'ISO19139',\n",
    "    'http://www.opengis.net/cat/csw/csdgm': 'FGDC',\n",
    "    'http://www.w3.org/1999/xhtml': 'HTML',\n",
    "    'http://lccn.loc.gov/sh85035852': 'Documentation',\n",
    "    'http://iiif.io/api/image': 'IIIF',\n",
    "    'http://iiif.io/api/presentation#manifest': 'Manifest',\n",
    "    'http://www.loc.gov/mods/v3': 'MODS',\n",
    "    'https://openindexmaps.org': 'Index Map',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wms': 'WMS',\n",
    "    'http://www.opengis.net/def/serviceType/ogc/wfs': 'WFS',\n",
    "    'urn:x-esri:serviceType:ArcGIS#FeatureLayer': 'FeatureServer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#TiledMapLayer': 'TileServer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#DynamicMapLayer': 'MapServer',\n",
    "    'urn:x-esri:serviceType:ArcGIS#ImageMapLayer': 'ImageServer',\n",
    "    'http://schema.org/DownloadAction': 'Harvard Download'\n",
    "    # Add more key-value pairs for renaming columns as needed\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de4050",
   "metadata": {},
   "source": [
    "### Reorder coordinates\n",
    "\n",
    "This will reorder the 4 bbox coordinates into W,S,E,N, which is what the Klokan Bounding Box tool produces on the CSV export option. The BTAA metadata editor uses this order as well when ingesting items. However, Aardvark ultimately uses W,E,N,S, so these would need to be reordered before converting back to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "ae0dc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split solr_geom coordinates and reorder from WENS to WSEN\n",
    "df[['w','e','n','s']] = df['solr_geom'].str.strip('ENVELOPE()').str.split(',', expand=True)\n",
    "df['Bounding Box'] = df[['w', 's','e','n']].agg(','.join, axis=1) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492f52e",
   "metadata": {},
   "source": [
    "## Part 3: Transform values for fields without a straight crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "43055315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Data Type to Resource Class value\n",
    "df['Resource Class'] = df['dc_type_s'].apply(lambda x: 'Datasets' if x == 'Dataset' else '')\n",
    "\n",
    "#Convert Geometry Type to Resource Type value\n",
    "df['Resource Type'] = df['layer_geom_type_s'].astype(str) + ' data'\n",
    "\n",
    "# Create Date Range field\n",
    "df['Date Range'] = df.apply(lambda row: f\"{row['dct_temporal_sm']}-{row['dct_temporal_sm']}\" if pd.notna(row['dct_temporal_sm']) else '', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069cc7b",
   "metadata": {},
   "source": [
    "### Check for GeoTIFFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "579a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if \"GeoTIFF\" is present in the \"dc_format_s\" column\n",
    "def check_geotiff(value):\n",
    "    if pd.notna(value) and \"GeoTIFF\" in value:\n",
    "        return \"true\"\n",
    "    else:\n",
    "        return \"false\"\n",
    "\n",
    "# Create the \"Georeferenced\" column using the check_geotiff function\n",
    "df[\"Georeferenced\"] = df[\"dc_format_s\"].apply(check_geotiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e528fac",
   "metadata": {},
   "source": [
    "### Extract Is Part Of strings and create new Collection level records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "fc2547c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n",
      "/var/folders/h1/wds8r6757dz_4h_l5tz85gyw0000gq/T/ipykernel_13518/2252186561.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(new_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "processed_collections = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    dct_isPartOf_sm = row['dct_isPartOf_sm']\n",
    "    if pd.notna(dct_isPartOf_sm):\n",
    "        if isinstance(dct_isPartOf_sm, str):\n",
    "            title = dct_isPartOf_sm.strip()\n",
    "            if title not in processed_collections:\n",
    "                new_id = str(uuid.uuid4())  # Generating a new UUID as the ID for the collection\n",
    "                new_row = {\n",
    "                    'dc_title_s': title,\n",
    "                    'layer_slug_s': new_id,\n",
    "                    'Resource Class': 'Collections',\n",
    "                    'dc_rights_s': 'Public'\n",
    "                }\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "                processed_collections[title] = new_id\n",
    "        elif isinstance(dct_isPartOf_sm, list):\n",
    "            for title in dct_isPartOf_sm:\n",
    "                title = title.strip()\n",
    "                if title not in processed_collections:\n",
    "                    new_id = str(uuid.uuid4())  # Generating a new UUID as the ID for each collection\n",
    "                    new_row = {\n",
    "                        'dc_title_s': title,\n",
    "                        'layer_slug_s': new_id,\n",
    "                        'Resource Class': 'Collections',\n",
    "                        'dc_rights_s': 'Public'\n",
    "                    }\n",
    "                    df = df.append(new_row, ignore_index=True)\n",
    "                    processed_collections[title] = new_id\n",
    "                    \n",
    "# Append the new rows to the DataFrame\n",
    "df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77865294",
   "metadata": {},
   "source": [
    "### Convert values in \"dc_subject_sm\" and create a new \"Theme\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "d75231a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the conversion mappings from old values to new values\n",
    "subject_sm_mapping = {\n",
    "    \"farming\": \"Agriculture\",\n",
    "    \"biota\": \"Biology\",\n",
    "    \"boundaries\": \"Boundaries\",\n",
    "    \"climatologymeteorologyatmosphere\": \"Climate\",\n",
    "    \"economy\": \"Economy\",\n",
    "    \"elevation\": \"Elevation\",\n",
    "    \"environment\": \"Environment\",\n",
    "    \"society; climatologyMeteorologyAtmosphere\": \"Events\",\n",
    "    \"geoscientificinformation\": \"Geology\",\n",
    "    \"health\": \"Health\",\n",
    "    \"imagerybasemapsearthcover\": \"Imagery\",\n",
    "    \"inlandwaters\": \"Inland Waters\",\n",
    "    \"location\": \"Location\",\n",
    "    \"intelligencemilitary\": \"Military\",\n",
    "    \"oceans\": \"Oceans\",\n",
    "    \"planningcadastre\": \"Property\",\n",
    "    \"society\": \"Society\",\n",
    "    \"structure\": \"Structure\",\n",
    "    \"transportation\": \"Transportation\",\n",
    "    \"utilitiescommunication\": \"Utilities\"\n",
    "    \n",
    "    # Add more key-value pairs for other conversions as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    subject_values = row['dc_subject_sm']\n",
    "    if pd.notna(subject_values):  # Check for NaN before splitting\n",
    "        subject_values = subject_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in subject_values:\n",
    "            value_lower = value.lower()\n",
    "            if value_lower in subject_sm_mapping:\n",
    "                converted_values.append(subject_sm_mapping[value_lower])\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "df['Theme'] = df.apply(convert_and_join, axis=1)\n",
    "\n",
    "# Drop duplicates from the \"Theme\" column\n",
    "df['Theme'] = df['Theme'].str.split('|').apply(lambda x: '|'.join(sorted(set(x), key=x.index)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e084b1",
   "metadata": {},
   "source": [
    "## Part 4: Export to a new CSV with Aardvark labels as headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080a2f7",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "02c55c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'geoblacklight_version',\n",
    "    'layer_modified_dt', \n",
    "#     'thumbnail_path_ss',\n",
    "    'w','e','n','s', \n",
    "    'solr_year_i',\n",
    "    'layer_geom_type_s',\n",
    "    'solr_geom',\n",
    "    'dct_references_s'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06042ee7",
   "metadata": {},
   "source": [
    "### Rename the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "b5c8d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'dc_title_s': 'Title', \n",
    "    'dc_description_s': 'Description',\n",
    "    'dc_creator_sm': 'Creator',\n",
    "    'dct_issued_s': 'Date Issued',\n",
    "    'dct_issued_dt': 'Date Issued',\n",
    "    'dc_rights_s' : 'Access Rights',\n",
    "    'dc_format_s': 'Format',\n",
    "    'layer_slug_s' : 'ID',\n",
    "    'layer_id_s' : 'WxS Identifier', \n",
    "    'dc_identifier_s' : 'Identifier',\n",
    "    'dc_language_s' : 'Language',\n",
    "    'dct_provenance_s' : 'Provider',\n",
    "    'dc_publisher_s' : 'Publisher',\n",
    "    'dc_publisher_sm' : 'Publisher',\n",
    "    'dc_source_sm' : 'Source',\n",
    "    'dct_spatial_sm' : 'Spatial Coverage',\n",
    "    'dc_subject_sm' : 'Subject',\n",
    "    'dct_temporal_sm' : 'Temporal Coverage',\n",
    "    'dct_isPartOf_sm' : 'Is Part Of'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "79bd3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Creator',\n",
    "'Publisher',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Subject',\n",
    "'Theme',\n",
    "'Keyword',\n",
    "'Temporal Coverage',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Geometry',\n",
    "'Member Of',\n",
    "'Source',\n",
    "'Format',\n",
    "'WxS Identifier',\n",
    "'Georeferenced',\n",
    "'Documentation',\n",
    "'Download',\n",
    "'FeatureServer',\n",
    "'FGDC',\n",
    "'Harvard Download',\n",
    "'HTML',\n",
    "'IIIF',\n",
    "'ImageServer',\n",
    "'Information',\n",
    "'ISO19139',\n",
    "'Manifest',\n",
    "'MapServer',\n",
    "'MODS',\n",
    "'oEmbed',\n",
    "'Index Map',\n",
    "'TileServer',\n",
    "'WCS',\n",
    "'WFS',\n",
    "'WMS',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Rights Holder',\n",
    "'License',\n",
    "'Access Rights',\n",
    "'Suppressed',\n",
    "'Child Record',\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487bcf0",
   "metadata": {},
   "source": [
    "### Check for multiple downloads and create a secondary CSV called \"multiple-downloads.csv\"\n",
    "\n",
    "See https://geobtaa.github.io/metadata/recipes/secondary-tables/ for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "25826b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to check if the value is an array\n",
    "def is_array_type(value):\n",
    "    return isinstance(value, list)\n",
    "\n",
    "# Function to extract the download information and write to \"multiple-downloads.csv\"\n",
    "def extract_downloads(row):\n",
    "    friendlier_id = row[\"ID\"]\n",
    "    downloads = row[\"Download\"]\n",
    "    extracted_downloads = []\n",
    "    if is_array_type(downloads):\n",
    "        for download in downloads:\n",
    "            if isinstance(download, dict):\n",
    "                label = download.get(\"label\", \"\")\n",
    "                value = download.get(\"url\", \"\")\n",
    "                extracted_downloads.append({\"friendlier_id\": friendlier_id, \"label\": label, \"value\": value})\n",
    "\n",
    "    return extracted_downloads\n",
    "\n",
    "# Apply the function to each row in the DataFrame where \"Download\" is an array\n",
    "download_list = df[df[\"Download\"].apply(is_array_type)].apply(extract_downloads, axis=1).explode().tolist()\n",
    "\n",
    "# Write the extracted downloads to \"multiple-downloads.csv\"\n",
    "with open(\"multiple-downloads.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = [\"friendlier_id\", \"label\", \"value\"]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(download_list)\n",
    "\n",
    "# Update the \"Download\" column in the main DataFrame (df) to remove array-type values\n",
    "df[\"Download\"] = df[\"Download\"].apply(lambda x: x[0][\"url\"] if is_array_type(x) else x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38345bf",
   "metadata": {},
   "source": [
    "### Write the DataFrame to a CSV file with Aardvark labels\n",
    "This can be uploaded to GEOMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "a5842da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}.csv\".format(csv_name), index=False, na_rep='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
