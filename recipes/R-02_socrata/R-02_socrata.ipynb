{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a446268",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan a list of Socrata data portals and return the metadata for all suitable items as a CSV file in the GeoBTAA Metadata Application Profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255ad2a",
   "metadata": {},
   "source": [
    "## View full recipe\n",
    "\n",
    "This Notebook is part of a workflow documented in the [Metadata Handbook Recipes section](https://geobtaa.github.io/metadata/recipes). This is the second recipe, called [Socrata](https://geobtaa.github.io/metadata/recipes/R-02_socrata/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7df424",
   "metadata": {},
   "source": [
    "## Prepare the list of active Socrata portals\n",
    "\n",
    "We maintain a list of active Socrata portals in GBL Admin. (Access to GBL Admin requires a login account. External users can create their own list or use one provided in this repository)\n",
    "\n",
    "1. Filter for items with these parameters:\n",
    "   - Resource Class: Websites\n",
    "   - Format: Socrata data portal\n",
    "   - [Shortcut query](https://geo.btaa.org/admin/documents?f%5Bb1g_publication_state_s%5D%5B%5D=published&f%5Bdct_format_s%5D%5B%5D=Socrata+data+portal&f%5Bgbl_resourceClass_sm%5D%5B%5D=Websites&rows=20&sort=score+desc)\n",
    "   \n",
    "2. Rename the downloaded file `socrataPortals.csv` and move it into the same directory as this Notebook.\n",
    "\n",
    "    \n",
    "Exporting from GBL Admin will produce a CSV containing all of the metadata associated with each Hub. For this script, the only fields used are:\n",
    "\n",
    "* **ID**: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.\n",
    "* **Title**: The name of the Hub. This is transferred to the \"Provider\" field for each dataset\n",
    "* **Publisher**: The place or administration associated with the portal. This is applied to the title in each dataset in brackets\n",
    "* **Spatial Coverage**: A list of place names. These are transferred to the Spatial Coverage for each dataset\n",
    "* **Bounding Box**: The Socrata metadata API does not include coordinates, so we just use the default bounding box for the portal's region\n",
    "* **Member Of**: a larger collection level record. Most of the portals are either part of our [Government Open Geospatial Data Collection](https://geo.btaa.org/catalog/ba5cc745-21c5-4ae9-954b-72dd8db6815a) or the [Research Institutes Geospatial Data Collection](https://geo.btaa.org/catalog/b0153110-e455-4ced-9114-9b13250a7093)\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8a6b2",
   "metadata": {},
   "source": [
    "## Define the module-level code\n",
    "\n",
    "This section includes the necessary imports, configuration settings, and function/class definitions that will be used by the rest of the code in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff38b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import csv # Provides functionality to read from and write to CSV files.\n",
    "import json # Provides functionality to work with JSON data.\n",
    "import os # Provides a way of using operating system dependent functionality, like reading or writing the file system.\n",
    "import re # Provides regular expression matching operations.\n",
    "import time # Provides functions for working with time, including time conversion, sleep function and timers.\n",
    "import urllib.request # provides functions for working with URLs, like opening URLs, reading data from URLs, etc.\n",
    "from html.parser import HTMLParser # provides an HTML parsing library that can be used to extract data from HTML docs.\n",
    "from urllib.parse import urlparse, parse_qs # provides a way to parse URLs into their components.\n",
    "\n",
    "import numpy as np # Provides numerical operations and array manipulation tools.\n",
    "import pandas as pd # Provides data manipulation and analysis functionality.\n",
    "import requests # Provides HTTP library for sending requests to servers and receiving responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274c4a",
   "metadata": {},
   "source": [
    "**Set up paths and output CSV field names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38c9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \".\"  # Set to directory containing socrataPortals.csv\n",
    "hubFile = \"socrataPortals.csv\"  # the name of the CSV file with the list of ArcGIS Hubs\n",
    "fieldnames = [  # DCAT schema fields to be included in report\n",
    "    \"Title\",\n",
    "    \"Alternative Title\",\n",
    "    \"Description\",\n",
    "    \"Language\",\n",
    "    \"Creator\",\n",
    "    \"Title Source\",\n",
    "    \"Resource Class\",\n",
    "    \"Resource Type\",\n",
    "    \"Date Issued\",\n",
    "    \"Temporal Coverage\",\n",
    "    \"Date Range\",\n",
    "    \"Spatial Coverage\",\n",
    "    \"Bounding Box\",\n",
    "    \"Format\",\n",
    "    \"Information\",\n",
    "    \"Download\",\n",
    "    \"ID\",\n",
    "    \"Identifier\",\n",
    "    \"Provider\",\n",
    "    \"Code\",\n",
    "    \"Member Of\",\n",
    "    \"Is Part Of\",\n",
    "    \"Rights\",\n",
    "    \"Accrual Method\",\n",
    "    \"Date Accessioned\",\n",
    "    \"Access Rights\",\n",
    "]\n",
    "\n",
    "ActionDate = time.strftime('%Y%m%d') # Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ba35c",
   "metadata": {},
   "source": [
    "**Function to remove HTML tags**\n",
    "\n",
    "Sometimes, the metadata fields we scrape contain HTML tags, such as links or formatting that do not work in the Geoportal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7616ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser): \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d): \n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self): # Returns a string of all the data in the list concatenated together.\n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html): # Defined by the MLS Stripper \n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value): # Calls strip_tags on the input value to remove any HTML tags present.\n",
    "    return strip_tags(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220f9c9",
   "metadata": {},
   "source": [
    "**Function to generate an output CSV**\n",
    "\n",
    "iterate over the keys and writes the corresponding values to the CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6391c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for hub in dictionary:\n",
    "            for keys in hub:\n",
    "                allvalues = hub[keys]\n",
    "                csvout.writerow(allvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba373d6",
   "metadata": {},
   "source": [
    "**Function to create a dictionary of metadata in the JSONs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3391b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the len function to get the number of datasets and the range function to loop through each dataset\n",
    "        \n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}  # Dictionary List\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a3aea",
   "metadata": {},
   "source": [
    "**Function to generate the title as: alternativeTitle [place name] {year}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1908624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function uses regular expressions to extract the year from the alternative title, and replaces it with an empty string to remove it from the title.\n",
    "\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    # find if year exist in alternativeTitle\n",
    "    year = ''\n",
    "    try:  \n",
    "      year_range = re.findall(r'(\\d{4})-(\\d{4})', alternativeTitle)\n",
    "    except:\n",
    "      year_range = ''\n",
    "    try: \n",
    "      single_year = re.match(r'.*(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})', alternativeTitle)\n",
    "    except:\n",
    "      single_year = ''    \n",
    "    if year_range:   # if a 'yyyy-yyyy' exists\n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  # or if a 'yyyy' exists\n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "     \n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'       \n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9df9",
   "metadata": {},
   "source": [
    "**Function to create a dictionary of scanned metadata**\n",
    "\n",
    "This code defines a function called `metadataNewItems()` which takes two arguments \n",
    "\n",
    "* `newdata` (a dictionary containing metadata information about new items)\n",
    "* `newitem_ids` (a dictionary containing information about the new items such as the position and the landing page URLs).\n",
    "\n",
    "The function processes the metadata information for each new item and creates a dictionary containing the formatted metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eb10c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes blank fields '' for some columns\n",
    "\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    # y = position of the dataset in the DCAT metadata json, v = landing page URLs\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "        \n",
    "\n",
    "#ALTERNATIVE TITLE\n",
    "       \n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = str(cleanData(newdata[\"dataset\"][y]['title']))\n",
    "        except:\n",
    "            alternativeTitle = str(newdata[\"dataset\"][y]['title'])\n",
    "            \n",
    "# TITLE\n",
    "            \n",
    "        # call the format_title function\n",
    "        title = format_title(alternativeTitle, titleSource)\n",
    "#         title = alternativeTitle\n",
    "            \n",
    "#DESCRIPTION\n",
    "\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        description = description.replace(\"{{default.description}}\", \"\").replace(\"{{description}}\", \"\")\n",
    "        description = re.sub(r'[\\n]+|[\\r\\n]+', ' ', description, flags=re.S)\n",
    "        description = re.sub(r'\\s{2,}', ' ', description)\n",
    "        description = description.translate({8217: \"'\", 8220: '\"', 8221: '\"', 160: \"\", 183: \"\", 8226: \"\", 8211: \"-\", 8203: \"\"})\n",
    "\n",
    "\n",
    "# RESOURCE TYPE\n",
    "\n",
    "        # if 'LiDAR' exists in Title or Description, add it to Resource Type\n",
    "        if 'LiDAR' in title or 'LiDAR' in description:\n",
    "            resourceType = 'LiDAR'\n",
    "                            \n",
    "#CREATOR\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "\n",
    "\n",
    "# DISTRIBUTION\n",
    "\n",
    "        information = str(newdata[\"dataset\"][y]['landingPage'])\n",
    "\n",
    "        format_types = []\n",
    "        formatElement = \"\"\n",
    "        downloadURL = \"\"\n",
    "        resourceType = \"\"\n",
    "\n",
    "\n",
    "        # Only fills metadata for Shapefile downloads\n",
    "        \n",
    "        try:\n",
    "            distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "            for dictionary in distribution:\n",
    "                media_type = dictionary[\"mediaType\"]\n",
    "\n",
    "                if media_type == \"application/zip\":\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    resourceType = \"Vector data\"\n",
    "                    downloadURL = dictionary[\"downloadURL\"]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        except:\n",
    "            pass  # Handle exceptions (e.g. if \"distribution\" or \"mediaType\" keys are missing)\n",
    "\n",
    "\n",
    "        \n",
    "# DATES\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "\n",
    "        # auto-generate Temporal Coverage and Date Range\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     # if title has {YYYY} or {YYYY-YYYY}\n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            dateRange = temporalCoverage[:4] + '-' + temporalCoverage[-4:]\n",
    "        else:\n",
    "            temporalCoverage = 'Continually updated resource'\n",
    "        \n",
    "#RIGHTS\n",
    "\n",
    "        rights = cleanData(newdata[\"dataset\"][y]['license']) if 'license' in newdata[\"dataset\"][y] else \"\"\n",
    "\n",
    "\n",
    "# IDENTIFIER\n",
    "        slug = identifier.split('/views/', 1)[-1]\n",
    "        identifier = identifier\n",
    "\n",
    "            \n",
    "# Define full metadata list\n",
    "\n",
    "        metadataList = [\n",
    "            title, \n",
    "            alternativeTitle, \n",
    "            description, \n",
    "            language, \n",
    "            creator,\n",
    "            titleSource,\n",
    "            resourceClass, \n",
    "            resourceType, \n",
    "            dateIssued, \n",
    "            temporalCoverage,\n",
    "            dateRange, \n",
    "            spatialCoverage, \n",
    "            bbox,\n",
    "            formatElement, \n",
    "            information, \n",
    "            downloadURL, \n",
    "            slug, \n",
    "            identifier,\n",
    "            provider, \n",
    "            hubCode, \n",
    "            memberOf, \n",
    "            isPartOf, \n",
    "            rights,\n",
    "            accrualMethod,\n",
    "            dateAccessioned, \n",
    "            accessRights\n",
    "        ]     \n",
    "\n",
    "        # deletes items where the resourceClass is empty\n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[13] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[slug] = metadata\n",
    "\n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef627b2b",
   "metadata": {},
   "source": [
    "## Run the executable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9c0be",
   "metadata": {},
   "source": [
    "**Declare a list to hold the scanned metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ba8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRecords = []\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1069f4",
   "metadata": {},
   "source": [
    "**Scan the metadata for each Hub**\n",
    "\n",
    "This code reads data from `socrataPortals.csv` using the `csv.DictReader` function. It then iterates over each row in the file and extracts values from specific columns to be used later in the script.\n",
    "\n",
    "For each row, the script also defines default values for a set of metadata fields. It then checks if the URL provided in the CSV file exists and is a valid JSON response. If the response is not valid, the script prints an error message and continues to the next row. Otherwise, it extracts dataset identifiers from the JSON response and passes the response along with the identifiers to a function called metadataNewItems. The metadata for each row is then appended to a list called `allRecords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908a93c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning  01c-01 https://data.bloomington.in.gov/data.json\n",
      "scanning  02b-17117 https://data.macoupincountyil.gov/data.json\n",
      "scanning  04b-24027 https://opendata.howardcountymd.gov/data.json\n",
      "scanning  04b-24033 https://data.princegeorgescountymd.gov/data.json\n",
      "scanning  11c-01 https://data.cincinnati-oh.gov/data.json\n",
      "scanning  12b-17031-2 https://datacatalog.cookcountyil.gov/data.json\n",
      "scanning  12c-01 https://data.cityofchicago.org/data.json\n"
     ]
    }
   ],
   "source": [
    "with open(hubFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Read in values from socrataPortals.csv to be used within the script or as part of the metadata report\n",
    "        hubCode = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        bbox = row['Bounding Box']\n",
    "        isPartOf = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        \n",
    "        # Define default values for each record\n",
    "        accrualMethod = \"Socrata\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        language = \"eng\"\n",
    "        resourceClass = \"Datasets\"\n",
    "\n",
    "        print(\"scanning \", hubCode, url)\n",
    "        \n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        # check if the Hub's URL is broken\n",
    "        if response.headers['content-type'] != 'application/json; charset=utf-8':\n",
    "            print(\"\\n--------------------- Data hub URL does not exist --------------------\\n\",\n",
    "                  hubCode, url,  \"\\n--------------------------------------------------------------------------\\n\")\n",
    "            continue\n",
    "        else:\n",
    "            newdata = json.load(response)\n",
    "\n",
    "\n",
    "        # Makes a list of dataset identifiers\n",
    "        newjson_ids = getIdentifiers(newdata)\n",
    "\n",
    "\n",
    "        allRecords.append(metadataNewItems(newdata, newjson_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec019d73",
   "metadata": {},
   "source": [
    "**Write the scanned metadata to a CSV in the GeoBTAA Metadata Profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d787b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "newItemsReport = f\"{directory}/{ActionDate}_scannedRecords.csv\"\n",
    "printItemReport(newItemsReport, fieldnames, allRecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5a7a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load into a CSV\n",
    "\n",
    "df = pd.read_csv(newItemsReport)\n",
    "\n",
    "# Drop duplicates based on the 'ID' column\n",
    "df = df.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea0aa13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the first CSV with all columns except 'information' and 'downloadURL'\n",
    "df_first_csv = df.drop(columns=['Information', 'Download'])\n",
    "df_first_csv.to_csv(f'{ActionDate}_Socrata-metadata.csv', index=False)\n",
    "\n",
    "# 2. Create the second CSV with 'friendlier_id', 'type', and 'value'\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    slug = row['ID']\n",
    "    \n",
    "    # Check if 'information' is not null or empty, then add to rows\n",
    "    if pd.notna(row['Information']) and row['Information'] != \"\":\n",
    "        rows.append({'friendlier_id': slug, 'type': 'Information', 'value': row['Information']})\n",
    "    \n",
    "    # Check if 'downloadURL' is not null or empty, then add to rows\n",
    "    if pd.notna(row['Download']) and row['Download'] != \"\":\n",
    "        rows.append({'friendlier_id': slug, 'type': 'Download', 'value': row['Download']})\n",
    "\n",
    "# Create a DataFrame from the rows and save to CSV\n",
    "df_second_csv = pd.DataFrame(rows)\n",
    "df_second_csv.to_csv(f'{ActionDate}_Socrata-links.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7cf85",
   "metadata": {},
   "source": [
    "**Drop duplicate items**\n",
    "\n",
    "Socrata administrators may have datasets from other portals in their own site. As a result, some datasets are duplicated. However, they always have the same Identifier, so we can use pandas to detect and remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ce2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe from the file\n",
    "df_newitems = pd.read_csv(newItemsReport)\n",
    "\n",
    "# Drop duplicates based on the 'ID' column\n",
    "df_finalItems = df_newitems.drop_duplicates(subset=['ID'])\n",
    "\n",
    "# Save the modified dataframe back to the file\n",
    "df_finalItems.to_csv(newItemsReport, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128029f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
