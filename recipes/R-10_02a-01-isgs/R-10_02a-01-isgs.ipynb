{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c743471b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import csv \n",
    "import time\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65c117",
   "metadata": {},
   "source": [
    "## Part 1: Obtain a list of dataset pages and query the discovery metadata\n",
    "\n",
    "We will query the website to return:\n",
    "\n",
    "- Title\n",
    "- Description\n",
    "- Metadata file link\n",
    "- Download link\n",
    "- Web Services link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb271d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://clearinghouse.isgs.illinois.edu'\n",
    "landing_pages = get_landing_pages(base_url)\n",
    "\n",
    "def get_landing_pages(base_url):\n",
    "    response = requests.get(f\"{base_url}/data\")\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    landing_pages = []\n",
    "    \n",
    "    \n",
    "    for theme_section in soup.select('.item-list'):\n",
    "        theme = theme_section.find('h3').text\n",
    "        for dataset in theme_section.select('.views-row'):\n",
    "            title = dataset.select_one('.views-field-title').text.strip()\n",
    "            relative_url = dataset.select_one('.views-field-title a')['href']\n",
    "            landing_page = f\"{base_url}{relative_url}\"\n",
    "            landing_pages.append((theme, title, landing_page))\n",
    "            \n",
    "    return landing_pages\n",
    "\n",
    "def get_dataset_info(theme, title, landing_page):\n",
    "    response = requests.get(landing_page)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    dataset_info = {\n",
    "        'Keyword': theme,\n",
    "        'Alternative Title': title,\n",
    "        'Information': landing_page,\n",
    "        'Description': '',\n",
    "        'HTML': '',\n",
    "        'FGDC': '',\n",
    "        'Documentation': '',\n",
    "        'ImageServer': '',\n",
    "        'MapServer': '',\n",
    "        'FeatureServer': '',\n",
    "        'Download': ''\n",
    "    }\n",
    "    \n",
    "    # Extract Summary\n",
    "    \n",
    "    summary_fieldset = soup.select_one('fieldset.group-summary')\n",
    "\n",
    "    if summary_fieldset:\n",
    "        summary_div = summary_fieldset.select_one('div.field-item')\n",
    "        if summary_div:\n",
    "            dataset_info['Description'] = summary_div.text.strip()\n",
    "        else:\n",
    "            dataset_info['Description'] = ''  # Fallback if 'div.field-item' is not found\n",
    "    else:\n",
    "        dataset_info['Description'] = ''  # Fallback if 'fieldset.group-summary' is not found\n",
    "\n",
    "    \n",
    "            \n",
    "    # Extract the first .zip Download Link\n",
    "    download_link = None\n",
    "    download_sections = soup.select('.group-downloads .field-item a')\n",
    "    if download_sections:\n",
    "        for download_section in download_sections:\n",
    "            if 'href' in download_section.attrs:\n",
    "                potential_link = download_section['href']\n",
    "                if potential_link.endswith('.zip'):\n",
    "                    download_link = potential_link\n",
    "                    break  # Exit the loop once the first .zip link is found\n",
    "\n",
    "    # Store the download link\n",
    "    if download_link:  # Only store if there's a .zip download link\n",
    "        dataset_info['Download'] = download_link\n",
    "    \n",
    "\n",
    "    metadata_sections = soup.select('.group-metadata .field-item a')\n",
    "    if metadata_sections is not None:\n",
    "        for metadata_section in metadata_sections:\n",
    "            metadata_link = metadata_section['href']\n",
    "\n",
    "            # Check if it's a relative link and prepend the base URL if needed\n",
    "            if not metadata_link.startswith('http'):\n",
    "                metadata_link = f\"{base_url}{metadata_link}\"\n",
    "\n",
    "            # Categorize by extension type\n",
    "            if metadata_link.endswith('.htm') or metadata_link.endswith('.html'):\n",
    "                dataset_info['HTML'] = metadata_link\n",
    "            elif metadata_link.endswith('.xml'):\n",
    "                dataset_info['FGDC'] = metadata_link\n",
    "            elif metadata_link.endswith('.pdf'):\n",
    "                dataset_info['Documentation'] = metadata_link\n",
    "\n",
    "        \n",
    "    service_sections = soup.select('.group_services .field-item a')\n",
    "    if service_sections is not None:\n",
    "        for service_section in service_sections:\n",
    "            service_link = service_section['href']\n",
    "            if service_link.endswith('/ImageServer'):\n",
    "                dataset_info['ImageServer'] = service_link\n",
    "            elif service_link.endswith('/MapServer'):\n",
    "                dataset_info['MapServer'] = service_link\n",
    "            elif service_link.endswith('/FeatureServer'):\n",
    "                dataset_info['FeatureServer'] = service_link\n",
    "                \n",
    "    \n",
    "\n",
    "    return dataset_info\n",
    "\n",
    "def main():\n",
    "#     base_url = 'https://clearinghouse.isgs.illinois.edu'\n",
    "#     landing_pages = get_landing_pages(base_url)\n",
    "    \n",
    "    all_datasets = []\n",
    "    for theme, title, landing_page in landing_pages:\n",
    "        dataset_info = get_dataset_info(theme, title, landing_page)\n",
    "        all_datasets.append(dataset_info)\n",
    "        \n",
    "    df = pd.DataFrame(all_datasets)\n",
    "    return df \n",
    "    print(\"Metadata harvesting completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747eb59e",
   "metadata": {},
   "source": [
    "## Part 2: Configure additional fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957115ca",
   "metadata": {},
   "source": [
    "### Create an ID based upon URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fe4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "def generate_id(url):\n",
    "    # Extract the path from the URL\n",
    "    path = urllib.parse.urlparse(url).path\n",
    "    \n",
    "    # Define the prefixes you're interested in\n",
    "    prefixes = [\"/data/\", \"/datasets/\"]\n",
    "    \n",
    "    # Initialize start_pos to -1 (not found)\n",
    "    start_pos = -1\n",
    "    \n",
    "    # Choose the appropriate prefix\n",
    "    for prefix in prefixes:\n",
    "        start_pos = path.find(prefix)\n",
    "        if start_pos != -1:\n",
    "            break\n",
    "    \n",
    "    # If neither prefix is found\n",
    "    if start_pos == -1:\n",
    "        return \"Prefix not found\"\n",
    "    \n",
    "    start_pos += len(prefix)\n",
    "    \n",
    "    # Extract the relevant part of the path\n",
    "    relevant_path = path[start_pos:]\n",
    "    \n",
    "    # Replace slashes with dashes\n",
    "    modified_path = relevant_path.replace('/', '-')\n",
    "    \n",
    "    # Construct the ID\n",
    "    dataset_id = f\"02a-01_{modified_path}\"\n",
    "    \n",
    "    return dataset_id\n",
    "\n",
    "# Apply the function to generate IDs and store them in a new DataFrame column\n",
    "df['ID'] = df['Information'].apply(generate_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c2906",
   "metadata": {},
   "source": [
    "### Transform the title and extract a year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87468817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def transform_title_and_extract_year(title):\n",
    "    # Remove the word \"Illinois\" if it exists in the title\n",
    "    title = re.sub(r'\\bin Illinois\\b', '', title)\n",
    "    title = re.sub(r'\\bIllinois, \\b', '', title)\n",
    "    title = re.sub(r'\\bIllinois\\b', '', title)\n",
    "    # Remove leading and trailing spaces and dashes\n",
    "    title = title.strip(' -')\n",
    "    \n",
    "    # Look for integers of 4 digits that could be years\n",
    "    year_match = re.search(r'\\b(\\d{4})\\b', title)\n",
    "    if year_match:\n",
    "        year = year_match.group(1)\n",
    "        # Remove the year from the original title\n",
    "        title = re.sub(r'\\b\\d{4}\\b', '', title).strip(' -')\n",
    "        transformed_title = f\"{title} [Illinois] {{{year}}}\"\n",
    "        return transformed_title, year\n",
    "    else:\n",
    "        transformed_title = f\"{title} [Illinois]\"\n",
    "        return transformed_title, None\n",
    "\n",
    "# Apply the function and store results in new columns\n",
    "df[['Title', 'Temporal Coverage']] = df['Alternative Title'].apply(\n",
    "    lambda x: pd.Series(transform_title_and_extract_year(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3307c",
   "metadata": {},
   "source": [
    "### Adjust spelling of ISO Topics and determine Resource Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cbe3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversion mappings from old values to new values\n",
    "subject_sm_mapping = {\n",
    "    \"Climate\": \"Climate\",\n",
    "    \"Coastal\": \"Environment|Inland Waters\",\n",
    "    \"Elevation\": \"Elevation\",\n",
    "    \"Geology\": \"Geology\",\n",
    "    \"Hydrology\": \"Inland Waters\",\n",
    "    \"Imagery\": \"Imagery\",\n",
    "    \"Infrastructure\": \"Boundaries\",\n",
    "    \"Landcover\": \"Land Cover\",\n",
    "    \"Reference\": \"Boundaries\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    subject_values = row['Keyword']\n",
    "    if pd.notna(subject_values):  # Check for NaN before splitting\n",
    "        subject_values = subject_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in subject_values:\n",
    "            if value in subject_sm_mapping:\n",
    "                converted_values.append(subject_sm_mapping[value])\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "df['Theme'] = df.apply(convert_and_join, axis=1)\n",
    "\n",
    "# Use Theme to determine Resource Class\n",
    "df['Resource Class'] = df['Keyword'].apply(lambda x: 'Imagery' if x == 'Imagery' else 'Datasets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30b541",
   "metadata": {},
   "source": [
    "### Append default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe779241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Code'] = '02a-01'\n",
    "df['Access Rights'] = 'Public'\n",
    "df['Accrual Method'] = 'HTML'\n",
    "df['Date Accessioned'] = time.strftime('%Y-%m-%d')\n",
    "df['Language'] = 'eng'\n",
    "df['Is Part Of'] = '02a-01'\n",
    "df['Member Of'] = 'ba5cc745-21c5-4ae9-954b-72dd8db6815a'\n",
    "df['Provider'] = 'Illinois Geospatial Data Clearinghouse'\n",
    "df['Identifier'] = df['Information']\n",
    "df['Format'] = 'File'\n",
    "df['Bounding Box'] = '-91.51,36.97,-87.02,42.51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d82bb3",
   "metadata": {},
   "source": [
    "## Part 3: Print to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bfbb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'02a-01_{actionDate}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f5c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
