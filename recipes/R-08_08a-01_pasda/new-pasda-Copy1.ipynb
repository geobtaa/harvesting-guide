{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99cc485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "import time\n",
    "import urllib.request # The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP)\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# Define default values\n",
    "code = '08a-01'  \n",
    "accessRights = 'Public' \n",
    "accrualMethod = 'HTML' \n",
    "dateAccessioned = time.strftime('%Y-%m-%d') \n",
    "language = 'eng' \n",
    "isPartOf = '08a-01'\n",
    "memberOf = 'ba5cc745-21c5-4ae9-954b-72dd8db6815a '\n",
    "provider = 'Pennsylvania Spatial Data Access (PASDA)'\n",
    "# resourceClass = ''\n",
    "# resourceType = ''\n",
    "# dateRange = ''\n",
    "\n",
    "\n",
    "# ... [Rest of your imports and definitions, removed for brevity] ...\n",
    "\n",
    "\n",
    "# Start with the main search page\n",
    "resURL = 'https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+'\n",
    "page = urllib.request.urlopen(resURL).read()\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "# Identify landing page URLs inside <h3> tags\n",
    "landing_page_links = soup.select('h3 a[href^=\"DataSummary.aspx?dataset=\"]')\n",
    "landing_pages = ['https://www.pasda.psu.edu/uci/' + link['href'] for link in landing_page_links]\n",
    "\n",
    "# For testing the code of just the first 5\n",
    "landing_pages = landing_pages[:5]  # Keep only the first 5 landing pages\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for url in landing_pages:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    landingPage = url\n",
    "    iden = 'pasda-' + landingPage.rsplit(\"=\",1)[1]\n",
    "\n",
    "    # Extract metadata fields\n",
    "    title = soup.find(attrs={'id': 'Label1'}).text.strip()\n",
    "    date = soup.find(attrs={'id': 'Label2'}).text.strip()\n",
    "    publisher = soup.find(attrs={'id': 'Label3'}).text.strip()\n",
    "    description = soup.find(attrs={'id': 'Label14'}).text.strip()\n",
    "    \n",
    "    metadataLink = soup.find('a', href=True, string='Metadata')\n",
    "    downloadLink = soup.find('a', href=True, string='Download')\n",
    "    \n",
    "    metadata = \"https://www.pasda.psu.edu/uci/\" + metadataLink['href']\n",
    "    try:\n",
    "        download = downloadLink['href']\n",
    "    except:\n",
    "        download = ''\n",
    "    \n",
    "    # ... [Rest of the data extraction code, similar to what you have] ...\n",
    "    \n",
    "    record = {\n",
    "    \"Information\": landingPage,\n",
    "    \"ID\": iden,\n",
    "    \"Title\": title,\n",
    "    \"Date\": date,\n",
    "    \"Publisher\": publisher, \n",
    "    \"Provider\": provider, \n",
    "    \"Language\": language,\n",
    "    \"Description\": description,\n",
    "    \"HTML\": metadata,\n",
    "    \"Download\": download,\n",
    "    \"Code\": code,\n",
    "    \"Is Part Of\": isPartOf,\n",
    "    \"Member Of\": memberOf,\n",
    "    \n",
    "        \n",
    "        \n",
    "    \"Date Accessioned\": dateAccessioned,\n",
    "}\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    all_data.append(record)\n",
    "    \n",
    "    data.append(\n",
    "        [landingPage, \n",
    "        iden, \n",
    "        title, \n",
    "        date, \n",
    "#         dateRange, \n",
    "        \n",
    "        description, \n",
    "#         resourceClass, \n",
    "#         resourceType, \n",
    "        metadata, \n",
    "        download, \n",
    "        code, \n",
    "        isPartOf, \n",
    "        memberOf, \n",
    "        accessRights, \n",
    "        accrualMethod, \n",
    "        dateAccessioned]\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Information', 'ID', 'Title', 'Temporal Coverage', 'Date Range', 'Publisher', 'Provider', 'Language', 'Description', 'Resource Class', 'Resource Type', 'HTML', 'Download', 'Code', 'Is Part Of', 'Member Of', 'Access Rights', 'Accrual Method', 'Date Accessioned'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: Extracting bounding boxes\n",
    "bounding_boxes = {}\n",
    "for metadata_url in df['HTML']:\n",
    "    response = requests.get(metadata_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    try: \n",
    "        try:\n",
    "            west = soup.find('i', string='West_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            west = ''\n",
    "\n",
    "        try:\n",
    "            south = soup.find('i', string='South_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            south = ''\n",
    "\n",
    "        try:\n",
    "            east = soup.find('i', string='East_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            east = ''\n",
    "\n",
    "        try:\n",
    "            north = soup.find('i', string='North_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            north = ''\n",
    "\n",
    "        bbox = west + ',' + south + ',' +east + ',' + north\n",
    "    except:\n",
    "        bbox = \"missing\"\n",
    "        \n",
    "        \n",
    "    bounding_boxes[metadata_url] = bbox\n",
    "\n",
    "df['Bounding Box'] = df['HTML'].map(bounding_boxes)\n",
    "\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'output_{actionDate}.csv', index=False)\n",
    "print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec9922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
