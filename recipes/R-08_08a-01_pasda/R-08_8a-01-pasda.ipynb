{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3638f749",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e3239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "import time\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773bdb3",
   "metadata": {},
   "source": [
    "## Part 1: Obtain a list of dataset pages and query the discovery metadata\n",
    "\n",
    "We use PASDA's search feature to return a page (https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+) that lists all of the activate dataset landing pages with some metadata. Then, we use the Beautiful Soup module to query this page and harvest the following values:\n",
    "\n",
    "- Title\n",
    "- Date Issued\n",
    "- Publisher\n",
    "- Description\n",
    "- Metadata file link\n",
    "- Download link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1000d",
   "metadata": {},
   "source": [
    "### MANUAL STEP!!\n",
    "\n",
    "1. Open \"https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+\" in a browser\n",
    "2. Download the page\n",
    "3. Save the file as \"pasda-search.html\" in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b45976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the downloaded file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'pasda-search.html'  # Modify this to the correct path to your downloaded HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Assuming every dataset is contained in its own <tr> tag\n",
    "datasets = soup.select('tr[align=\"left\"]')\n",
    "\n",
    "data = []\n",
    "\n",
    "dataset_entries = soup.select('td > h3 > a[href^=\"DataSummary.aspx?dataset=\"]')\n",
    "\n",
    "for entry in dataset_entries:\n",
    "    publisher = entry.find_next(\"td\").text.strip()\n",
    "    date = entry.find_previous(\"td\").find_previous(\"td\").text.strip()\n",
    "    title = entry.text.strip()\n",
    "    description = entry.find_next(\"span\", id=lambda x: x and x.startswith('DataGrid1_Label3_')).text.strip()\n",
    "    metadataFile = entry.parent.parent.find('a', href=True, string='Metadata')['href']\n",
    "    metadataLink = \"https://www.pasda.psu.edu/uci/\" + metadataFile \n",
    "    try:\n",
    "        download = entry.parent.parent.find('a', href=True, string='Download')['href']\n",
    "    except:\n",
    "        download = ''\n",
    "        \n",
    "    # obtain full landing page and create ID\n",
    "    landing_page = \"https://www.pasda.psu.edu/uci/\" + entry['href']  # Landing page URL\n",
    "    iden = 'pasda-' + landing_page.rsplit(\"=\",1)[1]\n",
    "\n",
    "    data.append([publisher, date, title, description, metadataFile, metadataLink, download, landing_page, iden])\n",
    "    \n",
    "\n",
    "# Convert to pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['Creator', 'Date Issued', 'Alternative Title', 'Description', 'Metadata File', 'HTML', 'Download', 'Information', 'ID'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11d24510-4a72-419c-80c4-69a0126e437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Job done ####\n"
     ]
    }
   ],
   "source": [
    "# optional: check the results\n",
    "df\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'pasda-aardvark_{actionDate}.csv', index=False)\n",
    "print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa55394",
   "metadata": {},
   "source": [
    "## Part 2: Download the supplemental metadata\n",
    "\n",
    "Context: Most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'HTML\" column. Although these files are created as XMLs, the link is a rendered HTML.\n",
    "\n",
    "There is additional information in these files that we want to scrape, including bounding boxes and geometry type.\n",
    "\n",
    "We will start by downloading the metadata files - this will save time and reduce the load on PASDA's servers because this part of the recipe may need to be run multiple times after troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbf4a1",
   "metadata": {},
   "source": [
    "### Download the metadata files to a folder called \"metadata_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e10ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory named 'metadata_files' to store the downloaded files\n",
    "download_folder = 'metadata_files'\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af38578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, folder):\n",
    "    \"\"\"\n",
    "    Download a file given its URL and store it in the specified folder.\n",
    "    \"\"\"\n",
    "    # Get the filename from the URL\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Handle the response's content in chunks (useful for large files)\n",
    "    with open(os.path.join(folder, filename), 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbb4f70-c481-420c-8f76-5af1ad8c07c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://www.pasda.psu.edu/uci/FullMetadataDisplay.aspx?file=AdamsCounty_AgSecurityAreas202309.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTML\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mdownload_file\u001b[0;34m(url, folder)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get the filename from the URL\u001b[39;00m\n\u001b[1;32m      8\u001b[0m filename \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Handle the response's content in chunks (useful for large files)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, filename), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n\u001b[1;32m   1045\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1046\u001b[0m         (\n\u001b[1;32m   1047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnverified HTTPS request is being made to host \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   1053\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connection.py:358\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 358\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    360\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msocket_options\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     84\u001b[0m         sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sock\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for url in df['HTML']:\n",
    "    try:\n",
    "        download_file(url, download_folder)\n",
    "        print(f\"Downloaded {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}. Reason: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a134b7f",
   "metadata": {},
   "source": [
    "## Part 3: Query the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051feb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WEST_BOUNDING = 'West_Bounding_Coordinate:'\n",
    "SOUTH_BOUNDING = 'South_Bounding_Coordinate:'\n",
    "EAST_BOUNDING = 'East_Bounding_Coordinate:'\n",
    "NORTH_BOUNDING = 'North_Bounding_Coordinate:'\n",
    "DIRECT_SPATIAL = 'Direct_Spatial_Reference_Method:'\n",
    "THEME_KEYWORD = 'Theme_Keyword:'\n",
    "PLACE_KEYWORD = 'Place_Keyword:'\n",
    "\n",
    "# Extract bounding box from a metadata file\n",
    "def extract_bbox(soup):\n",
    "    try:\n",
    "        west = soup.find('i', string=WEST_BOUNDING).next_sibling.strip()\n",
    "    except AttributeError:\n",
    "        west = '-80.52'\n",
    "    try:\n",
    "        south = soup.find('i', string='South_Bounding_Coordinate:').next_sibling.strip()   \n",
    "    except AttributeError:\n",
    "        south = '39.72'\n",
    "\n",
    "    try:\n",
    "        east = soup.find('i', string='East_Bounding_Coordinate:').next_sibling.strip()   \n",
    "    except AttributeError:\n",
    "        east = '-74.69'\n",
    "    try:\n",
    "        north = soup.find('i', string='North_Bounding_Coordinate:').next_sibling.strip()   \n",
    "    except AttributeError:\n",
    "        north = '42.51'\n",
    "\n",
    "    return f\"{west},{south},{east},{north}\"\n",
    "\n",
    "# Extract spatial reference method from a metadata file\n",
    "def extract_spatial_ref(soup):\n",
    "    try:\n",
    "        res_type = soup.find('i', string=DIRECT_SPATIAL).next_sibling.strip() + ' data'\n",
    "    except AttributeError:\n",
    "        res_type = ''\n",
    "    return res_type\n",
    "\n",
    "# Extract keywords from a metadata file\n",
    "def extract_keywords(soup, keyword_type):\n",
    "    try:\n",
    "        keywords = soup.findAll('i', string=keyword_type)\n",
    "        return \"|\".join([kw.next_sibling.strip() for kw in keywords])\n",
    "    except Exception as e:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1d618-2bbd-4156-8134-50592d32f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "metadata_folder = \"metadata_files\"\n",
    "bounding_boxes = {}\n",
    "spatial_reference_methods = {}\n",
    "theme_keywords_list = []\n",
    "place_keywords_list = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, metadata_file in enumerate(df['Metadata File']):\n",
    "    try:\n",
    "        file_path = os.path.join(metadata_folder, metadata_file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            soup = BeautifulSoup(file_content, \"html.parser\")\n",
    "            \n",
    "            print(f\"Processing metadata file {idx + 1} of {len(df)}: {metadata_file}\")  # Print progress\n",
    "\n",
    "            bbox = extract_bbox(soup)\n",
    "            res_type = extract_spatial_ref(soup)\n",
    "            theme_keywords = extract_keywords(soup, THEME_KEYWORD)\n",
    "            place_keywords = extract_keywords(soup, PLACE_KEYWORD)\n",
    "\n",
    "            bounding_boxes[metadata_file] = bbox\n",
    "            spatial_reference_methods[metadata_file] = res_type\n",
    "            \n",
    "            theme_keywords_list.append(theme_keywords)\n",
    "            place_keywords_list.append(place_keywords)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing metadata file {metadata_file}: {e}\")  # Print error message\n",
    "\n",
    "# Convert lists to Pandas Series\n",
    "theme_keywords_series = pd.Series(theme_keywords_list)\n",
    "place_keywords_series = pd.Series(place_keywords_list)\n",
    "\n",
    "\n",
    "\n",
    "df['Bounding Box'] = df['Metadata File'].map(bounding_boxes)\n",
    "df['Resource Type'] = df['Metadata File'].map(spatial_reference_methods)\n",
    "df['Resource Class'] = np.where(df['Resource Type'] == 'Raster data', 'Imagery', 'Datasets')\n",
    "df['Keyword'] = theme_keywords_series\n",
    "df['Spatial Coverage'] = place_keywords_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f483e-2c7a-4866-94ca-b1aeabb09dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: check the results\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fed73",
   "metadata": {},
   "source": [
    "## Part 4: add default and calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fee390-aab5-4829-b2d2-6b9a90cb23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range_formatter(date_issued):\n",
    "    # Extract years\n",
    "    years = re.findall(r'(\\d{4})', date_issued)\n",
    "    # If only one year is found, duplicate it to create a range\n",
    "    if len(years) == 1:\n",
    "        return f\"{years[0]}-{years[0]}\"\n",
    "    # If two years are found, format them as a range\n",
    "    elif len(years) == 2:\n",
    "        return f\"{years[0]}-{years[1]}\"\n",
    "    # Return original string if no match (or any other behavior you prefer)\n",
    "    else:\n",
    "        return date_issued\n",
    "\n",
    "df['Date Range'] = df['Date Issued'].apply(date_range_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1acc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append default values\n",
    "\n",
    "df['Code'] = '08a-01'\n",
    "df['Access Rights'] = 'Public'\n",
    "df['Accrual Method'] = 'HTML'\n",
    "df['Date Accessioned'] = time.strftime('%Y-%m-%d')\n",
    "df['Language'] = 'eng'\n",
    "df['Is Part Of'] = '08a-01'\n",
    "df['Member Of'] = 'ba5cc745-21c5-4ae9-954b-72dd8db6815a'\n",
    "df['Provider'] = 'Pennsylvania Spatial Data Access (PASDA)'\n",
    "df['Identifier'] = df['Information']\n",
    "df['Format'] = 'File'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef467d5",
   "metadata": {},
   "source": [
    "### Clean up the titles\n",
    "\n",
    "Title-case the \"Alternative Title\".\n",
    "Check for counties in the title and reformat accordingly.\n",
    "If no county is found, check for cities in the title and reformat accordingly.\n",
    "If neither county nor city is found, it checks for \"PA \" and replaces it with \"[Pennsylvania]\".\n",
    "It then captures the content in brackets, removes it from its original position, and appends it to the end of the title.\n",
    "Some specific transformations (cleanup) are performed post-transformation.\n",
    "The value from 'Date Issued' is appended at the end of the title, surrounded by curly brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1825f6e-99c5-418d-ac47-0366e3f0808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_in_pennsylvania = [\n",
    "    'Adams', 'Allegheny', 'Armstrong', 'Beaver', 'Bedford', 'Berks',\n",
    "    'Blair', 'Bradford', 'Bucks', 'Butler', 'Cambria', 'Cameron',\n",
    "    'Carbon', 'Centre', 'Chester', 'Clarion', 'Clearfield', 'Clinton',\n",
    "    'Columbia', 'Crawford', 'Cumberland', 'Dauphin', 'Delaware', 'Elk',\n",
    "    'Erie', 'Fayette', 'Forest', 'Franklin', 'Fulton', 'Greene',\n",
    "    'Huntingdon', 'Indiana', 'Jefferson', 'Juniata', 'Lackawanna',\n",
    "    'Lancaster', 'Lawrence', 'Lebanon', 'Lehigh', 'Luzerne', 'Lycoming',\n",
    "    'McKean', 'Mercer', 'Mifflin', 'Monroe', 'Montgomery', 'Montour',\n",
    "    'Northampton', 'Northumberland', 'Perry', 'Philadelphia', 'Pike',\n",
    "    'Potter', 'Schuylkill', 'Snyder', 'Somerset', 'Sullivan', 'Susquehanna',\n",
    "    'Tioga', 'Union', 'Venango', 'Warren', 'Washington', 'Wayne',\n",
    "    'Westmoreland', 'Wyoming', 'York'\n",
    "]\n",
    "\n",
    "cities_in_pennsylvania = [\n",
    "    'Allentown', 'Altoona', 'Bethlehem', 'Butler', 'Carlisle', 'Chambersburg', \n",
    "    'Chester', 'DuBois', 'Easton', 'Erie', 'Greensburg', 'Hanover', \n",
    "    'Harrisburg', 'Hazleton', 'Hermitage', 'Johnstown', 'Lancaster', \n",
    "    'Latrobe', 'Lebanon', 'Lock Haven', 'Lower Merion', 'McKeesport', \n",
    "    'Meadville', 'Monroeville', 'Nanticoke', 'New Castle', 'Norristown', \n",
    "    'Philadelphia', 'Phoenixville', 'Pittsburgh', 'Pottstown', 'Pottsville', \n",
    "    'Reading', 'Scranton', 'Sharon', 'State College', 'Uniontown', \n",
    "    'Warren', 'Washington', 'West Chester', 'Wilkes-Barre', 'Williamsport', 'York'\n",
    "]\n",
    "\n",
    "\n",
    "def transform_title(row):\n",
    "    alt_title = row['Alternative Title']\n",
    "    \n",
    "    # Search for a city or county name in the title.\n",
    "    for county in counties_in_pennsylvania:\n",
    "        if re.search(f\"{county} County\", alt_title, re.I):\n",
    "            alt_title = re.sub(f\"{county} County\", f\"[Pennsylvania--{county} County]\", alt_title, flags=re.I, count=1)\n",
    "            break\n",
    "    else:\n",
    "        for city in cities_in_pennsylvania:\n",
    "            if re.search(f\"\\b{city}\\b\", alt_title, re.I):\n",
    "                alt_title = re.sub(f\"\\b{city}\\b\", f\"[Pennsylvania--{city}]\", alt_title, flags=re.I, count=1)\n",
    "                break\n",
    "        else:\n",
    "            alt_title = re.sub(r\"\\b(PA|Pennsylvania)\\b\", \"[Pennsylvania]\", alt_title, flags=re.I, count=1)\n",
    "\n",
    "    # Capture content in brackets\n",
    "    bracket_content = re.findall(r'\\[(.*?)\\]', alt_title)\n",
    "    \n",
    "    if bracket_content:\n",
    "        # Remove bracketed content from original position\n",
    "        alt_title = re.sub(r'\\[.*?\\]', '', alt_title).strip()\n",
    "        \n",
    "        # Append bracketed content to the end of the title\n",
    "        alt_title = f\"{alt_title} [{bracket_content[0]}]\"\n",
    "\n",
    "    # Cleanup phrases post-transformation using case-insensitive matching\n",
    "    alt_title = re.sub(r\"For \\[\", \"[\", alt_title, flags=re.I)\n",
    "    alt_title = re.sub(r\"For The \\[\", \"[\", alt_title, flags=re.I)\n",
    "    alt_title = re.sub(r\"For The City Of \\[\", \"[\", alt_title, flags=re.I)\n",
    "\n",
    "    # Remove unwanted dashes at the beginning or just before a bracket\n",
    "    alt_title = re.sub(r\"^\\s*-\\s*|\\s*-\\s*(?=\\[)\", \"\", alt_title)\n",
    "    \n",
    "    # Make sure first letter is capitalized\n",
    "    alt_title = alt_title[0].capitalize() + alt_title[1:]\n",
    "\n",
    "    # Append the value from 'Date Issued' surrounded by curly brackets\n",
    "    alt_title += f\" {{{row['Date Issued']}}}\"\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "df['Title'] = df.apply(transform_title, axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8b3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_publisher(publisher):\n",
    "    # Dictionary mapping of publishers for direct transformation\n",
    "    publisher_mappings = {\n",
    "        \"U S Geological Survey\": \"Geological Survey (U.S.)\",\n",
    "        \"U S Fish and Wildlife Service\": \"U.S. Fish and Wildlife Service\",\n",
    "        \"U S Environmental Protection Agency\": \"United States. Environmental Protection Agency\",\n",
    "        \"U S Department of Agriculture\": \"United States. Department of Agriculture\",\n",
    "        \"U S Census Bureau\": \"U.S. Census Bureau\"\n",
    "    }\n",
    "    \n",
    "    # If a direct mapping is found, return the transformed value\n",
    "    if publisher in publisher_mappings:\n",
    "        return publisher_mappings[publisher]\n",
    "    \n",
    "    # Search for a county name in the publisher string.\n",
    "    for county in counties_in_pennsylvania:\n",
    "        if county + \" County\" in publisher:\n",
    "            return f\"Pennsylvania--{county} County\"\n",
    "    else:\n",
    "        for city in cities_in_pennsylvania:\n",
    "            if f\"City of {city}\" in publisher or city == publisher:\n",
    "                return f\"Pennsylvania--{city}\"\n",
    "    \n",
    "    # If no match found, return the original publisher string.\n",
    "    return publisher\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(transform_publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623595ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversion mappings from old values to new values\n",
    "subject_sm_mapping = {\n",
    "    \"farming\": \"Agriculture\",\n",
    "    \"farmin\": \"Agriculture\",\n",
    "    \"biota\": \"Biology\",\n",
    "    \"boundaries\": \"Boundaries\",\n",
    "    \"climatologymeteorologyatmosphere\": \"Climate\",\n",
    "    \"economy\": \"Economy\",\n",
    "    \"elevation\": \"Elevation\",\n",
    "    \"elevation data\": \"Elevation\",\n",
    "    \"environment\": \"Environment\",\n",
    "    \"environmental\": \"Environment\",\n",
    "    \"society; climatologyMeteorologyAtmosphere\": \"Events\",\n",
    "    \"geoscientificinformation\": \"Geology\",\n",
    "    \"health\": \"Health\",\n",
    "    \"imagerybasemapsearthcover\": \"Imagery|Land Cover\",\n",
    "    \"inlandwaters\": \"Inland Waters\",\n",
    "    \"location\": \"Location\",\n",
    "    \"intelligencemilitary\": \"Military\",\n",
    "    \"oceans\": \"Oceans\",\n",
    "    \"planningcadastre\": \"Property\",\n",
    "    \"planning\": \"Property\",\n",
    "    \"parcel\": \"Property\",\n",
    "    \"zoning\": \"Property\",\n",
    "    \"society\": \"Society\",\n",
    "    \"structure\": \"Structure\",\n",
    "    \"transportation\": \"Transportation\",\n",
    "    \"utilitiescommunication\": \"Utilities\"\n",
    "    \n",
    "    # Add more key-value pairs for other conversions as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    subject_values = row['Keyword']\n",
    "    if pd.notna(subject_values):  # Check for NaN before splitting\n",
    "        subject_values = subject_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in subject_values:\n",
    "            value_lower = value.lower()\n",
    "            if value_lower in subject_sm_mapping:\n",
    "                converted_values.append(subject_sm_mapping[value_lower])\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "df['Theme'] = df.apply(convert_and_join, axis=1)\n",
    "\n",
    "# Drop duplicates from the \"Theme\" column\n",
    "df['Theme'] = df['Theme'].str.split('|').apply(lambda x: '|'.join(sorted(set(x), key=x.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e55dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Theme',\n",
    "'Keyword',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Download',\n",
    "'HTML',\n",
    "'Information',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Access Rights',\n",
    "\n",
    "# # Add more columns as needed in the desired order\n",
    " ]\n",
    "\n",
    "# # Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80da07-e35c-4891-8e82-c44d3b516b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove punctuation characters from the beginning of a string\n",
    "def remove_special_characters(title):\n",
    "    # Use regular expression to remove special characters at the beginning of the title\n",
    "    cleaned_title = re.sub(r'^[^a-zA-Z0-9]+', '', title)\n",
    "    return cleaned_title\n",
    "\n",
    "# Apply the function to the \"Title\" column\n",
    "df['Title'] = df['Title'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20d06f-09b7-4b2d-b81e-d2eb0606d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'output_{actionDate}.csv', index=False)\n",
    "print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
