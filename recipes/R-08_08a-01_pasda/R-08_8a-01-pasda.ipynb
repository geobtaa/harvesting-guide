{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3638f749",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e3239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "import time\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773bdb3",
   "metadata": {},
   "source": [
    "## Part 1: Obtain a list of dataset pages and query the discovery metadata\n",
    "\n",
    "We use PASDA's search feature to return a page (https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+) that lists all of the activate dataset landing pages with some metadata. Then, we use the Beautiful Soup module to query this page and harvest the following values:\n",
    "\n",
    "- Title\n",
    "- Date Issued\n",
    "- Publisher\n",
    "- Description\n",
    "- Metadata file link\n",
    "- Download link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1000d",
   "metadata": {},
   "source": [
    "### MANUAL STEP!!\n",
    "\n",
    "1. Open \"https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+\" in a browser\n",
    "2. Download the page\n",
    "3. Save the file as \"pasda-search.html\" in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b45976",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read the downloaded file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'pasda-search.html'  # Modify this to the correct path to your downloaded HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Assuming every dataset is contained in its own <tr> tag\n",
    "datasets = soup.select('tr[align=\"left\"]')\n",
    "\n",
    "data = []\n",
    "\n",
    "dataset_entries = soup.select('td > h3 > a[href^=\"DataSummary.aspx?dataset=\"]')\n",
    "\n",
    "for entry in dataset_entries:\n",
    "    publisher = entry.find_next(\"td\").text.strip()\n",
    "    date = entry.find_previous(\"td\").find_previous(\"td\").text.strip()\n",
    "    title = entry.text.strip()\n",
    "    description = entry.find_next(\"span\", id=lambda x: x and x.startswith('DataGrid1_Label3_')).text.strip()\n",
    "    metadataFile = entry.parent.parent.find('a', href=True, string='Metadata')['href']\n",
    "    metadataLink = \"https://www.pasda.psu.edu/uci/\" + metadataFile \n",
    "    try:\n",
    "        download = entry.parent.parent.find('a', href=True, string='Download')['href']\n",
    "    except:\n",
    "        download = ''\n",
    "        \n",
    "    # obtain full landing page and create ID\n",
    "    landing_page = \"https://www.pasda.psu.edu/uci/\" + entry['href']  # Landing page URL\n",
    "    iden = 'pasda-' + landing_page.rsplit(\"=\",1)[1]\n",
    "\n",
    "    data.append([publisher, date, title, description, metadataFile, metadataLink, download, landing_page, iden])\n",
    "    \n",
    "\n",
    "# Convert to pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['Creator', 'Date Issued', 'Alternative Title', 'Description', 'Metadata File', 'HTML', 'Download', 'Information', 'ID'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d24510-4a72-419c-80c4-69a0126e437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: check the results\n",
    "df\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'pasda-aardvark_{actionDate}.csv', index=False)\n",
    "print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa55394",
   "metadata": {},
   "source": [
    "## Part 2: Download the supplemental metadata\n",
    "\n",
    "Context: Most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'HTML\" column. Although these files are created as XMLs, the link is a rendered HTML.\n",
    "\n",
    "There is additional information in these files that we want to scrape, including bounding boxes and geometry type.\n",
    "\n",
    "We will start by downloading the metadata files - this will save time and reduce the load on PASDA's servers because this part of the recipe may need to be run multiple times after troubleshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbf4a1",
   "metadata": {},
   "source": [
    "### Download the metadata files to a folder called \"metadata_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e10ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory named 'metadata_files' to store the downloaded files\n",
    "download_folder = 'metadata_files'\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af38578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, folder):\n",
    "    \"\"\"\n",
    "    Download a file given its URL and store it in the specified folder.\n",
    "    \"\"\"\n",
    "    # Get the filename from the URL\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Handle the response's content in chunks (useful for large files)\n",
    "    with open(os.path.join(folder, filename), 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb4f70-c481-420c-8f76-5af1ad8c07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in df['HTML']:\n",
    "    try:\n",
    "        download_file(url, download_folder)\n",
    "        print(f\"Downloaded {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}. Reason: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a134b7f",
   "metadata": {},
   "source": [
    "## Part 3: Query the downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051feb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "WEST_BOUNDING = 'West_Bounding_Coordinate:'\n",
    "SOUTH_BOUNDING = 'South_Bounding_Coordinate:'\n",
    "EAST_BOUNDING = 'East_Bounding_Coordinate:'\n",
    "NORTH_BOUNDING = 'North_Bounding_Coordinate:'\n",
    "DIRECT_SPATIAL = 'Direct_Spatial_Reference_Method:'\n",
    "THEME_KEYWORD = 'Theme_Keyword:'\n",
    "PLACE_KEYWORD = 'Place_Keyword:'\n",
    "\n",
    "# Extract bounding box from a metadata file\n",
    "def extract_bbox(soup):\n",
    "    try:\n",
    "        west = soup.find('i', string=WEST_BOUNDING).next_sibling.strip()\n",
    "    except AttributeError:\n",
    "        west = '-80.52'\n",
    "    try:\n",
    "        south = soup.find('i', string='South_Bounding_Coordinate:').next_sibling.strip()   \n",
    "    except AttributeError:\n",
    "        south = '39.72'\n",
    "\n",
    "    try:\n",
    "        east = soup.find('i', string='East_Bounding_Coordinate:').next_sibling.strip()   \n",
    "    except AttributeError:\n",
    "        east = '-74.69'\n",
    "    try:\n",
    "        north = soup.find('i', string='North_Bounding_Coordinate:').next_sibling.strip()   \n",
    "    except AttributeError:\n",
    "        north = '42.51'\n",
    "\n",
    "    return f\"{west},{south},{east},{north}\"\n",
    "\n",
    "# Extract spatial reference method from a metadata file\n",
    "def extract_spatial_ref(soup):\n",
    "    try:\n",
    "        res_type = soup.find('i', string=DIRECT_SPATIAL).next_sibling.strip() + ' data'\n",
    "    except AttributeError:\n",
    "        res_type = ''\n",
    "    return res_type\n",
    "\n",
    "# Extract keywords from a metadata file\n",
    "def extract_keywords(soup, keyword_type):\n",
    "    try:\n",
    "        keywords = soup.findAll('i', string=keyword_type)\n",
    "        return \"|\".join([kw.next_sibling.strip() for kw in keywords])\n",
    "    except Exception as e:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1d618-2bbd-4156-8134-50592d32f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "metadata_folder = \"metadata_files\"\n",
    "bounding_boxes = {}\n",
    "spatial_reference_methods = {}\n",
    "theme_keywords_list = []\n",
    "place_keywords_list = []\n",
    "\n",
    "\n",
    "\n",
    "for idx, metadata_file in enumerate(df['Metadata File']):\n",
    "    try:\n",
    "        file_path = os.path.join(metadata_folder, metadata_file)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "            soup = BeautifulSoup(file_content, \"html.parser\")\n",
    "            \n",
    "            print(f\"Processing metadata file {idx + 1} of {len(df)}: {metadata_file}\")  # Print progress\n",
    "\n",
    "            bbox = extract_bbox(soup)\n",
    "            res_type = extract_spatial_ref(soup)\n",
    "            theme_keywords = extract_keywords(soup, THEME_KEYWORD)\n",
    "            place_keywords = extract_keywords(soup, PLACE_KEYWORD)\n",
    "\n",
    "            bounding_boxes[metadata_file] = bbox\n",
    "            spatial_reference_methods[metadata_file] = res_type\n",
    "            \n",
    "            theme_keywords_list.append(theme_keywords)\n",
    "            place_keywords_list.append(place_keywords)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing metadata file {metadata_file}: {e}\")  # Print error message\n",
    "\n",
    "# Convert lists to Pandas Series\n",
    "theme_keywords_series = pd.Series(theme_keywords_list)\n",
    "place_keywords_series = pd.Series(place_keywords_list)\n",
    "\n",
    "\n",
    "\n",
    "df['Bounding Box'] = df['Metadata File'].map(bounding_boxes)\n",
    "df['Resource Type'] = df['Metadata File'].map(spatial_reference_methods)\n",
    "df['Resource Class'] = np.where(df['Resource Type'] == 'Raster data', 'Imagery', 'Datasets')\n",
    "df['Keyword'] = theme_keywords_series\n",
    "df['Spatial Coverage'] = place_keywords_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f483e-2c7a-4866-94ca-b1aeabb09dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: check the results\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fed73",
   "metadata": {},
   "source": [
    "## Part 4: add default and calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fee390-aab5-4829-b2d2-6b9a90cb23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range_formatter(date_issued):\n",
    "    # Extract years\n",
    "    years = re.findall(r'(\\d{4})', date_issued)\n",
    "    # If only one year is found, duplicate it to create a range\n",
    "    if len(years) == 1:\n",
    "        return f\"{years[0]}-{years[0]}\"\n",
    "    # If two years are found, format them as a range\n",
    "    elif len(years) == 2:\n",
    "        return f\"{years[0]}-{years[1]}\"\n",
    "    # Return original string if no match (or any other behavior you prefer)\n",
    "    else:\n",
    "        return date_issued\n",
    "\n",
    "df['Date Range'] = df['Date Issued'].apply(date_range_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1acc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append default values\n",
    "\n",
    "df['Code'] = '08a-01'\n",
    "df['Access Rights'] = 'Public'\n",
    "df['Accrual Method'] = 'HTML'\n",
    "df['Date Accessioned'] = time.strftime('%Y-%m-%d')\n",
    "df['Language'] = 'eng'\n",
    "df['Is Part Of'] = '08a-01'\n",
    "df['Member Of'] = 'ba5cc745-21c5-4ae9-954b-72dd8db6815a'\n",
    "df['Provider'] = 'Pennsylvania Spatial Data Access (PASDA)'\n",
    "df['Identifier'] = df['Information']\n",
    "df['Format'] = 'File'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef467d5",
   "metadata": {},
   "source": [
    "### Clean up the titles\n",
    "\n",
    "Title-case the \"Alternative Title\".\n",
    "Check for counties in the title and reformat accordingly.\n",
    "If no county is found, check for cities in the title and reformat accordingly.\n",
    "If neither county nor city is found, it checks for \"PA \" and replaces it with \"[Pennsylvania]\".\n",
    "It then captures the content in brackets, removes it from its original position, and appends it to the end of the title.\n",
    "Some specific transformations (cleanup) are performed post-transformation.\n",
    "The value from 'Date Issued' is appended at the end of the title, surrounded by curly brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1825f6e-99c5-418d-ac47-0366e3f0808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_in_pennsylvania = [\n",
    "    'Adams', 'Allegheny', 'Armstrong', 'Beaver', 'Bedford', 'Berks',\n",
    "    'Blair', 'Bradford', 'Bucks', 'Butler', 'Cambria', 'Cameron',\n",
    "    'Carbon', 'Centre', 'Chester', 'Clarion', 'Clearfield', 'Clinton',\n",
    "    'Columbia', 'Crawford', 'Cumberland', 'Dauphin', 'Delaware', 'Elk',\n",
    "    'Erie', 'Fayette', 'Forest', 'Franklin', 'Fulton', 'Greene',\n",
    "    'Huntingdon', 'Indiana', 'Jefferson', 'Juniata', 'Lackawanna',\n",
    "    'Lancaster', 'Lawrence', 'Lebanon', 'Lehigh', 'Luzerne', 'Lycoming',\n",
    "    'McKean', 'Mercer', 'Mifflin', 'Monroe', 'Montgomery', 'Montour',\n",
    "    'Northampton', 'Northumberland', 'Perry', 'Philadelphia', 'Pike',\n",
    "    'Potter', 'Schuylkill', 'Snyder', 'Somerset', 'Sullivan', 'Susquehanna',\n",
    "    'Tioga', 'Union', 'Venango', 'Warren', 'Washington', 'Wayne',\n",
    "    'Westmoreland', 'Wyoming', 'York'\n",
    "]\n",
    "\n",
    "cities_in_pennsylvania = [\n",
    "    'Allentown', 'Altoona', 'Bethlehem', 'Butler', 'Carlisle', 'Chambersburg', \n",
    "    'Chester', 'DuBois', 'Easton', 'Erie', 'Greensburg', 'Hanover', \n",
    "    'Harrisburg', 'Hazleton', 'Hermitage', 'Johnstown', 'Lancaster', \n",
    "    'Latrobe', 'Lebanon', 'Lock Haven', 'Lower Merion', 'McKeesport', \n",
    "    'Meadville', 'Monroeville', 'Nanticoke', 'New Castle', 'Norristown', \n",
    "    'Philadelphia', 'Phoenixville', 'Pittsburgh', 'Pottstown', 'Pottsville', \n",
    "    'Reading', 'Scranton', 'Sharon', 'State College', 'Uniontown', \n",
    "    'Warren', 'Washington', 'West Chester', 'Wilkes-Barre', 'Williamsport', 'York'\n",
    "]\n",
    "\n",
    "\n",
    "def transform_title(row):\n",
    "    alt_title = row['Alternative Title']\n",
    "    \n",
    "    # Search for a city or county name in the title.\n",
    "    for county in counties_in_pennsylvania:\n",
    "        if re.search(f\"{county} County\", alt_title, re.I):\n",
    "            alt_title = re.sub(f\"{county} County\", f\"[Pennsylvania--{county} County]\", alt_title, flags=re.I, count=1)\n",
    "            break\n",
    "    else:\n",
    "        for city in cities_in_pennsylvania:\n",
    "            if re.search(f\"\\b{city}\\b\", alt_title, re.I):\n",
    "                alt_title = re.sub(f\"\\b{city}\\b\", f\"[Pennsylvania--{city}]\", alt_title, flags=re.I, count=1)\n",
    "                break\n",
    "        else:\n",
    "            alt_title = re.sub(r\"\\b(PA|Pennsylvania)\\b\", \"[Pennsylvania]\", alt_title, flags=re.I, count=1)\n",
    "\n",
    "    # Capture content in brackets\n",
    "    bracket_content = re.findall(r'\\[(.*?)\\]', alt_title)\n",
    "    \n",
    "    if bracket_content:\n",
    "        # Remove bracketed content from original position\n",
    "        alt_title = re.sub(r'\\[.*?\\]', '', alt_title).strip()\n",
    "        \n",
    "        # Append bracketed content to the end of the title\n",
    "        alt_title = f\"{alt_title} [{bracket_content[0]}]\"\n",
    "\n",
    "    # Cleanup phrases post-transformation using case-insensitive matching\n",
    "    alt_title = re.sub(r\"For \\[\", \"[\", alt_title, flags=re.I)\n",
    "    alt_title = re.sub(r\"For The \\[\", \"[\", alt_title, flags=re.I)\n",
    "    alt_title = re.sub(r\"For The City Of \\[\", \"[\", alt_title, flags=re.I)\n",
    "\n",
    "    # Remove unwanted dashes at the beginning or just before a bracket\n",
    "    alt_title = re.sub(r\"^\\s*-\\s*|\\s*-\\s*(?=\\[)\", \"\", alt_title)\n",
    "    \n",
    "    # Make sure first letter is capitalized\n",
    "    alt_title = alt_title[0].capitalize() + alt_title[1:]\n",
    "\n",
    "    # Append the value from 'Date Issued' surrounded by curly brackets\n",
    "    alt_title += f\" {{{row['Date Issued']}}}\"\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "df['Title'] = df.apply(transform_title, axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8b3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_publisher(publisher):\n",
    "    # Dictionary mapping of publishers for direct transformation\n",
    "    publisher_mappings = {\n",
    "        \"U S Geological Survey\": \"Geological Survey (U.S.)\",\n",
    "        \"U S Fish and Wildlife Service\": \"U.S. Fish and Wildlife Service\",\n",
    "        \"U S Environmental Protection Agency\": \"United States. Environmental Protection Agency\",\n",
    "        \"U S Department of Agriculture\": \"United States. Department of Agriculture\",\n",
    "        \"U S Census Bureau\": \"U.S. Census Bureau\"\n",
    "    }\n",
    "    \n",
    "    # If a direct mapping is found, return the transformed value\n",
    "    if publisher in publisher_mappings:\n",
    "        return publisher_mappings[publisher]\n",
    "    \n",
    "    # Search for a county name in the publisher string.\n",
    "    for county in counties_in_pennsylvania:\n",
    "        if county + \" County\" in publisher:\n",
    "            return f\"Pennsylvania--{county} County\"\n",
    "    else:\n",
    "        for city in cities_in_pennsylvania:\n",
    "            if f\"City of {city}\" in publisher or city == publisher:\n",
    "                return f\"Pennsylvania--{city}\"\n",
    "    \n",
    "    # If no match found, return the original publisher string.\n",
    "    return publisher\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(transform_publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "623595ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversion mappings from old values to new values\n",
    "subject_sm_mapping = {\n",
    "    \"farming\": \"Agriculture\",\n",
    "    \"farmin\": \"Agriculture\",\n",
    "    \"biota\": \"Biology\",\n",
    "    \"boundaries\": \"Boundaries\",\n",
    "    \"climatologymeteorologyatmosphere\": \"Climate\",\n",
    "    \"economy\": \"Economy\",\n",
    "    \"elevation\": \"Elevation\",\n",
    "    \"elevation data\": \"Elevation\",\n",
    "    \"environment\": \"Environment\",\n",
    "    \"environmental\": \"Environment\",\n",
    "    \"society; climatologyMeteorologyAtmosphere\": \"Events\",\n",
    "    \"geoscientificinformation\": \"Geology\",\n",
    "    \"health\": \"Health\",\n",
    "    \"imagerybasemapsearthcover\": \"Imagery|Land Cover\",\n",
    "    \"inlandwaters\": \"Inland Waters\",\n",
    "    \"location\": \"Location\",\n",
    "    \"intelligencemilitary\": \"Military\",\n",
    "    \"oceans\": \"Oceans\",\n",
    "    \"planningcadastre\": \"Property\",\n",
    "    \"planning\": \"Property\",\n",
    "    \"parcel\": \"Property\",\n",
    "    \"zoning\": \"Property\",\n",
    "    \"society\": \"Society\",\n",
    "    \"structure\": \"Structure\",\n",
    "    \"transportation\": \"Transportation\",\n",
    "    \"utilitiescommunication\": \"Utilities\"\n",
    "    \n",
    "    # Add more key-value pairs for other conversions as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    subject_values = row['Keyword']\n",
    "    if pd.notna(subject_values):  # Check for NaN before splitting\n",
    "        subject_values = subject_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in subject_values:\n",
    "            value_lower = value.lower()\n",
    "            if value_lower in subject_sm_mapping:\n",
    "                converted_values.append(subject_sm_mapping[value_lower])\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "df['Theme'] = df.apply(convert_and_join, axis=1)\n",
    "\n",
    "# Drop duplicates from the \"Theme\" column\n",
    "df['Theme'] = df['Theme'].str.split('|').apply(lambda x: '|'.join(sorted(set(x), key=x.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e55dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Theme',\n",
    "'Keyword',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Download',\n",
    "'HTML',\n",
    "'Information',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Access Rights',\n",
    "\n",
    "# # Add more columns as needed in the desired order\n",
    " ]\n",
    "\n",
    "# # Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20d06f-09b7-4b2d-b81e-d2eb0606d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'output_{actionDate}.csv', index=False)\n",
    "print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
