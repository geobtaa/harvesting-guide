{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566a209c",
   "metadata": {},
   "source": [
    "# PASDA Harvest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c043e",
   "metadata": {},
   "source": [
    "[Pennsylvania Spatial Data Access (PASDA](https://www.pasda.psu.edu) is the stateâ€™s comprehensive GIS clearinghouse. Most Pennsylvania statewide agencies and regional organizations provide their data through this site. Many counties and cities do as well.\n",
    "\n",
    "\n",
    "Part 1: Parse the PASDA portal\n",
    "1. Use the script `getURLs.py` to obtain a list of all of the records currently in PASDA. The resulting CSV will be called `URLS_{today's date}.csv` which is just a list of the landing pages for the datasets in the PASDA portal.\n",
    "\n",
    "\n",
    "2. Use the `pasdaURLS_{today's date}.csv` and the `html2csv.py`script to scrape the metadata from the PASDA landing pages. This resulting CSV will be called `output_{today's date}.csv`.\n",
    "\n",
    "\n",
    "Part 2: Extract the bounding boxes\n",
    "Context: most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'Metadata\" column.\n",
    "Although these files are created as XMLs, the link is a rendered HTML.\n",
    "\n",
    "1. Create a CSV file that is a list of just the metadata file pages. See sample-inputMetadataUrls.csv for an example.\n",
    "\n",
    "2. Use the getBbox.py script to parse the files and extract the bounding boxes.\n",
    "3. Merge these values back into the output CSV from Part 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3720d7",
   "metadata": {},
   "source": [
    "## Part 1: Get URLs\n",
    "\n",
    "Purpose: This script will crawl the PASDA website and return all of the published dataset landing pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5c447",
   "metadata": {},
   "source": [
    "### Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # The csv module implements classes to read and write tabular data in CSV format.\n",
    "import time # This module provides various time-related functions.\n",
    "import urllib.request # The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP)\n",
    "from bs4 import BeautifulSoup # For pulling data out of HTML and XML files\n",
    "import re\n",
    "from urllib.request import urlopen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddda94",
   "metadata": {},
   "source": [
    "### Search for the ID of all of the published items.\n",
    "\n",
    "PASDA's website layout does not have a page that lists all of the published datasets. Therefore, we will perform an  empty search using the keyword value '+' to find all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74081bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the main search page\n",
    "resURL = 'https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+'\n",
    "page = urllib.request.urlopen(resURL).read()\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "# Identify landing page URLs inside <h3> tags\n",
    "landing_page_links = soup.select('h3 a[href^=\"DataSummary.aspx?dataset=\"]')\n",
    "landing_pages = ['https://www.pasda.psu.edu/uci/' + link['href'] for link in landing_page_links]\n",
    "\n",
    "# For testing the code of just the first 5\n",
    "landing_pages = landing_pages[:5]  # Keep only the first 5 landing pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6c203",
   "metadata": {},
   "source": [
    "### Write a CSV file that contains a list of the landing page URLs for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full landing pages to a CSV\n",
    "# actionDate = time.strftime('%Y%m%d')\n",
    "# with open(f'LandingPages_{actionDate}.csv', 'w', newline='') as fw:\n",
    "#     writer = csv.writer(fw)\n",
    "#     writer.writerow([\"Landing Page URL\"])  # Write header\n",
    "#     for url in landing_pages:\n",
    "#         writer.writerow([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f8f61",
   "metadata": {},
   "source": [
    "## Part 2: Parse the discovery metadata on each page\n",
    "\n",
    "Next, this script will scan each page listed in the URLs_{actionDate}.csv file to look for the following information:\n",
    "\n",
    "- Title\n",
    "- Date\n",
    "- Publisher\n",
    "- Description\n",
    "- Metadata Link (a linked supplemental file - more details on this in Part 3)\n",
    "\n",
    "The script will then write a new CSV file in the GeoBTAA template format that contains all of this parsed information as well as default values for other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a796a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '08a-01'  # Portal Hub Header\n",
    "accessRights = \"Public\" # Access Rights Header\n",
    "accrualMethod = \"HTML\" # Accrual Method Header\n",
    "dateAccessioned = time.strftime('%Y-%m-%d') # Data Accessioned Header\n",
    "language = \"eng\" # Language Header\n",
    "isPartOf = \"08a-01\" # Part of Portal Header\n",
    "memberOf = \"ba5cc745-21c5-4ae9-954b-72dd8db6815a\" # Member of Link Header\n",
    "provider = \"Pennsylvania Spatial Data Access (PASDA)\" # Provider Agency Header\n",
    "resourceClass = \"\" # Resource Class Header\n",
    "resourceType = '' # Resource Type Header\n",
    "dateRange = '' # Date Range Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parseElements = []\n",
    "\n",
    "# Loop through the landing pages\n",
    "for url in landing_pages:\n",
    "    page = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    landingPage = url\n",
    "    iden = 'pasda-' + landingPage.rsplit(\"=\",1)[1]\n",
    "\n",
    "    # Extract metadata fields\n",
    "    title = soup.find(attrs={'id': 'Label1'}).text.strip()\n",
    "    date = soup.find(attrs={'id': 'Label2'}).text.strip()\n",
    "    publisher = soup.find(attrs={'id': 'Label3'}).text.strip()\n",
    "    description = soup.find(attrs={'id': 'Label14'}).text.strip()\n",
    "    \n",
    "    metadataLink = soup.find('a', href=True, string='Metadata')\n",
    "    downloadLink = soup.find('a', href=True, string='Download')\n",
    "    \n",
    "    metadata = \"https://www.pasda.psu.edu/uci/\" + metadataLink['href']\n",
    "    try:\n",
    "        download = downloadLink['href']\n",
    "    except:\n",
    "        download = ''\n",
    "            \n",
    "    parseElements.append([landingPage,iden,title,date,dateRange,publisher,provider,language,description,resourceClass,resourceType,metadata,download,code,isPartOf,memberOf,accessRights,accrualMethod,dateAccessioned])\n",
    "    \n",
    "# Generate action date with format YYYYMMDD\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "\n",
    "# Write outputs to a CSV file\n",
    "with open(f'output_{actionDate}.csv', 'w') as fw:\n",
    "    fields = ['Information','ID','Title','Temporal Coverage','Date Range','Publisher','Provider','Language','Description','Resource Class','Resource Type','HTML','Download','Code','Is Part Of','Member Of','Access Rights','Accrual Method','Date Accessioned']\n",
    "\n",
    "    writer = csv.writer(fw)\n",
    "    writer.writerow(fields)\n",
    "    writer.writerows(parseElements)\n",
    "\n",
    "    print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1d1b4",
   "metadata": {},
   "source": [
    "## Part 3: Extract the bounding boxes\n",
    "\n",
    "Context: The bounding box coordinates are not part of the discovery metadata on each item's landing page. However, most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'Metadata\" column. Although these files are created as XMLs, the link is a rendered HTML, so we can parse them using a similar method as shown in part 2.\n",
    "\n",
    "1. Create a CSV file that is a list of just the metadata file pages. See sample-inputMetadataUrls.csv for an example.\n",
    "2. Use the getBbox.py script to parse the files and extract the bounding boxes.\n",
    "3. Merge these values back into the output CSV from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79ad597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Job done ####\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to maintain the bounding boxes\n",
    "bounding_boxes = {}\n",
    "resource_type = {}\n",
    "\n",
    "# Extract bounding boxes for the metadata URLs present in `parseElements`\n",
    "for element in parseElements:\n",
    "    metadata_url = element[11]  # This index might change if you modify your parseElements structure\n",
    "    try:\n",
    "        page = urlopen(metadata_url).read()\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        \n",
    "        try:\n",
    "            west = soup.find('i', string='West_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            west = ''\n",
    "        \n",
    "        try:\n",
    "            south = soup.find('i', string='South_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            south = ''\n",
    "        \n",
    "        try:\n",
    "            east = soup.find('i', string='East_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            east = ''\n",
    "        \n",
    "        try:\n",
    "            north = soup.find('i', string='North_Bounding_Coordinate:').next_sibling.strip()   \n",
    "        except:\n",
    "            north = ''\n",
    "        \n",
    "        bbox = west + ',' + south + ',' +east + ',' + north\n",
    "    except:\n",
    "        bbox = \"missing\"\n",
    "        \n",
    "    bounding_boxes[metadata_url] = bbox\n",
    "\n",
    "            \n",
    "\n",
    "# You might want to add a column in your parseElements for the bounding box or replace an existing one.\n",
    "\n",
    "# Generate action date with format YYYYMMDD\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "\n",
    "# Write outputs to local CSV file\n",
    "with open(f'output_{actionDate}.csv', 'w') as fw:\n",
    "    fields = ['Information', 'ID', 'Title', 'Temporal Coverage', 'Date Range', 'Publisher', 'Provider', 'Language', 'Description', 'Resource Class', 'Resource Type', 'HTML', 'Download', 'Code', 'Is Part Of', 'Member Of', 'Access Rights', 'Accrual Method', 'Date Accessioned', 'Bounding Box']\n",
    "\n",
    "    writer = csv.writer(fw)\n",
    "    writer.writerow(fields)\n",
    "    for element in parseElements:\n",
    "        metadata_url = element[11]  # This index might change if you modify your parseElements structure\n",
    "        bbox = bounding_boxes.get(metadata_url, 'missing')  # Get the bounding box or default to 'missing'\n",
    "        element.append(bbox)  # Append the bounding box to the element. Adjust if you want to replace an existing column.\n",
    "        writer.writerow(element)\n",
    "\n",
    "    print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb21522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
