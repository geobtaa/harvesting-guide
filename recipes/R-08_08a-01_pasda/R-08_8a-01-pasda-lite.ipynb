{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3638f749",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "65c061c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45e3239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv \n",
    "import time\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')  # Add the parent directory to the path\n",
    "\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773bdb3",
   "metadata": {},
   "source": [
    "## Part 1: Obtain a list of dataset pages and query the discovery metadata\n",
    "\n",
    "We use PASDA's search feature to return a page (https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+) that lists all of the activate dataset landing pages with some metadata. Then, we use the Beautiful Soup module to query this page and harvest the following values:\n",
    "\n",
    "- Title\n",
    "- Date Issued\n",
    "- Publisher\n",
    "- Description\n",
    "- Metadata file link\n",
    "- Download link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1000d",
   "metadata": {},
   "source": [
    "### MANUAL STEP!!\n",
    "\n",
    "1. Open https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+ in a browser\n",
    "2. Download the page\n",
    "3. Save the file as \"pasda-search.html\" in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827524fc",
   "metadata": {},
   "source": [
    "### Read the downloaded file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66b246ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'pasda-search.html'  # Modify this to the correct path to your downloaded HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Assuming every dataset is contained in its own <tr> tag\n",
    "datasets = soup.select('tr[align=\"left\"]')\n",
    "\n",
    "data = []\n",
    "\n",
    "dataset_entries = soup.select('td > h3 > a[href^=\"DataSummary.aspx?dataset=\"]')\n",
    "\n",
    "for entry in dataset_entries:\n",
    "    publisher = entry.find_next(\"td\").text.strip()\n",
    "    date = entry.find_previous(\"td\").find_previous(\"td\").text.strip()\n",
    "    title = entry.text.strip()\n",
    "    description = entry.find_next(\"span\", id=lambda x: x and x.startswith('DataGrid1_Label3_')).text.strip()\n",
    "    metadataFile = entry.parent.parent.find('a', href=True, string='Metadata')['href']\n",
    "    metadataLink = \"https://www.pasda.psu.edu/uci/\" + metadataFile \n",
    "    try:\n",
    "        download = entry.parent.parent.find('a', href=True, string='Download')['href']\n",
    "    except:\n",
    "        download = ''\n",
    "        \n",
    "    # obtain full landing page and create ID\n",
    "    landing_page = \"https://www.pasda.psu.edu/uci/\" + entry['href']  # Landing page URL\n",
    "    iden = 'pasda-' + landing_page.rsplit(\"=\",1)[1]\n",
    "\n",
    "    data.append([publisher, date, title, description, metadataFile, metadataLink, download, landing_page, iden])\n",
    "    \n",
    "\n",
    "# Convert to pandas dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['Creator', 'Date Issued', 'Alternative Title', 'Description', 'Metadata File', 'metadata_html', 'download', 'full_layer_description', 'ID'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "11d24510-4a72-419c-80c4-69a0126e437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Job done ####\n"
     ]
    }
   ],
   "source": [
    "# optional: check the results\n",
    "df\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "df.to_csv(f'pasda-minimal_{actionDate}.csv', index=False)\n",
    "print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b941d",
   "metadata": {},
   "source": [
    "### Drop federal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "779c1f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of creator values to drop\n",
    "values_to_drop = [\n",
    "    \"United States Army Corps of Engineers USACE\",\n",
    "    \"U S Geological Survey\",\n",
    "    \"U S Fish and Wildlife Service\",\n",
    "    \"U S Environmental Protection Agency\",\n",
    "    \"U S Department of Justice\",\n",
    "    \"U S Department of Commerce\",\n",
    "    \"U S Department of Agriculture\",\n",
    "    \"U S Census Bureau\",\n",
    "    \"National Weather Service NOAA NWS\",\n",
    "    \"National Renewable Energy Laboratory NREL\",\n",
    "    \"National Park Service\",\n",
    "    \"National Geodetic Survey\",\n",
    "    \"National Aeronautics and Space Administration NASA\"\n",
    "]\n",
    "\n",
    "# Drop rows where 'Creator' column contains any of these values\n",
    "df = df[~df['Creator'].isin(values_to_drop)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9fed73",
   "metadata": {},
   "source": [
    "## Part 3: add default and calculated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "89fee390-aab5-4829-b2d2-6b9a90cb23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range_formatter(date_issued):\n",
    "    # Extract years\n",
    "    years = re.findall(r'(\\d{4})', date_issued)\n",
    "    # If only one year is found, duplicate it to create a range\n",
    "    if len(years) == 1:\n",
    "        return f\"{years[0]}-{years[0]}\"\n",
    "    # If two years are found, format them as a range\n",
    "    elif len(years) == 2:\n",
    "        return f\"{years[0]}-{years[1]}\"\n",
    "    # Return original string if no match (or any other behavior you prefer)\n",
    "    else:\n",
    "        return date_issued\n",
    "\n",
    "df['Date Range'] = df['Date Issued'].apply(date_range_formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b1acc480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append default values\n",
    "\n",
    "df['Code'] = '08a-01'\n",
    "df['Access Rights'] = 'Public'\n",
    "df['Accrual Method'] = 'HTML'\n",
    "df['Date Accessioned'] = time.strftime('%Y-%m-%d')\n",
    "df['Language'] = 'eng'\n",
    "df['Is Part Of'] = '08a-01'\n",
    "df['Member Of'] = 'ba5cc745-21c5-4ae9-954b-72dd8db6815a'\n",
    "df['Provider'] = 'Pennsylvania Spatial Data Access (PASDA)'\n",
    "df['Identifier'] = df['full_layer_description']\n",
    "df['Format'] = 'File'\n",
    "df['Resource Class'] = 'Datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef467d5",
   "metadata": {},
   "source": [
    "### Clean up the titles\n",
    "\n",
    "Title-case the \"Alternative Title\".\n",
    "Check for counties in the title and reformat accordingly.\n",
    "If no county is found, check for cities in the title and reformat accordingly.\n",
    "If neither county nor city is found, it checks for \"PA \" and replaces it with \"[Pennsylvania]\".\n",
    "It then captures the content in brackets, removes it from its original position, and appends it to the end of the title.\n",
    "Some specific transformations (cleanup) are performed post-transformation.\n",
    "The value from 'Date Issued' is appended at the end of the title, surrounded by curly brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a1825f6e-99c5-418d-ac47-0366e3f0808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.spatial_coverage_transformer import apply_transformations\n",
    "\n",
    "json_path = os.path.join('../../', 'data', 'locations.json')\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    locations = json.load(file)\n",
    "\n",
    "counties_in_pennsylvania = locations['counties_in_pennsylvania']\n",
    "cities_in_pennsylvania = locations['cities_in_pennsylvania']\n",
    "\n",
    "\n",
    "\n",
    "def transform_title(row):\n",
    "    alt_title = row['Alternative Title']\n",
    "    \n",
    "    # Search for a city or county name in the title.\n",
    "    for county in counties_in_pennsylvania:\n",
    "        if re.search(f\"{county} County\", alt_title, re.I):\n",
    "            alt_title = re.sub(f\"{county} County\", f\"[Pennsylvania--{county} County]\", alt_title, flags=re.I, count=1)\n",
    "            break\n",
    "    else:\n",
    "        for city in cities_in_pennsylvania:\n",
    "            if re.search(f\"\\b{city}\\b\", alt_title, re.I):\n",
    "                alt_title = re.sub(f\"\\b{city}\\b\", f\"[Pennsylvania--{city}]\", alt_title, flags=re.I, count=1)\n",
    "                break\n",
    "        else:\n",
    "            alt_title = re.sub(r\"\\b(PA|Pennsylvania)\\b\", \"[Pennsylvania]\", alt_title, flags=re.I, count=1)\n",
    "\n",
    "    # Capture content in brackets\n",
    "    bracket_content = re.findall(r'\\[(.*?)\\]', alt_title)\n",
    "    \n",
    "    if bracket_content:\n",
    "        # Remove bracketed content from original position\n",
    "        alt_title = re.sub(r'\\[.*?\\]', '', alt_title).strip()\n",
    "        \n",
    "        # Append bracketed content to the end of the title\n",
    "        alt_title = f\"{alt_title} [{bracket_content[0]}]\"\n",
    "\n",
    "    # Cleanup phrases post-transformation using case-insensitive matching\n",
    "    alt_title = re.sub(r\"For \\[\", \"[\", alt_title, flags=re.I)\n",
    "    alt_title = re.sub(r\"For The \\[\", \"[\", alt_title, flags=re.I)\n",
    "    alt_title = re.sub(r\"For The City Of \\[\", \"[\", alt_title, flags=re.I)\n",
    "\n",
    "    # Remove unwanted dashes at the beginning or just before a bracket\n",
    "    alt_title = re.sub(r\"^\\s*-\\s*|\\s*-\\s*(?=\\[)\", \"\", alt_title)\n",
    "    \n",
    "    # Make sure first letter is capitalized\n",
    "    alt_title = alt_title[0].capitalize() + alt_title[1:]\n",
    "\n",
    "    # Append the value from 'Date Issued' surrounded by curly brackets\n",
    "    alt_title += f\" {{{row['Date Issued']}}}\"\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "df['Title'] = df.apply(transform_title, axis=1)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cde67ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_transformations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2a8b3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_publisher(publisher):\n",
    "    \n",
    "    # Search for a county name in the publisher string.\n",
    "    for county in counties_in_pennsylvania:\n",
    "        if county + \" County\" in publisher:\n",
    "            return f\"Pennsylvania--{county} County\"\n",
    "    else:\n",
    "        for city in cities_in_pennsylvania:\n",
    "            if f\"City of {city}\" in publisher or city == publisher:\n",
    "                return f\"Pennsylvania--{city}\"\n",
    "    \n",
    "    # If no match found, return the original publisher string.\n",
    "    return publisher\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(transform_publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e12e0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill spatial values\n",
    "\n",
    "from modules.spatial_values import load_csv_data, match_and_append_values\n",
    "\n",
    "\n",
    "# Define the path to the CSV\n",
    "csv_path = os.path.join('../../', 'data', 'spatial_counties.csv')\n",
    "\n",
    "# Load CSV and append data\n",
    "try:\n",
    "    csv_data = load_csv_data(csv_path)\n",
    "    df = match_and_append_values(df, csv_data)  # Update the DataFrame in place\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fd1356fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default values\n",
    "default_values = {\n",
    "    'Bounding Box': \"-80.52,39.72,-74.69,42.27\",\n",
    "    'Geometry': \"MultiPolygon(((-75.6 39.8, -75.8 39.7, -80.5 39.7, -80.5 42.3, -79.8 42.5, -79.8 42, -75.3 42, -75.1 41.8, -75 41.5, -74.7 41.4, -75.1 41, -75.1 40.9, -75.2 40.7, -74.7 40.2, -75.1 39.9, -75.6 39.8)))\",\n",
    "    'GeoNames ID': \"http://sws.geonames.org/6254927\"\n",
    "}\n",
    "\n",
    "# Fill blanks in the DataFrame\n",
    "for column, default in default_values.items():\n",
    "    if column in df.columns:\n",
    "        df[column] = df[column].fillna(default)\n",
    "    else:\n",
    "        df[column] = default  # Create column if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cc80da07-e35c-4891-8e82-c44d3b516b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove punctuation characters from the beginning of a string\n",
    "def remove_special_characters(title):\n",
    "    # Use regular expression to remove special characters at the beginning of the title\n",
    "    cleaned_title = re.sub(r'^[^a-zA-Z0-9]+', '', title)\n",
    "    return cleaned_title\n",
    "\n",
    "# Apply the function to the \"Title\" column\n",
    "df['Title'] = df['Title'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0e55dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the desired order of columns\n",
    "desired_order = [\n",
    "'Spatial Coverage',\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Resource Type',\n",
    "'Theme',\n",
    "'Keyword',\n",
    "'Date Issued',\n",
    "'Date Range',\n",
    "'Date Accessioned',\n",
    "'Bounding Box',\n",
    "'Geometry',\n",
    "'GeoNames ID',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'Accrual Method',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Access Rights',\n",
    "'download',\n",
    "'metadata_html',\n",
    "'full_layer_description'\n",
    " ]\n",
    "\n",
    "# # Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f98b1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punctuation(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove leading and trailing pipes, dashes, or spaces\n",
    "        return text.strip('|- ')\n",
    "    return text\n",
    "\n",
    "# Apply the function to each cell in the DataFrame\n",
    "df = df.applymap(clean_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "008d5438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create the first CSV (all fields except links)\n",
    "# Updated fields for links\n",
    "link_fields = ['full_layer_description', 'download', 'metadata_html']\n",
    "df_first_csv = df.drop(columns=link_fields)\n",
    "df_first_csv.to_csv(f'{actionDate}_pasda-metadata.csv', index=False)\n",
    "\n",
    "# Create the second CSV with friendlier_id, reference_type, distribution_url, and label\n",
    "rows = []\n",
    "\n",
    "for _, r in df.iterrows():\n",
    "    slug = r['ID']\n",
    "    for lf in link_fields:\n",
    "        if pd.notna(r[lf]) and r[lf] != \"\":\n",
    "            # Add Format as the label only if the reference_type is \"download\"\n",
    "            label_value = r['Format'] if lf.lower() == 'download' else ''\n",
    "            rows.append({\n",
    "                'friendlier_id': slug, \n",
    "                'reference_type': lf, \n",
    "                'distribution_url': r[lf], \n",
    "                'label': label_value\n",
    "            })\n",
    "\n",
    "df_second_csv = pd.DataFrame(rows, columns=['friendlier_id', 'reference_type', 'distribution_url', 'label'])\n",
    "df_second_csv.to_csv(f'{actionDate}_pasda-links.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
