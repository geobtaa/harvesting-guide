{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the CKAN API for the Humanitarian Data Exchange and return the metadata for all items as a CSV file in the GeoBTAA Metadata Application Profile. It will also create a secondary CSV file for the associated multiple downloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# auto-generate the current time in 'YYYY-MM-DD' format\n",
    "action_date = time.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare paths and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path to the JSON file\n",
    "output_filename = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://data.humdata.org\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = action_date + \"_99-1400.csv\"  # Update with the desired path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Download the metadata to your desktop\n",
    "\n",
    "This cell will scan the API and create a JSON file on your desktop. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the CKAN portal URL you want to harvest from\n",
    "# packageURL = \"https://data.humdata.org/api/3/action/package_search\"\n",
    "\n",
    "# # Specify the number of items per page\n",
    "# items_per_page = 10\n",
    "\n",
    "# # Initialize variables for pagination\n",
    "# start = 0\n",
    "# total_results = 0\n",
    "\n",
    "# # List to store all metadata\n",
    "# all_metadata = []\n",
    "\n",
    "# # Request metadata in paginated manner\n",
    "# while True:\n",
    "#     try:\n",
    "#         # Construct the API request URL with pagination parameters\n",
    "#         api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "\n",
    "#         # Set up a request with a user-agent\n",
    "#         request = urllib.request.Request(api_request_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        \n",
    "#         # Request metadata\n",
    "#         context = ssl._create_unverified_context()\n",
    "#         response = urllib.request.urlopen(request, context=context, timeout=30)  # Increased timeout\n",
    "\n",
    "#         # Check if the response is valid (status code 200)\n",
    "#         if response.status == 200:\n",
    "#             response_json = json.loads(response.read().decode('utf-8'))\n",
    "            \n",
    "#             # Extract metadata from the response\n",
    "#             metadata = response_json['result']['results']\n",
    "#             all_metadata.extend(metadata)\n",
    "            \n",
    "#             # Update pagination variables\n",
    "#             start += items_per_page\n",
    "#             total_results = response_json['result']['count']\n",
    "            \n",
    "#             # Break the loop if we have collected all items\n",
    "#             if start >= total_results:\n",
    "#                 break\n",
    "#         else:\n",
    "#             print(f\"Failed to fetch data: HTTP status code {response.status}\")\n",
    "#             break\n",
    "\n",
    "#     except urllib.error.URLError as e:\n",
    "#         print(f\"Failed to fetch data: {e.reason}\")\n",
    "#         break\n",
    "\n",
    "# # Save the metadata to a local JSON file on your desktop\n",
    "# desktop_path = \"\"  # Replace with your desktop path\n",
    "# output_filename = \"ckan_metadata.json\"\n",
    "# output_path = os.path.join(desktop_path, output_filename)\n",
    "\n",
    "# with open(output_path, \"w\") as json_file:\n",
    "#     json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "# print(f\"Metadata for {total_results} items saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Read the JSON into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file into a DataFrame\n",
    "all_df = pd.read_json(output_filename)\n",
    "filtered_df = all_df[(all_df['archived'] == False) & (all_df['has_geodata'] == True) & (all_df['private'] == False)]\n",
    "\n",
    "\n",
    "# Specify the columns you want to keep and their new names\n",
    "columns_to_keep_and_rename = {\n",
    "    \"id\": \"ID\",\n",
    "    \"title\": \"Alternative Title\",\n",
    "    \"dataset_source\": \"Publisher\",\n",
    "    \"license_url\": \"License\",\n",
    "    \"notes\": \"Description\",\n",
    "    \"methodology_other\":\"Methodology\",\n",
    "    \"dataset_date\":\"dataset_date\",\n",
    "    \"metadata_created\":\"metadata_created\",\n",
    "    \"name\": \"Identifier\",\n",
    "    \"solr_additions\": \"solr_additions\",\n",
    "    \"resources\": \"resources\",\n",
    "    \"tags\": \"tags\"\n",
    "}\n",
    "\n",
    "# Select and rename the specified columns\n",
    "df = filtered_df[list(columns_to_keep_and_rename.keys())].rename(columns=columns_to_keep_and_rename)\n",
    "\n",
    "# Remove duplicate items\n",
    "df = df.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match the nation place names to bounding boxes from an external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bounding box data\n",
    "bbox_df = pd.read_csv('../../data/nation-bbox.csv')\n",
    "\n",
    "# Function to clean and prepare string data for matching\n",
    "def clean_place_name(name):\n",
    "    return str(name).strip().lower()\n",
    "\n",
    "# Prepare the bounding box DataFrame\n",
    "bbox_df['Label'] = bbox_df['Label'].apply(clean_place_name)\n",
    "bbox_df['altLabel'] = bbox_df['altLabel'].apply(clean_place_name)\n",
    "\n",
    "# Create a dictionary to map both 'Label' and 'altLabel' to the 'Bounding Box'\n",
    "bbox_map = {}\n",
    "for _, row in bbox_df.iterrows():\n",
    "    bbox_map[row['Label']] = row['Bounding Box']\n",
    "    if pd.notna(row['altLabel']):\n",
    "        bbox_map[row['altLabel']] = row['Bounding Box']\n",
    "\n",
    "# Function to parse countries from JSON\n",
    "def parse_countries(places):\n",
    "    try:\n",
    "        # Convert the string to a JSON object\n",
    "        places_json = json.loads(places)\n",
    "        # Extract countries and join with a pipe\n",
    "        countries = '|'.join(places_json.get('countries', []))\n",
    "        return countries\n",
    "    except json.JSONDecodeError:\n",
    "        return ''  # Return an empty string in case of parsing error\n",
    "\n",
    "# Function to find the bounding box for each place name\n",
    "def find_bounding_boxes(place_names):\n",
    "    boxes = []\n",
    "    place_list = place_names.split('|')  # Split the string by pipe\n",
    "    for name in place_list:\n",
    "        clean_name = clean_place_name(name)\n",
    "        box = bbox_map.get(clean_name)\n",
    "        if box:\n",
    "            boxes.append(box)\n",
    "    return '|'.join(boxes) if boxes else None\n",
    "\n",
    "# Apply the function to extract countries\n",
    "df['Spatial Coverage'] = df['solr_additions'].apply(parse_countries)\n",
    "\n",
    "# Drop the \"Places\" column\n",
    "df.drop(columns=['solr_additions'], inplace=True)\n",
    "\n",
    "# Apply the function to find bounding boxes\n",
    "df['Bounding Boxes'] = df['Spatial Coverage'].apply(find_bounding_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge multiple bounding boxes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bounding_boxes(bboxes):\n",
    "    if not bboxes:\n",
    "        return None\n",
    "\n",
    "    # Initialize min and max values with extreme values\n",
    "    min_lon = float('inf')\n",
    "    min_lat = float('inf')\n",
    "    max_lon = float('-inf')\n",
    "    max_lat = float('-inf')\n",
    "\n",
    "    # Iterate through all bounding boxes\n",
    "    for bbox in bboxes.split('|'):\n",
    "        if bbox:\n",
    "            # Parse the bounding box coordinates\n",
    "            coords = list(map(float, bbox.split(',')))\n",
    "            # Update the min and max values\n",
    "            min_lon = min(min_lon, coords[0])  # west - minimum longitude\n",
    "            min_lat = min(min_lat, coords[1])  # south - minimum latitude\n",
    "            max_lon = max(max_lon, coords[2])  # east - maximum longitude\n",
    "            max_lat = max(max_lat, coords[3])  # north - maximum latitude\n",
    "\n",
    "    # Create the combined bounding box\n",
    "    return f\"{min_lon},{min_lat},{max_lon},{max_lat}\"\n",
    "\n",
    "df['Bounding Box'] = df['Bounding Boxes'].apply(combine_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(dataset_date):\n",
    "    # Use regular expression to find all occurrences of four consecutive digits (years)\n",
    "    years = re.findall(r'\\b\\d{4}\\b', dataset_date)\n",
    "\n",
    "    # Check if there are years found\n",
    "    if years:\n",
    "        start_year = years[0]\n",
    "        # If the second date is an asterisk, use '2024' as the end year\n",
    "        end_year = '2024' if dataset_date.endswith('TO *]') else years[-1]\n",
    "\n",
    "        # Prepare Date Range and Temporal Coverage\n",
    "        date_range = f\"{start_year}-{end_year}\" if start_year != end_year else start_year\n",
    "        temporal_coverage = start_year if start_year == end_year else f\"{start_year}-{end_year}\"\n",
    "\n",
    "        return date_range, temporal_coverage\n",
    "    else:\n",
    "        return None, None  # Return None if years are not found\n",
    "\n",
    "# Apply the function to the 'dataset_date' column and split the results\n",
    "df['Date Range'], df['Temporal Coverage'] = zip(*df['dataset_date'].apply(extract_dates))\n",
    "# df.drop(columns=['dataset_date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_range(dataset_date):\n",
    "    # Use regular expression to find all occurrences of four consecutive digits (years)\n",
    "    years = re.findall(r'\\b\\d{4}\\b', dataset_date)\n",
    "\n",
    "    # Check if there are years found\n",
    "    if years:\n",
    "        start_year = years[0]\n",
    "        # If the second date is an asterisk, use '2024' as the end year\n",
    "        end_year = '2024' if dataset_date.endswith('TO *]') else years[-1]\n",
    "\n",
    "        # Always prepare Date Range in the format yyyy-yyyy\n",
    "        date_range = f\"{start_year}-{end_year}\"\n",
    "\n",
    "        return date_range\n",
    "    else:\n",
    "        return None  # Return None if years are not found\n",
    "\n",
    "# Apply the function to the 'dataset_date' column and assign results\n",
    "df['Date Range'] = df['dataset_date'].apply(extract_date_range)\n",
    "# df.drop(columns=['dataset_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(metadata_created):\n",
    "    # Split the string at the first hyphen and take the first part\n",
    "    year = metadata_created.split('-')[0]\n",
    "    return year\n",
    "\n",
    "# Apply the function to the 'metadata_created' column\n",
    "df['Date Issued'] = df['metadata_created'].apply(extract_year)\n",
    "# df.drop(columns=['metadata_created'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tags):\n",
    "    # Extract 'display_name' from each tag in the array, join them with a pipe, and omit 'geodata'\n",
    "    return '|'.join(tag['display_name'] for tag in tags if 'display_name' in tag and tag['display_name'].lower() != 'geodata')\n",
    "\n",
    "# Apply the function to the 'tags' column\n",
    "df['Keyword'] = df['tags'].apply(extract_keywords)\n",
    "df.drop(columns=['tags'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Parse the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_shp_geodatabase_resources(resources):\n",
    "#     # Initialize an empty list to collect the download URLs\n",
    "#     download_urls = []\n",
    "\n",
    "#     # Iterate through each resource in the array\n",
    "#     for resource in resources:\n",
    "#         # Check if the format is either \"SHP\" or \"Geodatabase\"\n",
    "#         if resource.get('format') in ['SHP', 'Geodatabase']:\n",
    "#             # Extract the download URL\n",
    "#             url = resource.get('download_url')\n",
    "#             if url:\n",
    "#                 download_urls.append(url)\n",
    "\n",
    "#     # Join the URLs with a pipe\n",
    "#     return '|'.join(download_urls)\n",
    "\n",
    "# # Apply the function to the 'resources' column\n",
    "# df['SHP_Geodatabase_URLs'] = df['resources'].apply(extract_shp_geodatabase_resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add default and constructed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date Accessioned'] = action_date\n",
    "df['Code'] = \"99-1400\"\n",
    "df['Is Part Of'] = \"99-1400\"\n",
    "df['Member Of'] = \"b0153110-e455-4ced-9114-9b13250a7093\"\n",
    "df['Accrual Method'] = \"CKAN\"\n",
    "df['Access Rights'] = \"Public\"\n",
    "df['Language'] = \"eng\"\n",
    "df['Provider'] = \"Humanitarian Data Exchange\"\n",
    "df['Information'] = \"https://data.humdata.org/dataset/\" + df['ID'].astype(str)\n",
    "df['Format'] = \"Files\"\n",
    "df['Resource Class'] = \"Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Theme',\n",
    "'Temporal Coverage',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'FeatureServer',\n",
    "'HTML',\n",
    "'ImageServer',\n",
    "'Information',\n",
    "'MapServer',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Access Rights',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method',\n",
    "'License',\n",
    "'Bounding Boxes'\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for US state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state bounding box data from the CSV file\n",
    "state_df = pd.read_csv('../../data/states.csv')\n",
    "# Create a dictionary that maps state names to bounding boxes\n",
    "state_bbox_map = dict(zip(state_df['State'], state_df['Bounding Box']))\n",
    "\n",
    "def update_spatial_coverage_and_bounding_box(row):\n",
    "    title = row['Alternative Title']\n",
    "    # Regex to capture state from the title format \"United States (state)\"\n",
    "    match = re.match(r'United States \\(([^)]+)\\)', title)\n",
    "    if match:\n",
    "        state = match.group(1)\n",
    "        # Append the state to the Spatial Coverage field\n",
    "        if 'Spatial Coverage' in row and pd.notna(row['Spatial Coverage']):\n",
    "            row['Spatial Coverage'] += f\"|{state}\"\n",
    "        else:\n",
    "            row['Spatial Coverage'] = state\n",
    "        \n",
    "        # Update the Bounding Box if the state is found in the map\n",
    "        if state in state_bbox_map:\n",
    "            row['Bounding Box'] = state_bbox_map[state]\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "df = df.apply(update_spatial_coverage_and_bounding_box, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the nation names\n",
    "bbox_path = os.path.join('../../', 'data', 'nation-bbox.csv')\n",
    "bbox_df = pd.read_csv(bbox_path)\n",
    "\n",
    "# Extract unique nation names from the 'Label' and 'altLabel' columns and sort by length\n",
    "nations = pd.concat([bbox_df['Label'], bbox_df['altLabel'].dropna()]).unique()\n",
    "nations = sorted(nations, key=len, reverse=True)  # Sort by length, longest first\n",
    "\n",
    "def transform_title(row, nations):\n",
    "    alt_title = row['Alternative Title']\n",
    "    context = \"\"\n",
    "\n",
    "    # First, handle the United States (state) pattern\n",
    "    us_state_match = re.match(r'United States \\(([^)]+)\\)', alt_title)\n",
    "    if us_state_match:\n",
    "        state = us_state_match.group(1)\n",
    "        # Remove the \"United States (state)\" part and clean up the title\n",
    "        alt_title = re.sub(r'United States \\([^)]+\\)\\s*', '', alt_title).strip()\n",
    "        context = f\"[{state}]\"\n",
    "\n",
    "    # Next, handle nation names if no US state context has been set\n",
    "    if not context:\n",
    "        for nation in nations:\n",
    "            if re.search(rf\"\\b{re.escape(nation)}\\b\", alt_title, re.I):\n",
    "                # Place the nation name in brackets and remove it from its original place\n",
    "                alt_title = re.sub(rf\"\\b{re.escape(nation)}\\b\", '', alt_title, flags=re.I).strip()\n",
    "                context = f\"[{nation}]\"\n",
    "                break\n",
    "\n",
    "    # Append the context (nation or state) to the end of the title\n",
    "    if context:\n",
    "        alt_title = f\"{alt_title.strip()} {context}\"\n",
    "\n",
    "    # Capitalize only the first character of the title if it starts with a lowercase letter\n",
    "    if alt_title and alt_title[0].islower():\n",
    "        alt_title = alt_title[0].upper() + alt_title[1:]\n",
    "\n",
    "    # Remove leading non-word characters and multiple spaces\n",
    "    alt_title = re.sub(r\"^[^\\w]+\", \"\", alt_title)  # Removes leading non-word characters\n",
    "    alt_title = re.sub(r'\\s+', ' ', alt_title).strip()\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "# Apply the transformed title directly to the DataFrame\n",
    "df['Title'] = df.apply(lambda x: transform_title(x, nations), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Theme categories based on keywords in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the themes data\n",
    "theme_df = pd.read_csv('../../data/theme.csv')\n",
    "\n",
    "# Prepare a dictionary mapping keywords to themes\n",
    "theme_map = {}\n",
    "for index, row in theme_df.iterrows():\n",
    "    # Include the label itself as a keyword\n",
    "    keywords = row['keywords'].split('|') + [row['label']]\n",
    "    for keyword in keywords:\n",
    "        theme_map[keyword.lower()] = row['label']\n",
    "\n",
    "def assign_theme(title):\n",
    "    matched_themes = []\n",
    "    # Scan each keyword and check for a match in the title\n",
    "    for keyword in theme_map:\n",
    "        if keyword in title.lower():  # Check if keyword is in the title\n",
    "            matched_theme = theme_map[keyword]\n",
    "            if matched_theme not in matched_themes:  # Avoid duplicates\n",
    "                matched_themes.append(matched_theme)\n",
    "    \n",
    "    if matched_themes:\n",
    "        return '|'.join(matched_themes)  # Join all matched themes with a pipe\n",
    "    return None  # Return None if no keywords match\n",
    "\n",
    "# Apply the function to the Title column to update the Theme column\n",
    "df['Theme'] = df['Title'].apply(assign_theme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove HTML tags from a string\n",
    "def remove_html_tags(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "# Function to clean up and standardize text data in DataFrame\n",
    "def clean_text_columns(column):\n",
    "    if column.dtype == \"object\":\n",
    "        # Remove HTML tags\n",
    "        column = column.apply(remove_html_tags)\n",
    "        # Replace newlines, carriage returns, tabs, and reduce multiple spaces to one\n",
    "        column = column.str.replace('\\n', ' ').replace('\\x0D', ' ').str.replace('\\t', ' ').str.replace('<br/>', ' ').str.replace('\\s+', ' ', regex=True).str.strip()\n",
    "    return column\n",
    "\n",
    "# Apply cleaning functions to each column\n",
    "df = df.apply(clean_text_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
