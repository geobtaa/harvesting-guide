{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the CKAN API for the Humanitarian Data Exchange and return the metadata for all items as a CSV file in the GeoBTAA Metadata Application Profile. It will also create a secondary CSV file for the associated multiple downloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# auto-generate the current time in 'YYYYMM' format\n",
    "action_date = time.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare paths and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path to the JSON file\n",
    "output_filename = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://data.humdata.org\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = action_date + \"_99-1400.csv\"  # Update with the desired path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Download the metadata to your desktop\n",
    "\n",
    "This cell will scan the API and create a JSON file on your desktop. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 23126 items saved to ckan_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Specify the CKAN portal URL you want to harvest from\n",
    "packageURL = \"https://data.humdata.org/api/3/action/package_search\"\n",
    "\n",
    "# Specify the number of items per page\n",
    "items_per_page = 10\n",
    "\n",
    "# Initialize variables for pagination\n",
    "start = 0\n",
    "total_results = 0\n",
    "\n",
    "# List to store all metadata\n",
    "all_metadata = []\n",
    "\n",
    "# Request metadata in paginated manner\n",
    "while True:\n",
    "    try:\n",
    "        # Construct the API request URL with pagination parameters\n",
    "        api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "\n",
    "        # Set up a request with a user-agent\n",
    "        request = urllib.request.Request(api_request_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        \n",
    "        # Request metadata\n",
    "        context = ssl._create_unverified_context()\n",
    "        response = urllib.request.urlopen(request, context=context, timeout=30)  # Increased timeout\n",
    "\n",
    "        # Check if the response is valid (status code 200)\n",
    "        if response.status == 200:\n",
    "            response_json = json.loads(response.read().decode('utf-8'))\n",
    "            \n",
    "            # Extract metadata from the response\n",
    "            metadata = response_json['result']['results']\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            # Update pagination variables\n",
    "            start += items_per_page\n",
    "            total_results = response_json['result']['count']\n",
    "            \n",
    "            # Break the loop if we have collected all items\n",
    "            if start >= total_results:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: HTTP status code {response.status}\")\n",
    "            break\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"Failed to fetch data: {e.reason}\")\n",
    "        break\n",
    "\n",
    "# Save the metadata to a local JSON file on your desktop\n",
    "desktop_path = \"\"  # Replace with your desktop path\n",
    "output_filename = \"ckan_metadata.json\"\n",
    "output_path = os.path.join(desktop_path, output_filename)\n",
    "\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Metadata for {total_results} items saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Read the JSON into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file into a DataFrame\n",
    "all_df = pd.read_json(output_filename)\n",
    "filtered_df = all_df[(all_df['archived'] == False) & (all_df['has_geodata'] == True) & (all_df['private'] == False)]\n",
    "\n",
    "\n",
    "# Specify the columns you want to keep and their new names\n",
    "columns_to_keep_and_rename = {\n",
    "    \"id\": \"ID\",\n",
    "    \"title\": \"Alternative Title\",\n",
    "    \"dataset_source\": \"Publisher\",\n",
    "    \"license_url\": \"License\",\n",
    "    \"notes\": \"Description\",\n",
    "    \"methodology_other\":\"Methodology\",\n",
    "    \"dataset_date\":\"dataset_date\",\n",
    "    \"metadata_created\":\"metadata_created\",\n",
    "    \"name\": \"Identifier\",\n",
    "    \"solr_additions\": \"Places\",\n",
    "    \"resources\": \"resources\",\n",
    "    \"tags\": \"tags\"\n",
    "}\n",
    "\n",
    "# Select and rename the specified columns\n",
    "df = filtered_df[list(columns_to_keep_and_rename.keys())].rename(columns=columns_to_keep_and_rename)\n",
    "\n",
    "# Remove duplicate items\n",
    "df = df.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_countries(places):\n",
    "    try:\n",
    "        # Convert the string to a JSON object\n",
    "        places_json = json.loads(places)\n",
    "        # Extract countries and join with a pipe\n",
    "        countries = '|'.join(places_json.get('countries', []))\n",
    "        return countries\n",
    "    except json.JSONDecodeError:\n",
    "        return ''  # Return an empty string in case of parsing error\n",
    "\n",
    "# Apply the function to the 'solr_additions' column\n",
    "df['Spatial Coverage'] = df['Places'].apply(parse_countries)\n",
    "# Drop the \"Places\" column\n",
    "df.drop(columns=['Places'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(dataset_date):\n",
    "    # Use regular expression to find all occurrences of four consecutive digits (years)\n",
    "    years = re.findall(r'\\b\\d{4}\\b', dataset_date)\n",
    "\n",
    "    # Check if there are years found\n",
    "    if years:\n",
    "        start_year = years[0]\n",
    "        # If the second date is an asterisk, use '2024' as the end year\n",
    "        end_year = '2024' if dataset_date.endswith('TO *]') else years[-1]\n",
    "\n",
    "        # Prepare Date Range and Temporal Coverage\n",
    "        date_range = f\"{start_year}-{end_year}\" if start_year != end_year else start_year\n",
    "        temporal_coverage = start_year if start_year == end_year else f\"{start_year}-{end_year}\"\n",
    "\n",
    "        return date_range, temporal_coverage\n",
    "    else:\n",
    "        return None, None  # Return None if years are not found\n",
    "\n",
    "# Apply the function to the 'dataset_date' column and split the results\n",
    "df['Date Range'], df['Temporal Coverage'] = zip(*df['dataset_date'].apply(extract_dates))\n",
    "df.drop(columns=['dataset_date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(metadata_created):\n",
    "    # Split the string at the first hyphen and take the first part\n",
    "    year = metadata_created.split('-')[0]\n",
    "    return year\n",
    "\n",
    "# Apply the function to the 'metadata_created' column\n",
    "df['Date Issued'] = df['metadata_created'].apply(extract_year)\n",
    "df.drop(columns=['metadata_created'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tags):\n",
    "    # Extract 'display_name' from each tag in the array, join them with a pipe, and omit 'geodata'\n",
    "    return '|'.join(tag['display_name'] for tag in tags if 'display_name' in tag and tag['display_name'].lower() != 'geodata')\n",
    "\n",
    "# Apply the function to the 'tags' column\n",
    "df['Keyword'] = df['tags'].apply(extract_keywords)\n",
    "df.drop(columns=['tags'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shp_geodatabase_resources(resources):\n",
    "    # Initialize an empty list to collect the download URLs\n",
    "    download_urls = []\n",
    "\n",
    "    # Iterate through each resource in the array\n",
    "    for resource in resources:\n",
    "        # Check if the format is either \"SHP\" or \"Geodatabase\"\n",
    "        if resource.get('format') in ['SHP', 'Geodatabase']:\n",
    "            # Extract the download URL\n",
    "            url = resource.get('download_url')\n",
    "            if url:\n",
    "                download_urls.append(url)\n",
    "\n",
    "    # Join the URLs with a pipe\n",
    "    return '|'.join(download_urls)\n",
    "\n",
    "# Apply the function to the 'resources' column\n",
    "df['SHP_Geodatabase_URLs'] = df['resources'].apply(extract_shp_geodatabase_resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the resources, assign web service types, and determine the Resource Class\n",
    "\n",
    "def classify_urls(row):\n",
    "    for resource in row['resources']:\n",
    "        resource_type = resource['resource_type']\n",
    "        url = resource['url']                  \n",
    "         # Handle the other resource types\n",
    "        dataset_types = ['fgdb', 'gpkg', 'shp', 'csv', 'xlsx', 'kmz', 'cad', 'aaigrid', 'geojson', 'json']\n",
    "        imagery_type = 'tif'\n",
    "\n",
    "        # Check if the \"Resource Class\" has not been set or it's set to 'Other'\n",
    "        if pd.isna(row.get('Resource Class')):\n",
    "            if resource_type in dataset_types:\n",
    "                row['Resource Class'] = 'Datasets'\n",
    "        elif resource_type == imagery_type:\n",
    "            row['Resource Class'] = 'Imagery'\n",
    "                \n",
    "    return row\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = df.apply(classify_urls, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the multiple downloads file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m multiple_downloads_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Replace the values in the 'label' column according to the mapping provided in download_formats\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m multiple_downloads_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmultiple_downloads_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(download_formats)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Write the DataFrame to a CSV\u001b[39;00m\n\u001b[1;32m     36\u001b[0m multiple_downloads_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m99-1400_multiple-downloads.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/notebooks/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/notebooks/lib/python3.10/site-packages/pandas/core/indexes/range.py:349\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# List of download formats\n",
    "download_formats = {\n",
    "    \"fgdb\": \"Geodatabase\",\n",
    "    \"gpkg\": \"GeoPackage\",\n",
    "    \"shp\": \"Shapefile\",\n",
    "    \"csv\": \"CSV\",\n",
    "    \"xlsx\": \"XLSX\",\n",
    "    \"tif\": \"TIF\",\n",
    "    \"cad\": \"CAD\",\n",
    "    \"kmz\": \"KMZ\",\n",
    "    \"geojson\": \"GeoJSON\",\n",
    "    \"json\": \"JSON\",\n",
    "    'aaigrid': \"ArcGRID\"\n",
    "}\n",
    "# List to collect the rows for the new DataFrame\n",
    "rows = []\n",
    "\n",
    "# Iterate through the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    friendlier_id = row['ID']\n",
    "    resources = row['resources']\n",
    "    for resource in resources:\n",
    "        resource_type = resource['format']\n",
    "        url = resource['url']\n",
    "        if resource_type in download_formats and pd.notna(url):\n",
    "            # Append the row if the resource_type matches and url is not NaN\n",
    "            rows.append({\"friendlier_id\": friendlier_id, 'label': format, \"value\": url})\n",
    "\n",
    "# Create a new DataFrame using the collected rows\n",
    "multiple_downloads_df = pd.DataFrame(rows)\n",
    "\n",
    "# Replace the values in the 'label' column according to the mapping provided in download_formats\n",
    "multiple_downloads_df['label'] = multiple_downloads_df['label'].replace(download_formats)\n",
    "\n",
    "# Write the DataFrame to a CSV\n",
    "multiple_downloads_df.to_csv(\"99-1400_multiple-downloads.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add default and constructed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date Accessioned'] = action_date\n",
    "df['Code'] = \"99-1400\"\n",
    "df['Is Part Of'] = \"99-1400\"\n",
    "df['Member Of'] = \"b0153110-e455-4ced-9114-9b13250a7093\"\n",
    "df['Accrual Method'] = \"CKAN\"\n",
    "df['Access Rights'] = \"Public\"\n",
    "# df['Language'] = \"eng\"\n",
    "df['Provider'] = \"Humanitarian Data Exchange\"\n",
    "df['Information'] = \"https://data.humdata.org/dataset/\" + df['ID'].astype(str)\n",
    "df['Format'] = \"Files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Theme',\n",
    "'Temporal Coverage',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'FeatureServer',\n",
    "'HTML',\n",
    "'ImageServer',\n",
    "'Information',\n",
    "'MapServer',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Access Rights',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method'\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply .str.strip() method to all string columns in the DataFrame and replace newline and tab characters\n",
    "df = df.apply(lambda x: x.str.replace('\\n', ' ').str.replace('\\t', ' ').str.replace('<br/>', ' ').str.replace('<br/><br/>', '|').str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
