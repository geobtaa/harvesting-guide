{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the CKAN API for the Humanitarian Data Exchange and return the metadata for all items as a CSV file in the GeoBTAA Metadata Application Profile. It will also create a secondary CSV file for the associated multiple downloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# auto-generate the current time in 'YYYYMM' format\n",
    "action_date = time.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare paths and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the path to the JSON file\n",
    "output_filename = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://data.humdata.org\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = action_date + \"_99-1400.csv\"  # Update with the desired path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Download the metadata to your desktop\n",
    "\n",
    "This cell will scan the API and create a JSON file on your desktop. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the CKAN portal URL you want to harvest from\n",
    "packageURL = \"https://data.humdata.org/api/3/action/package_search\"\n",
    "\n",
    "# Specify the number of items per page\n",
    "items_per_page = 10\n",
    "\n",
    "# Initialize variables for pagination\n",
    "start = 0\n",
    "total_results = 0\n",
    "\n",
    "# List to store all metadata\n",
    "all_metadata = []\n",
    "\n",
    "# Request metadata in paginated manner\n",
    "while True:\n",
    "    try:\n",
    "        # Construct the API request URL with pagination parameters\n",
    "        api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "\n",
    "        # Set up a request with a user-agent\n",
    "        request = urllib.request.Request(api_request_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        \n",
    "        # Request metadata\n",
    "        context = ssl._create_unverified_context()\n",
    "        response = urllib.request.urlopen(request, context=context, timeout=30)  # Increased timeout\n",
    "\n",
    "        # Check if the response is valid (status code 200)\n",
    "        if response.status == 200:\n",
    "            response_json = json.loads(response.read().decode('utf-8'))\n",
    "            \n",
    "            # Extract metadata from the response\n",
    "            metadata = response_json['result']['results']\n",
    "            all_metadata.extend(metadata)\n",
    "            \n",
    "            # Update pagination variables\n",
    "            start += items_per_page\n",
    "            total_results = response_json['result']['count']\n",
    "            \n",
    "            # Break the loop if we have collected all items\n",
    "            if start >= total_results:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Failed to fetch data: HTTP status code {response.status}\")\n",
    "            break\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"Failed to fetch data: {e.reason}\")\n",
    "        break\n",
    "\n",
    "# Save the metadata to a local JSON file on your desktop\n",
    "desktop_path = \"\"  # Replace with your desktop path\n",
    "output_filename = \"ckan_metadata.json\"\n",
    "output_path = os.path.join(desktop_path, output_filename)\n",
    "\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Metadata for {total_results} items saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Read the JSON into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file into a DataFrame\n",
    "all_df = pd.read_json(output_filename)\n",
    "filtered_df = all_df[(all_df['archived'] == False) & (all_df['has_geodata'] == True) & (all_df['private'] == False)]\n",
    "\n",
    "\n",
    "# Specify the columns you want to keep and their new names\n",
    "columns_to_keep_and_rename = {\n",
    "    \"id\": \"ID\",\n",
    "    \"title\": \"Alternative Title\",\n",
    "    \"dataset_source\": \"Publisher\",\n",
    "    \"license_url\": \"License\",\n",
    "    \"notes\": \"Description\",\n",
    "    \"methodology_other\":\"Methodology\",\n",
    "    \"dataset_date\":\"dataset_date\",\n",
    "    \"metadata_created\":\"metadata_created\",\n",
    "    \"name\": \"Identifier\",\n",
    "    \"solr_additions\": \"Places\",\n",
    "    \"resources\": \"resources\",\n",
    "    \"tags\": \"tags\"\n",
    "}\n",
    "\n",
    "# Select and rename the specified columns\n",
    "df = filtered_df[list(columns_to_keep_and_rename.keys())].rename(columns=columns_to_keep_and_rename)\n",
    "\n",
    "# Remove duplicate items\n",
    "df = df.drop_duplicates(subset=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_countries(places):\n",
    "    try:\n",
    "        # Convert the string to a JSON object\n",
    "        places_json = json.loads(places)\n",
    "        # Extract countries and join with a pipe\n",
    "        countries = '|'.join(places_json.get('countries', []))\n",
    "        return countries\n",
    "    except json.JSONDecodeError:\n",
    "        return ''  # Return an empty string in case of parsing error\n",
    "\n",
    "# Apply the function to the 'solr_additions' column\n",
    "df['Spatial Coverage'] = df['Places'].apply(parse_countries)\n",
    "# Drop the \"Places\" column\n",
    "df.drop(columns=['Places'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(dataset_date):\n",
    "    # Use regular expression to find all occurrences of four consecutive digits (years)\n",
    "    years = re.findall(r'\\b\\d{4}\\b', dataset_date)\n",
    "\n",
    "    # Check if there are years found\n",
    "    if years:\n",
    "        start_year = years[0]\n",
    "        # If the second date is an asterisk, use '2024' as the end year\n",
    "        end_year = '2024' if dataset_date.endswith('TO *]') else years[-1]\n",
    "\n",
    "        # Prepare Date Range and Temporal Coverage\n",
    "        date_range = f\"{start_year}-{end_year}\" if start_year != end_year else start_year\n",
    "        temporal_coverage = start_year if start_year == end_year else f\"{start_year}-{end_year}\"\n",
    "\n",
    "        return date_range, temporal_coverage\n",
    "    else:\n",
    "        return None, None  # Return None if years are not found\n",
    "\n",
    "# Apply the function to the 'dataset_date' column and split the results\n",
    "df['Date Range'], df['Temporal Coverage'] = zip(*df['dataset_date'].apply(extract_dates))\n",
    "df.drop(columns=['dataset_date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(metadata_created):\n",
    "    # Split the string at the first hyphen and take the first part\n",
    "    year = metadata_created.split('-')[0]\n",
    "    return year\n",
    "\n",
    "# Apply the function to the 'metadata_created' column\n",
    "df['Date Issued'] = df['metadata_created'].apply(extract_year)\n",
    "df.drop(columns=['metadata_created'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(tags):\n",
    "    # Extract 'display_name' from each tag in the array, join them with a pipe, and omit 'geodata'\n",
    "    return '|'.join(tag['display_name'] for tag in tags if 'display_name' in tag and tag['display_name'].lower() != 'geodata')\n",
    "\n",
    "# Apply the function to the 'tags' column\n",
    "df['Keyword'] = df['tags'].apply(extract_keywords)\n",
    "df.drop(columns=['tags'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Parse the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_shp_geodatabase_resources(resources):\n",
    "#     # Initialize an empty list to collect the download URLs\n",
    "#     download_urls = []\n",
    "\n",
    "#     # Iterate through each resource in the array\n",
    "#     for resource in resources:\n",
    "#         # Check if the format is either \"SHP\" or \"Geodatabase\"\n",
    "#         if resource.get('format') in ['SHP', 'Geodatabase']:\n",
    "#             # Extract the download URL\n",
    "#             url = resource.get('download_url')\n",
    "#             if url:\n",
    "#                 download_urls.append(url)\n",
    "\n",
    "#     # Join the URLs with a pipe\n",
    "#     return '|'.join(download_urls)\n",
    "\n",
    "# # Apply the function to the 'resources' column\n",
    "# df['SHP_Geodatabase_URLs'] = df['resources'].apply(extract_shp_geodatabase_resources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add default and constructed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date Accessioned'] = action_date\n",
    "df['Code'] = \"99-1400\"\n",
    "df['Is Part Of'] = \"99-1400\"\n",
    "df['Member Of'] = \"b0153110-e455-4ced-9114-9b13250a7093\"\n",
    "df['Accrual Method'] = \"CKAN\"\n",
    "df['Access Rights'] = \"Public\"\n",
    "# df['Language'] = \"eng\"\n",
    "df['Provider'] = \"Humanitarian Data Exchange\"\n",
    "df['Information'] = \"https://data.humdata.org/dataset/\" + df['ID'].astype(str)\n",
    "df['Format'] = \"Files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Theme',\n",
    "'Temporal Coverage',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'FeatureServer',\n",
    "'HTML',\n",
    "'ImageServer',\n",
    "'Information',\n",
    "'MapServer',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Access Rights',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method'\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply .str.strip() method to all string columns in the DataFrame and replace newline and tab characters\n",
    "df = df.apply(lambda x: x.str.replace('\\n', ' ').str.replace('\\t', ' ').str.replace('<br/>', ' ').str.replace('<br/><br/>', '|').str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
