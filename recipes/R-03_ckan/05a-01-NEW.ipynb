{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the CKAN API for the Minnesota Geospatial Commons and return the metadata for all items as two CSV files:\n",
    "\n",
    "1. the GeoBTAA Metadata Application Profile.\n",
    "2. secondary CSV file for all links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare paths and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-generate the current time in 'YYYYMM' format\n",
    "action_date = time.strftime('%Y%m%d')\n",
    "\n",
    "# # Specify the path to the JSON file\n",
    "output_filename = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://gisdata.mn.gov/\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = action_date + \"_05a-01.csv\"  # Update with the desired path\n",
    "\n",
    "DEFAULT_STATE = \"Minnesota\"\n",
    "\n",
    "counties_in_minnesota = [\n",
    "    'Aitkin', 'Anoka', 'Becker', 'Beltrami', 'Benton', 'Big Stone',\n",
    "    'Blue Earth', 'Brown', 'Carlton', 'Carver', 'Cass', 'Chippewa',\n",
    "    'Chisago', 'Clay', 'Clearwater', 'Cook', 'Cottonwood', 'Crow Wing',\n",
    "    'Dakota', 'Dodge', 'Douglas', 'Faribault', 'Fillmore', 'Freeborn',\n",
    "    'Goodhue', 'Grant', 'Hennepin', 'Houston', 'Hubbard', 'Isanti',\n",
    "    'Itasca', 'Jackson', 'Kanabec', 'Kandiyohi', 'Kittson', 'Koochiching',\n",
    "    'Lac qui Parle', 'Lake', 'Lake of the Woods', 'Le Sueur', 'Lincoln',\n",
    "    'Lyon', 'Mahnomen', 'Marshall', 'Martin', 'McLeod', 'Meeker',\n",
    "    'Mille Lacs', 'Morrison', 'Mower', 'Murray', 'Nicollet', 'Nobles',\n",
    "    'Norman', 'Olmsted', 'Otter Tail', 'Pennington', 'Pine', 'Pipestone',\n",
    "    'Polk', 'Pope', 'Ramsey', 'Red Lake', 'Redwood', 'Renville',\n",
    "    'Rice', 'Rock', 'Roseau', 'St. Louis', 'Scott', 'Sherburne',\n",
    "    'Sibley', 'Stearns', 'Steele', 'Stevens', 'Swift', 'Todd',\n",
    "    'Traverse', 'Wabasha', 'Wadena', 'Waseca', 'Washington', 'Watonwan',\n",
    "    'Wilkin', 'Winona', 'Wright', 'Yellow Medicine'\n",
    "]\n",
    "\n",
    "cities_in_minnesota = [\n",
    "    'Minneapolis', 'St. Paul', 'Rochester', 'Duluth', 'Bloomington',\n",
    "    'Brooklyn Park', 'Plymouth', 'Woodbury', 'St. Cloud', 'Eagan',\n",
    "    'Maple Grove', 'Eden Prairie', 'Coon Rapids', 'Burnsville', 'Blaine',\n",
    "    'Lakeville', 'Minnetonka', 'Apple Valley', 'Edina', 'St. Louis Park','Twin Cities'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the metadata to your desktop\n",
    "\n",
    "This cell will scan the API and create a JSON file on your desktop. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of items per page\n",
    "items_per_page = 10\n",
    "\n",
    "# Initialize variables for pagination\n",
    "start = 0\n",
    "total_results = 0\n",
    "\n",
    "# List to store all metadata\n",
    "all_metadata = []\n",
    "\n",
    "# Request metadata in paginated manner\n",
    "while True:\n",
    "    # Construct the API request URL with pagination parameters\n",
    "    api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "    \n",
    "    # Request metadata\n",
    "    context = ssl._create_unverified_context()\n",
    "    response = urllib.request.urlopen(api_request_url, context=context).read()\n",
    "    response_json = json.loads(response.decode('utf-8'))\n",
    "    \n",
    "    # Extract metadata from the response\n",
    "    metadata = response_json['result']['results']\n",
    "    all_metadata.extend(metadata)\n",
    "    \n",
    "    # Update pagination variables\n",
    "    start += items_per_page\n",
    "    total_results = response_json['result']['count']\n",
    "    \n",
    "    # Break the loop if we have collected all items\n",
    "    if start >= total_results:\n",
    "        break\n",
    "\n",
    "\n",
    "# Save the metadata to a local JSON file on your desktop\n",
    "desktop_path = \"\"  # Replace with your desktop path\n",
    "output_filename = \"ckan_metadata.json\"\n",
    "output_path = os.path.join(desktop_path, output_filename) # More portable way to join paths\n",
    "\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Metadata for {total_results} items saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the JSON into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "You now have the metadata on your desktop inside the same directory as this Notebook.  If any of the following steps fail, you can come back to reading the JSON into a dataframe. You do not have to download the metadata again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file into a DataFrame\n",
    "all_df = pd.read_json(output_filename)\n",
    "\n",
    "# Specify the columns you want to keep and their new names\n",
    "columns_to_keep_and_rename = {\n",
    "    \"id\": \"ID\",\n",
    "    \"title\": \"Alternative Title\",\n",
    "    \"notes\": \"Description\",\n",
    "    \"name\": \"Identifier\",\n",
    "    \"extras\": \"extras\",\n",
    "    \"resources\": \"resources\",\n",
    "    \"groups\": \"groups\",\n",
    "    \"tags\": \"tags\"\n",
    "}\n",
    "\n",
    "# Select and rename the specified columns\n",
    "df = all_df[list(columns_to_keep_and_rename.keys())].rename(columns=columns_to_keep_and_rename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the tags and groups arrays to create Keyword and Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the values for tags into Keyword\n",
    "\n",
    "df[\"Keyword\"] = df[\"tags\"].apply(lambda x: \"|\".join([tag[\"display_name\"] for tag in x]) if isinstance(x, list) else \"\")\n",
    "\n",
    "# Some of the Theme values need to be renamed\n",
    "theme_mapping = {\n",
    "    \"Biota\": \"Biology\",\n",
    "    \"Geoscientific\": \"Geology\",\n",
    "    \"Imagery + Basemaps\": \"Imagery|Land Cover\",\n",
    "    \"Planning + Cadastre\": \"Property\",\n",
    "    \"Utilities + Communication\": \"Utilities\"\n",
    "}\n",
    "\n",
    "# Function to flatten the \"groups\" and apply the theme mapping\n",
    "def flatten_groups(groups):\n",
    "    if isinstance(groups, list):\n",
    "        group_values = [group[\"display_name\"] for group in groups]\n",
    "        return \"|\".join([theme_mapping.get(value, value) for value in group_values])\n",
    "    return \"\"\n",
    "\n",
    "# Use the flatten_groups function to create the \"Theme\" column\n",
    "df[\"Theme\"] = df[\"groups\"].apply(flatten_groups)\n",
    "\n",
    "# Drop the \"groups\" and \"tags\" columns\n",
    "df.drop(columns=['groups', 'tags'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional check: Write the selected DataFrame to a CSV file\n",
    "# At this point there will be 9 columns \n",
    "# ID\n",
    "# Alternative \n",
    "# Title\n",
    "# Description\n",
    "# Identifier\n",
    "# extras - arrays of various fields\n",
    "# resources - arrays of links\n",
    "# Keyword\n",
    "# Theme\n",
    "\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten extras into a dictionary and expand into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_extras_df = df[\"extras\"].apply(lambda x: {item[\"key\"]: item[\"value\"] for item in x}).apply(pd.Series)\n",
    "\n",
    "# Specify the columns you want to keep from flattened extras and their new names\n",
    "extras_to_keep_and_rename = {\n",
    "    \"dsAccessConst\": \"Rights\",\n",
    "    \"dsCurrentRef\": \"Currentness\",\n",
    "    \"dsMetadataUrl\": \"metadata_html\",\n",
    "    \"dsOriginator\": \"Creator\",\n",
    "    \"dsPeriodOfContent\": \"Temporal Coverage\",\n",
    "    \"dsPurpose\": \"Purpose\",\n",
    "    \"spatial\": \"spatial\"\n",
    "}\n",
    "\n",
    "# Select, rename, and concatenate the required extras columns\n",
    "df = pd.concat([df, flattened_extras_df[list(extras_to_keep_and_rename.keys())].rename(columns=extras_to_keep_and_rename)], axis=1)\n",
    "\n",
    "# Combine the \"Purpose\", \"Currentness\", and \"Description\" columns, using '|' as a delimiter\n",
    "df['Description'] = df['Description'].fillna('') + '|' + df['Purpose'].fillna('') + '|' + df['Currentness'].fillna('')\n",
    "\n",
    "# Drop the columns that were moved\n",
    "df.drop(columns=['extras', 'Purpose', 'Currentness'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the WKT coordinates into W,S,E,N for Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(wkt_str):\n",
    "    # Default bounding box for Minnesota\n",
    "    default_bounding_box = \"-97.2392,43.4994,-89.4919,49.3845\"\n",
    "\n",
    "    # Check if the value is missing or not a string\n",
    "    if wkt_str is None or not isinstance(wkt_str, str):\n",
    "        return default_bounding_box\n",
    "\n",
    "    try:\n",
    "        # Parse the string as JSON\n",
    "        wkt_json = json.loads(wkt_str)\n",
    "\n",
    "        # Extract the coordinates\n",
    "        coordinates = wkt_json[\"coordinates\"][0]\n",
    "\n",
    "        # Calculate the bounding box\n",
    "        west = min(coord[0] for coord in coordinates)\n",
    "        south = min(coord[1] for coord in coordinates)\n",
    "        east = max(coord[0] for coord in coordinates)\n",
    "        north = max(coord[1] for coord in coordinates)\n",
    "\n",
    "        # Return the bounding box in the required format\n",
    "        return f\"{west},{south},{east},{north}\"\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError):\n",
    "        return default_bounding_box\n",
    "\n",
    "# Apply the function to the 'spatial' column and create a new 'Bounding Box' column\n",
    "df['Bounding Box'] = df['spatial'].apply(get_bounding_box)\n",
    "\n",
    "# Drop the spatial column\n",
    "df.drop(columns=['spatial'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get place names based on Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_coverage(originator):\n",
    "    if pd.isnull(originator): # Check for NaN\n",
    "        return DEFAULT_STATE\n",
    "\n",
    "    coverage_strings = []  # List to store coverage strings\n",
    "\n",
    "    # Check for counties\n",
    "    for county in counties_in_minnesota:\n",
    "        if county in originator:\n",
    "            coverage_strings.append(f\"{DEFAULT_STATE}--{county} County\")\n",
    "\n",
    "    # Check for specific organizations\n",
    "    if \"Metropolitan Council\" in originator or \"MetroGIS\" in originator:\n",
    "        coverage_strings += [f\"{DEFAULT_STATE}--{county} County\" for county in [\"Anoka\", \"Carver\", \"Dakota\", \"Hennepin\", \"Ramsey\", \"Scott\", \"Washington\"]]\n",
    "        coverage_strings.append(f\"{DEFAULT_STATE}--Twin Cities Metropolitan Area\")\n",
    "    elif \"Metropolitan Emergency Services Board\" in originator:\n",
    "        coverage_strings += [f\"{DEFAULT_STATE}--{county} County\" for county in [\"Anoka\", \"Carver\", \"Chisago\", \"Dakota\", \"Hennepin\", \"Isanti\", \"Ramsey\", \"Scott\", \"Sherburne\", \"Washington\"]]\n",
    "        coverage_strings.append(f\"{DEFAULT_STATE}--Twin Cities Metropolitan Area\")\n",
    "    \n",
    "    # If no specific coverage strings were found, return the default state\n",
    "    if not coverage_strings:\n",
    "        return DEFAULT_STATE\n",
    "    \n",
    "    # Join all the coverage strings with the default state at the end\n",
    "    return \"|\".join(coverage_strings) + f\"|{DEFAULT_STATE}\"\n",
    "\n",
    "# Apply the function to the 'dsOriginator' column and create a new 'Spatial Coverage' column\n",
    "df['Spatial Coverage'] = df['Creator'].apply(get_spatial_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat Temporal Coverage and get the Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Temporal Coverage\" column to a datetime object with the correct format\n",
    "# Using errors='coerce' to handle incorrect date formats\n",
    "df['Temporal Coverage'] = pd.to_datetime(df['Temporal Coverage'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "# Convert the datetime object to the desired string format\n",
    "df['Temporal Coverage'] = df['Temporal Coverage'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Handle NaT values by replacing them with a default value or NaN\n",
    "df['Temporal Coverage'].replace('NaT', pd.NA, inplace=True)\n",
    "\n",
    "def get_date_range(date):\n",
    "    if pd.isnull(date):  # Check for NaN\n",
    "        return None\n",
    "    year = str(date).split('-')[0]  # Extract the year from the date string\n",
    "    return f\"{year}-{year}\"  # Format the date range\n",
    "\n",
    "# Apply the function to the 'Temporal Coverage' column and create a new 'Date Range' column\n",
    "df['Date Range'] = df['Temporal Coverage'].apply(get_date_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_date(alt_title, date):\n",
    "    # Transform the title with the existing logic\n",
    "    transformed_title = transform_title(alt_title)\n",
    "\n",
    "    # Check if date is not missing and is a string\n",
    "    if isinstance(date, str) and '/' in date:\n",
    "        # Extract the year from the date string\n",
    "        year = date.split('/')[-1]\n",
    "\n",
    "        # Append the year in curly brackets\n",
    "        transformed_title += f\" {{{year}}}\"\n",
    "\n",
    "    return transformed_title\n",
    "\n",
    "\n",
    "def transform_title(alt_title):\n",
    "    bracket_content = None\n",
    "    \n",
    "    # 1. Look for \", {county} County, Minnesota\" and replace with \"[Minnesota--{county} County]\"\n",
    "    for county in counties_in_minnesota:\n",
    "        if f\", {county} County, Minnesota\" in alt_title:\n",
    "            bracket_content = f\"[Minnesota--{county} County]\"\n",
    "            alt_title = re.sub(f\", {county} County, Minnesota\", \"\", alt_title, 1)\n",
    "\n",
    "    # 2. Look for \"{city}, Minnesota\" and replace with \"[Minnesota--{city}]\"\n",
    "    if not bracket_content:\n",
    "        for city in cities_in_minnesota:\n",
    "            if f\"{city}, Minnesota\" in alt_title:\n",
    "                bracket_content = f\"[Minnesota--{city}]\"\n",
    "                alt_title = re.sub(f\"{city}, Minnesota\", \"\", alt_title, 1)\n",
    "\n",
    "    # 3. Look for variations of just the state of Minnesota\n",
    "    if not bracket_content:\n",
    "        variations = [r\", Minnesota\", r\"For Minnesota\", r\"Of Minnesota\", r\"In Minnesota\"]\n",
    "        for variation in variations:\n",
    "            if variation.lower() in alt_title.lower():\n",
    "                bracket_content = \"[Minnesota]\"\n",
    "                alt_title = re.sub(variation, \"\", alt_title, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. For everything else, add [\"Minnesota\"] to the end\n",
    "    if not bracket_content:\n",
    "        bracket_content = \"[Minnesota]\"\n",
    "\n",
    "    # Append bracketed content to the end of the title\n",
    "    alt_title = f\"{alt_title.strip()} {bracket_content}\"\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "df['Title'] = df.apply(lambda row: transform_with_date(row['Alternative Title'], row['Temporal Coverage']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_publisher(publisher):\n",
    "    # Dictionary mapping of publishers for direct transformation\n",
    "    publisher_mappings = {\n",
    "        \"Metropolitan Council\" : \"Metropolitan Council of the Twin Cities Area\",\n",
    "        \"U.S. Geological Survey\" : \"Geological Survey (U.S.)\"\n",
    "\n",
    "    }\n",
    "    \n",
    "    # If a direct mapping is found, return the transformed value\n",
    "    if publisher in publisher_mappings:\n",
    "        return publisher_mappings[publisher]\n",
    "    \n",
    "    # Check for values that start with \"Minnesota Department of Natural Resources\"\n",
    "    if publisher.startswith(\"Minnesota Department of Natural Resources\"):\n",
    "        return \"Minnesota. Department of Natural Resources\"\n",
    "    \n",
    "    # Search for a county name in the publisher string.\n",
    "    for county in counties_in_minnesota:\n",
    "        if county + \" County\" in publisher:\n",
    "            return f\"Minnesota--{county} County\"\n",
    "    else:\n",
    "        for city in cities_in_minnesota:\n",
    "            if f\"City of {city}\" in publisher or city == publisher:\n",
    "                return f\"Minnesota--{city}\"\n",
    "    \n",
    "    # If no match found, return the original publisher string.\n",
    "    return publisher\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(transform_publisher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below code removes records that are already covered by other harvests\n",
    "values_to_remove = [\n",
    "    \"b903cc92-6740-475b-8a27-4f69bb2436ff\",\n",
    "    \"912ae6b8-cf8b-4336-b392-3226e6e9bc86\",\n",
    "    \"d58e62bb-cb41-474e-b200-1220759f3f7e\",\n",
    "    \"16b4ade6-398a-4aaa-8290-9fabe3a70c63\",\n",
    "    \"a11d5da8-a932-4c54-a5d5-9946d0dbfe28\",\n",
    "    \"09d1f9dc-0a26-4a2c-bb28-f866e10af95e\",\n",
    "    \"6b6a200c-7791-40b2-8063-e348691b6ff8\",\n",
    "    \"6073b93f-68c0-4316-8346-59f63beb62e6\",\n",
    "    \"bdfa0cd7-84bc-4017-8124-98803f594e4f\",\n",
    "    \"e87315d6-a614-45ad-878b-66e8337b21cb\",\n",
    "    \"906ae99c-cc6c-486b-bf7c-72c1ef9c046c\",\n",
    "    \"573c2362-bc9a-4031-aa60-3b0fee613471\"\n",
    "]\n",
    "\n",
    "df = df[~df['ID'].isin(values_to_remove)]\n",
    "\n",
    "creators_to_remove = [\n",
    "    \"Minnesota Geological Survey\",\n",
    "    \"Minnesota Geological Survey (MGS)\"\n",
    "]\n",
    "\n",
    "df = df[~df['Creator'].isin(creators_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional check: Write the selected DataFrame to a CSV file\n",
    "# at this point there will be 15 columns \n",
    "# ID\n",
    "# Alternative Title\n",
    "# Description\n",
    "# Identifier\n",
    "# resources - arrays of links\n",
    "# Keyword\n",
    "# Theme\n",
    "# Rights\n",
    "# HTML\n",
    "# Creator\n",
    "# Temporal Coverage\n",
    "# Bounding Box\n",
    "# Spatial Coverage\n",
    "# Date Range\n",
    "# Title\n",
    "\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the URL types to determine Resource Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the resources, assign web service types, and determine the Resource Class\n",
    "# Add new columns and initialize with None\n",
    "\n",
    "\n",
    "df['Display Note'] = None\n",
    "df['Resource Class'] = None\n",
    "\n",
    "def classify_urls(row):\n",
    "    for resource in row['resources']:\n",
    "        resource_type = resource.get('resource_type', '')\n",
    "        url = resource.get('url', '')\n",
    "        \n",
    "        # Classify based on resource_type\n",
    "        if resource_type == 'ags_mapserver':\n",
    "            row['arcgis_dynamic_map_layer'] = url\n",
    "        elif resource_type == 'ags_featureserver':\n",
    "            row['arcgis_feature_layer'] = url\n",
    "        elif resource_type == 'ags_imageserver':\n",
    "            row['arcgis_image_layer'] = url\n",
    "\n",
    "        # Add a display note for recognized ArcGIS services\n",
    "        if resource_type in ['ags_mapserver', 'ags_featureserver', 'ags_imageserver']:\n",
    "            creator = row.get('Creator', 'Unknown Creator')\n",
    "            row['Display Note'] = (\n",
    "                f\"Tip: This page displays a preview of one layer from the {creator} ArcGIS Rest Service. \"\n",
    "                f\"However, if the web service is in a different project or contains multiple layers, it may not display. \"\n",
    "                f\"Try opening the layer in ArcGIS Online or click the 'Visit Source' to browse for other layers in the service.\"\n",
    "            )\n",
    "                       \n",
    "         # Handle the other resource types\n",
    "        dataset_types = ['fgdb', 'gpkg', 'shp', 'csv', 'xlsx', 'kmz', 'cad', 'aaigrid', 'geojson', 'json']\n",
    "        imagery_type = 'tif'\n",
    "\n",
    "        # Check if the \"Resource Class\" has not been set or it's set to 'Other'\n",
    "        if pd.isna(row.get('Resource Class')):\n",
    "            if resource_type in dataset_types:\n",
    "                row['Resource Class'] = 'Datasets'\n",
    "        elif resource_type == imagery_type:\n",
    "            row['Resource Class'] = 'Imagery'\n",
    "                \n",
    "    return row\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = df.apply(classify_urls, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a Resource Class to the remaining items\n",
    "\n",
    "# Define a pattern that matches 'image' or 'photo' anywhere in the string, without capturing groups\n",
    "pattern = r'(?:image|photo)'\n",
    "\n",
    "# Identify rows where the \"Title\" column contains the pattern and the \"Resource Class\" is NaN\n",
    "mask = df['Title'].str.contains(pattern, case=False, na=False) & pd.isna(df['Resource Class'])\n",
    "\n",
    "# Set the \"Resource Class\" for those rows to \"Imagery\"\n",
    "df.loc[mask, 'Resource Class'] = 'Imagery'\n",
    "\n",
    "# Fill missing values in the \"Resource Class\" column with the string \"Other\"\n",
    "df['Resource Class'] = df['Resource Class'].fillna('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Distributions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary CSV for downloads and links created as '05a-01_secondary-links.csv'\n"
     ]
    }
   ],
   "source": [
    "# Mapping from JSON resource_type to reference_type\n",
    "resource_type_mapping = {\n",
    "    \"ags_mapserver\": \"arcgis_dynamic_map_layer\",\n",
    "    \"ags_featureserver\": \"arcgis_feature_layer\",\n",
    "    \"ags_imageserver\": \"arcgis_image_map_layer\",\n",
    "    \"ags_tiled\": \"arcgis_tiled_map_layer\",\n",
    "    \"preview\": \"image\",\n",
    "    \"metadata\": \"metadata_html\",\n",
    "    # Add any additional mappings as needed\n",
    "}\n",
    "\n",
    "# List of download formats with labels\n",
    "download_formats = {\n",
    "    \"fgdb\": \"Geodatabase\",\n",
    "    \"gpkg\": \"GeoPackage\",\n",
    "    \"shp\": \"Shapefile\",\n",
    "    \"csv\": \"CSV\",\n",
    "    \"xlsx\": \"XLSX\",\n",
    "    \"tif\": \"TIF\",\n",
    "    \"cad\": \"CAD\",\n",
    "    \"kmz\": \"KMZ\",\n",
    "    \"geojson\": \"GeoJSON\",\n",
    "    \"json\": \"JSON\",\n",
    "    \"aaigrid\": \"ArcGRID\"\n",
    "}\n",
    "\n",
    "# Combine resource type and download mappings for reference\n",
    "valid_resource_types = set(resource_type_mapping.keys()).union(download_formats.keys())\n",
    "combined_mapping = {**resource_type_mapping, **{key: \"download\" for key in download_formats}}\n",
    "\n",
    "# List to collect rows for the new secondary CSV\n",
    "secondary_rows = []\n",
    "\n",
    "# Iterate through the original DataFrame to process resources\n",
    "for index, row in df.iterrows():\n",
    "    friendlier_id = row['ID']\n",
    "    resources = row['resources']\n",
    "\n",
    "    # Iterate through each resource in the \"resources\" field\n",
    "    for resource in resources:\n",
    "        resource_type = resource.get('resource_type', '')\n",
    "        url = resource.get('url', '')\n",
    "        name = resource.get('name', 'Unspecified')\n",
    "\n",
    "        # Keep only explicitly listed resource types\n",
    "        if resource_type not in valid_resource_types:\n",
    "            continue\n",
    "\n",
    "        if pd.notna(url):\n",
    "            # Determine reference_type and label\n",
    "            reference_type = combined_mapping.get(resource_type, \"other\")  # Default to \"other\" if no match\n",
    "            label = download_formats.get(resource_type, name if name else resource_type)\n",
    "\n",
    "            secondary_rows.append({\n",
    "                \"friendlier_id\": friendlier_id,\n",
    "                \"reference_type\": reference_type,\n",
    "                \"distribution_url\": url,\n",
    "                \"label\": label\n",
    "            })\n",
    "\n",
    "# Add the constructed landing page link\n",
    "for index, row in df.iterrows():\n",
    "    friendlier_id = row['ID']\n",
    "    landing_page_url = f\"https://gisdata.mn.gov/dataset/{friendlier_id}\"\n",
    "    secondary_rows.append({\n",
    "        \"friendlier_id\": friendlier_id,\n",
    "        \"reference_type\": \"documentation_external\",\n",
    "        \"distribution_url\": landing_page_url,\n",
    "        \"label\": \"Landing Page\"\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for the secondary CSV\n",
    "secondary_links_df = pd.DataFrame(secondary_rows)\n",
    "\n",
    "# Write the DataFrame to a CSV file with the required column order\n",
    "secondary_links_df = secondary_links_df[\n",
    "    [\"friendlier_id\", \"reference_type\", \"distribution_url\", \"label\"]\n",
    "]\n",
    "secondary_links_df.to_csv(\"05a-01_secondary-links.csv\", index=False)\n",
    "\n",
    "print(\"Secondary CSV for downloads and links created as '05a-01_secondary-links.csv'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add default and constructed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date Accessioned'] = action_date\n",
    "df['Code'] = \"05a-01\"\n",
    "df['Is Part Of'] = \"05a-01\"\n",
    "df['Member Of'] = \"ba5cc745-21c5-4ae9-954b-72dd8db6815a\"\n",
    "df['Accrual Method'] = \"CKAN\"\n",
    "df['Access Rights'] = \"Public\"\n",
    "df['Language'] = \"eng\"\n",
    "df['Provider'] = \"Minnesota Geospatial Commons\"\n",
    "df['Format'] = \"Files\"\n",
    "df['Publication State'] = \"published\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns and drop unused columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Format',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Theme',\n",
    "'Temporal Coverage',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Access Rights',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method',\n",
    "'Publication State'\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply .str.strip() method to all string columns in the DataFrame and replace newline and tab characters\n",
    "df = df.apply(lambda x: x.str.replace('\\n', ' ').str.replace('\\t', ' ').str.replace('<br/>', ' ').str.replace('<br/><br/>', '|').str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
