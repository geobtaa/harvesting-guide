{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script aims to run the regular re-accession for CKAN portals. Compared with DCAT portals, CKAN updates less frequently. Thus, we often run the script every 3 months.\n",
    "\n",
    "\n",
    "> Orignal created by Yijing Zhou (@YijingZhou33) and Ziying Cheng(@Ziiiiing)\n",
    "\n",
    "> Updated January 15, 2021                           \n",
    "> Updated by Ziying Cheng (@Ziiiiing)\n",
    "\n",
    "> Updated July 05, 2021                           \n",
    "> Updated by Ziying Cheng (@Ziiiiing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that you have the following files and folders in the same directory as this Notebook:\n",
    "\n",
    "- `resource` folder collects existing resource names by portal for each re-accession. The new one will be compared with the latest one to get both the created and deleted datasets.\n",
    "- `reports` folder stores the metadata CSV files for all **New** datasets which are named as `allNewItems_YYYYMMDD.csv`. **Deleted** datasets are also stored within CSV files called `allDeletedItems_YYYYMMDD.csv`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import json \n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-generate the current time in 'YYYYMM' format\n",
    "actionDate = time.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://gisdata.mn.gov/\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the number of items per page\n",
    "items_per_page = 10\n",
    "\n",
    "# Initialize variables for pagination\n",
    "start = 0\n",
    "total_results = 0\n",
    "\n",
    "# List to store all metadata\n",
    "all_metadata = []\n",
    "\n",
    "# Request metadata in paginated manner\n",
    "while True:\n",
    "    # Construct the API request URL with pagination parameters\n",
    "    api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "    \n",
    "    # Request metadata\n",
    "    context = ssl._create_unverified_context()\n",
    "    response = urllib.request.urlopen(api_request_url, context=context).read()\n",
    "    response_json = json.loads(response.decode('utf-8'))\n",
    "    \n",
    "    # Extract metadata from the response\n",
    "    metadata = response_json['result']['results']\n",
    "    all_metadata.extend(metadata)\n",
    "    \n",
    "    # Update pagination variables\n",
    "    start += items_per_page\n",
    "    total_results = response_json['result']['count']\n",
    "    \n",
    "    # Break the loop if we have collected all items\n",
    "    if start >= total_results:\n",
    "        break\n",
    "\n",
    "# Save the metadata to a local JSON file on your desktop\n",
    "desktop_path = \"\"  # Replace with your desktop path\n",
    "output_filename = \"ckan_metadata.json\"\n",
    "output_path = desktop_path + output_filename\n",
    "\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Metadata for {total_results} items saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Specify the path to the JSON file\n",
    "# json_file_path = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = pd.read_json(output_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the resources\n",
    "def process_resources(resources):\n",
    "    result = {}\n",
    "    for resource in resources:\n",
    "        resource_type = resource[\"resource_type\"]\n",
    "        url = resource[\"url\"]\n",
    "        result[resource_type] = url\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Apply the function to create the resource columns\n",
    "resource_columns = df['resources'].apply(process_resources)\n",
    "\n",
    "# Flatten the nested dictionaries within the \"extras\" array\n",
    "def flatten_extras(extras):\n",
    "    flattened = {}\n",
    "    for item in extras:\n",
    "        key = item[\"key\"]\n",
    "        value = item[\"value\"]\n",
    "        flattened[key] = value\n",
    "    return flattened\n",
    "\n",
    "df[\"flattened_extras\"] = df[\"extras\"].apply(flatten_extras)\n",
    "\n",
    "# Expand the flattened_extras dictionary columns into separate columns\n",
    "flattened_extras_df = df[\"flattened_extras\"].apply(pd.Series)\n",
    "\n",
    "# Flatten the \"groups\" dictionaries within the \"groups\" array\n",
    "def flatten_groups(groups):\n",
    "    if isinstance(groups, list):\n",
    "        return \"|\".join([group[\"display_name\"] for group in groups])\n",
    "    return \"\"\n",
    "\n",
    "df[\"group_titles\"] = df[\"groups\"].apply(flatten_groups)\n",
    "\n",
    "# Flatten the \"tags\" dictionaries within the \"tags\" array\n",
    "def flatten_tags(tags):\n",
    "    if isinstance(tags, list):\n",
    "        return \"|\".join([tag[\"display_name\"] for tag in tags])\n",
    "    return \"\"\n",
    "df[\"tag_titles\"] = df[\"tags\"].apply(flatten_tags)\n",
    "\n",
    "# Extract the \"title\" value from the \"organization\" dictionary\n",
    "df[\"organization_title\"] = df[\"organization\"].apply(lambda x: x[\"title\"] if isinstance(x, dict) else \"\")\n",
    "\n",
    "# Select the columns you want to keep from the original DataFrame\n",
    "selected_columns = [\"title\", \"license_title\", \"id\", \"type\", \"name\", \"notes\"]\n",
    "\n",
    "# Select the columns you want to keep from the flattened_extras DataFrame\n",
    "selected_extras_columns = [\"dsAccessConst\", \"dsCurrentRef\", \"dsMetadataUrl\", \"dsModifiedDate\", \"dsOriginator\", \"dsPeriodOfContent\", \"dsPurpose\", \"gdrsDsGuid\", \"spatial\"]\n",
    "\n",
    "# Combine the selected columns from the original, flattened_extras, and resource_columns DataFrames\n",
    "selected_df = pd.concat([df[selected_columns], flattened_extras_df[selected_extras_columns], df[\"group_titles\"], df[\"organization_title\"], df[\"tag_titles\"], resource_columns], axis=1)\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = \"selected_ckan_metadata_with_groups.csv\"  # Update with the desired path\n",
    "\n",
    "# Write the selected DataFrame to a CSV file\n",
    "selected_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"Selected columns, flattened extras, group titles, tag titles, and resources have been written to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
