{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the CKAN API for the Minnesota Geospatial Commons and return the metadata for all items as a CSV file in the GeoBTAA Metadata Application Profile. It will also create a secondary CSV file for the associated multiple downloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare paths and defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-generate the current time in 'YYYYMM' format\n",
    "action_date = time.strftime('%Y%m%d')\n",
    "\n",
    "# # Specify the path to the JSON file\n",
    "output_filename = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://gisdata.mn.gov/\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = action_date + \"_05a-01.csv\"  # Update with the desired path\n",
    "\n",
    "DEFAULT_STATE = \"Minnesota\"\n",
    "\n",
    "counties_in_minnesota = [\n",
    "    'Aitkin', 'Anoka', 'Becker', 'Beltrami', 'Benton', 'Big Stone',\n",
    "    'Blue Earth', 'Brown', 'Carlton', 'Carver', 'Cass', 'Chippewa',\n",
    "    'Chisago', 'Clay', 'Clearwater', 'Cook', 'Cottonwood', 'Crow Wing',\n",
    "    'Dakota', 'Dodge', 'Douglas', 'Faribault', 'Fillmore', 'Freeborn',\n",
    "    'Goodhue', 'Grant', 'Hennepin', 'Houston', 'Hubbard', 'Isanti',\n",
    "    'Itasca', 'Jackson', 'Kanabec', 'Kandiyohi', 'Kittson', 'Koochiching',\n",
    "    'Lac qui Parle', 'Lake', 'Lake of the Woods', 'Le Sueur', 'Lincoln',\n",
    "    'Lyon', 'Mahnomen', 'Marshall', 'Martin', 'McLeod', 'Meeker',\n",
    "    'Mille Lacs', 'Morrison', 'Mower', 'Murray', 'Nicollet', 'Nobles',\n",
    "    'Norman', 'Olmsted', 'Otter Tail', 'Pennington', 'Pine', 'Pipestone',\n",
    "    'Polk', 'Pope', 'Ramsey', 'Red Lake', 'Redwood', 'Renville',\n",
    "    'Rice', 'Rock', 'Roseau', 'St. Louis', 'Scott', 'Sherburne',\n",
    "    'Sibley', 'Stearns', 'Steele', 'Stevens', 'Swift', 'Todd',\n",
    "    'Traverse', 'Wabasha', 'Wadena', 'Waseca', 'Washington', 'Watonwan',\n",
    "    'Wilkin', 'Winona', 'Wright', 'Yellow Medicine'\n",
    "]\n",
    "\n",
    "cities_in_minnesota = [\n",
    "    'Minneapolis', 'St. Paul', 'Rochester', 'Duluth', 'Bloomington',\n",
    "    'Brooklyn Park', 'Plymouth', 'Woodbury', 'St. Cloud', 'Eagan',\n",
    "    'Maple Grove', 'Eden Prairie', 'Coon Rapids', 'Burnsville', 'Blaine',\n",
    "    'Lakeville', 'Minnetonka', 'Apple Valley', 'Edina', 'St. Louis Park','Twin Cities'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the metadata to your desktop\n",
    "\n",
    "This cell will scan the API and create a JSON file on your desktop. This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify the number of items per page\n",
    "# items_per_page = 10\n",
    "\n",
    "# # Initialize variables for pagination\n",
    "# start = 0\n",
    "# total_results = 0\n",
    "\n",
    "# # List to store all metadata\n",
    "# all_metadata = []\n",
    "\n",
    "# # Request metadata in paginated manner\n",
    "# while True:\n",
    "#     # Construct the API request URL with pagination parameters\n",
    "#     api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "    \n",
    "#     # Request metadata\n",
    "#     context = ssl._create_unverified_context()\n",
    "#     response = urllib.request.urlopen(api_request_url, context=context).read()\n",
    "#     response_json = json.loads(response.decode('utf-8'))\n",
    "    \n",
    "#     # Extract metadata from the response\n",
    "#     metadata = response_json['result']['results']\n",
    "#     all_metadata.extend(metadata)\n",
    "    \n",
    "#     # Update pagination variables\n",
    "#     start += items_per_page\n",
    "#     total_results = response_json['result']['count']\n",
    "    \n",
    "#     # Break the loop if we have collected all items\n",
    "#     if start >= total_results:\n",
    "#         break\n",
    "\n",
    "\n",
    "# # Save the metadata to a local JSON file on your desktop\n",
    "# desktop_path = \"\"  # Replace with your desktop path\n",
    "# output_filename = \"ckan_metadata.json\"\n",
    "# output_path = os.path.join(desktop_path, output_filename) # More portable way to join paths\n",
    "\n",
    "# with open(output_path, \"w\") as json_file:\n",
    "#     json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "# print(f\"Metadata for {total_results} items saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the JSON into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file into a DataFrame\n",
    "all_df = pd.read_json(output_filename)\n",
    "\n",
    "# Specify the columns you want to keep and their new names\n",
    "columns_to_keep_and_rename = {\n",
    "    \"id\": \"ID\",\n",
    "    \"title\": \"Alternative Title\",\n",
    "    \"notes\": \"Description\",\n",
    "    \"name\": \"Identifier\",\n",
    "    \"extras\": \"extras\",\n",
    "    \"resources\": \"resources\",\n",
    "    \"groups\": \"groups\",\n",
    "    \"tags\": \"tags\"\n",
    "}\n",
    "\n",
    "# Select and rename the specified columns\n",
    "df = all_df[list(columns_to_keep_and_rename.keys())].rename(columns=columns_to_keep_and_rename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the tags and groups arrays to create Keyword and Theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the values for tags into Keyword\n",
    "\n",
    "df[\"Keyword\"] = df[\"tags\"].apply(lambda x: \"|\".join([tag[\"display_name\"] for tag in x]) if isinstance(x, list) else \"\")\n",
    "\n",
    "# Some of the Theme values need to be renamed\n",
    "theme_mapping = {\n",
    "    \"Biota\": \"Biology\",\n",
    "    \"Geoscientific\": \"Geology\",\n",
    "    \"Imagery + Basemaps\": \"Imagery|Land Cover\",\n",
    "    \"Planning + Cadastre\": \"Property\",\n",
    "    \"Utilities + Communication\": \"Utilities\"\n",
    "}\n",
    "\n",
    "# Function to flatten the \"groups\" and apply the theme mapping\n",
    "def flatten_groups(groups):\n",
    "    if isinstance(groups, list):\n",
    "        group_values = [group[\"display_name\"] for group in groups]\n",
    "        return \"|\".join([theme_mapping.get(value, value) for value in group_values])\n",
    "    return \"\"\n",
    "\n",
    "# Use the flatten_groups function to create the \"Theme\" column\n",
    "df[\"Theme\"] = df[\"groups\"].apply(flatten_groups)\n",
    "\n",
    "# Drop the \"groups\" and \"tags\" columns\n",
    "df.drop(columns=['groups', 'tags'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Write the selected DataFrame to a CSV file\n",
    "# At this point there will be 9 columns \n",
    "# ID\n",
    "# Alternative \n",
    "# Title\n",
    "# Description\n",
    "# Identifier\n",
    "# extras - arrays of various fields\n",
    "# resources - arrays of links\n",
    "# Keyword\n",
    "# Theme\n",
    "\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten extras into a dictionary and expand into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_extras_df = df[\"extras\"].apply(lambda x: {item[\"key\"]: item[\"value\"] for item in x}).apply(pd.Series)\n",
    "\n",
    "# Specify the columns you want to keep from flattened extras and their new names\n",
    "extras_to_keep_and_rename = {\n",
    "    \"dsAccessConst\": \"Rights\",\n",
    "    \"dsCurrentRef\": \"Currentness\",\n",
    "    \"dsMetadataUrl\": \"HTML\",\n",
    "    \"dsOriginator\": \"Creator\",\n",
    "    \"dsPeriodOfContent\": \"Temporal Coverage\",\n",
    "    \"dsPurpose\": \"Purpose\",\n",
    "    \"spatial\": \"spatial\"\n",
    "}\n",
    "\n",
    "# Select, rename, and concatenate the required extras columns\n",
    "df = pd.concat([df, flattened_extras_df[list(extras_to_keep_and_rename.keys())].rename(columns=extras_to_keep_and_rename)], axis=1)\n",
    "\n",
    "# Combine the \"Purpose\", \"Currentness\", and \"Description\" columns, using '|' as a delimiter\n",
    "df['Description'] = df['Description'].fillna('') + '|' + df['Purpose'].fillna('') + '|' + df['Currentness'].fillna('')\n",
    "\n",
    "# Drop the columns that were moved\n",
    "df.drop(columns=['extras', 'Purpose', 'Currentness'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the WKT coordinates into W,S,E,N for Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(wkt_str):\n",
    "    # Default bounding box for Minnesota\n",
    "    default_bounding_box = \"-97.2392,43.4994,-89.4919,49.3845\"\n",
    "\n",
    "    # Check if the value is missing or not a string\n",
    "    if wkt_str is None or not isinstance(wkt_str, str):\n",
    "        return default_bounding_box\n",
    "\n",
    "    try:\n",
    "        # Parse the string as JSON\n",
    "        wkt_json = json.loads(wkt_str)\n",
    "\n",
    "        # Extract the coordinates\n",
    "        coordinates = wkt_json[\"coordinates\"][0]\n",
    "\n",
    "        # Calculate the bounding box\n",
    "        west = min(coord[0] for coord in coordinates)\n",
    "        south = min(coord[1] for coord in coordinates)\n",
    "        east = max(coord[0] for coord in coordinates)\n",
    "        north = max(coord[1] for coord in coordinates)\n",
    "\n",
    "        # Return the bounding box in the required format\n",
    "        return f\"{west},{south},{east},{north}\"\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError):\n",
    "        return default_bounding_box\n",
    "\n",
    "# Apply the function to the 'spatial' column and create a new 'Bounding Box' column\n",
    "df['Bounding Box'] = df['spatial'].apply(get_bounding_box)\n",
    "\n",
    "# Drop the spatial column\n",
    "df.drop(columns=['spatial'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get place names based on Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_coverage(originator):\n",
    "    if pd.isnull(originator): # Check for NaN\n",
    "        return DEFAULT_STATE\n",
    "\n",
    "    coverage_strings = []  # List to store coverage strings\n",
    "\n",
    "    # Check for counties\n",
    "    for county in counties_in_minnesota:\n",
    "        if county in originator:\n",
    "            coverage_strings.append(f\"{DEFAULT_STATE}--{county} County\")\n",
    "\n",
    "    # Check for specific organizations\n",
    "    if \"Metropolitan Council\" in originator or \"MetroGIS\" in originator:\n",
    "        coverage_strings += [f\"{DEFAULT_STATE}--{county} County\" for county in [\"Anoka\", \"Carver\", \"Dakota\", \"Hennepin\", \"Ramsey\", \"Scott\", \"Washington\"]]\n",
    "        coverage_strings.append(f\"{DEFAULT_STATE}--Twin Cities Metropolitan Area\")\n",
    "    elif \"Metropolitan Emergency Services Board\" in originator:\n",
    "        coverage_strings += [f\"{DEFAULT_STATE}--{county} County\" for county in [\"Anoka\", \"Carver\", \"Chisago\", \"Dakota\", \"Hennepin\", \"Isanti\", \"Ramsey\", \"Scott\", \"Sherburne\", \"Washington\"]]\n",
    "        coverage_strings.append(f\"{DEFAULT_STATE}--Twin Cities Metropolitan Area\")\n",
    "    \n",
    "    # If no specific coverage strings were found, return the default state\n",
    "    if not coverage_strings:\n",
    "        return DEFAULT_STATE\n",
    "    \n",
    "    # Join all the coverage strings with the default state at the end\n",
    "    return \"|\".join(coverage_strings) + f\"|{DEFAULT_STATE}\"\n",
    "\n",
    "# Apply the function to the 'dsOriginator' column and create a new 'Spatial Coverage' column\n",
    "df['Spatial Coverage'] = df['Creator'].apply(get_spatial_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat Temporal Coverage and get the Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"Temporal Coverage\" column to a datetime object with the correct format\n",
    "# Using errors='coerce' to handle incorrect date formats\n",
    "df['Temporal Coverage'] = pd.to_datetime(df['Temporal Coverage'], errors='coerce', format='%m/%d/%Y')\n",
    "\n",
    "# Convert the datetime object to the desired string format\n",
    "df['Temporal Coverage'] = df['Temporal Coverage'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Handle NaT values by replacing them with a default value or NaN\n",
    "df['Temporal Coverage'].replace('NaT', pd.NA, inplace=True)\n",
    "\n",
    "def get_date_range(date):\n",
    "    if pd.isnull(date):  # Check for NaN\n",
    "        return None\n",
    "    year = str(date).split('-')[0]  # Extract the year from the date string\n",
    "    return f\"{year}-{year}\"  # Format the date range\n",
    "\n",
    "# Apply the function to the 'Temporal Coverage' column and create a new 'Date Range' column\n",
    "df['Date Range'] = df['Temporal Coverage'].apply(get_date_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_date(alt_title, date):\n",
    "    # Transform the title with the existing logic\n",
    "    transformed_title = transform_title(alt_title)\n",
    "\n",
    "    # Check if date is not missing and is a string\n",
    "    if isinstance(date, str) and '/' in date:\n",
    "        # Extract the year from the date string\n",
    "        year = date.split('/')[-1]\n",
    "\n",
    "        # Append the year in curly brackets\n",
    "        transformed_title += f\" {{{year}}}\"\n",
    "\n",
    "    return transformed_title\n",
    "\n",
    "\n",
    "def transform_title(alt_title):\n",
    "    bracket_content = None\n",
    "    \n",
    "    # 1. Look for \", {county} County, Minnesota\" and replace with \"[Minnesota--{county} County]\"\n",
    "    for county in counties_in_minnesota:\n",
    "        if f\", {county} County, Minnesota\" in alt_title:\n",
    "            bracket_content = f\"[Minnesota--{county} County]\"\n",
    "            alt_title = re.sub(f\", {county} County, Minnesota\", \"\", alt_title, 1)\n",
    "\n",
    "    # 2. Look for \"{city}, Minnesota\" and replace with \"[Minnesota--{city}]\"\n",
    "    if not bracket_content:\n",
    "        for city in cities_in_minnesota:\n",
    "            if f\"{city}, Minnesota\" in alt_title:\n",
    "                bracket_content = f\"[Minnesota--{city}]\"\n",
    "                alt_title = re.sub(f\"{city}, Minnesota\", \"\", alt_title, 1)\n",
    "\n",
    "    # 3. Look for variations of just the state of Minnesota\n",
    "    if not bracket_content:\n",
    "        variations = [r\", Minnesota\", r\"For Minnesota\", r\"Of Minnesota\", r\"In Minnesota\"]\n",
    "        for variation in variations:\n",
    "            if variation.lower() in alt_title.lower():\n",
    "                bracket_content = \"[Minnesota]\"\n",
    "                alt_title = re.sub(variation, \"\", alt_title, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. For everything else, add [\"Minnesota\"] to the end\n",
    "    if not bracket_content:\n",
    "        bracket_content = \"[Minnesota]\"\n",
    "\n",
    "    # Append bracketed content to the end of the title\n",
    "    alt_title = f\"{alt_title.strip()} {bracket_content}\"\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "df['Title'] = df.apply(lambda row: transform_with_date(row['Alternative Title'], row['Temporal Coverage']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Creators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_publisher(publisher):\n",
    "    # Dictionary mapping of publishers for direct transformation\n",
    "    publisher_mappings = {\n",
    "        \"Metropolitan Council\" : \"Metropolitan Council of the Twin Cities Area\",\n",
    "        \"U.S. Geological Survey\" : \"Geological Survey (U.S.)\"\n",
    "\n",
    "    }\n",
    "    \n",
    "    # If a direct mapping is found, return the transformed value\n",
    "    if publisher in publisher_mappings:\n",
    "        return publisher_mappings[publisher]\n",
    "    \n",
    "    # Check for values that start with \"Minnesota Department of Natural Resources\"\n",
    "    if publisher.startswith(\"Minnesota Department of Natural Resources\"):\n",
    "        return \"Minnesota. Department of Natural Resources\"\n",
    "    \n",
    "    # Search for a county name in the publisher string.\n",
    "    for county in counties_in_minnesota:\n",
    "        if county + \" County\" in publisher:\n",
    "            return f\"Minnesota--{county} County\"\n",
    "    else:\n",
    "        for city in cities_in_minnesota:\n",
    "            if f\"City of {city}\" in publisher or city == publisher:\n",
    "                return f\"Minnesota--{city}\"\n",
    "    \n",
    "    # If no match found, return the original publisher string.\n",
    "    return publisher\n",
    "\n",
    "df['Creator'] = df['Creator'].apply(transform_publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Write the selected DataFrame to a CSV file\n",
    "# at this point there will be 15 columns \n",
    "# ID\n",
    "# Alternative Title\n",
    "# Description\n",
    "# Identifier\n",
    "# resources - arrays of links\n",
    "# Keyword\n",
    "# Theme\n",
    "# Rights\n",
    "# HTML\n",
    "# Creator\n",
    "# Temporal Coverage\n",
    "# Bounding Box\n",
    "# Spatial Coverage\n",
    "# Date Range\n",
    "# Title\n",
    "\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns and initialize with None\n",
    "\n",
    "df['Image'] = None\n",
    "df['Display Note'] = None\n",
    "df['Resource Class'] = None\n",
    "df['FeatureServer'] = None\n",
    "df['MapServer'] = None\n",
    "df['ImageServer'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the rows of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    resources = row['resources']\n",
    "    \n",
    "    # Iterate through the resources in the \"resources\" field\n",
    "    for resource in resources:\n",
    "        resource_type = resource.get('resource_type')\n",
    "        url = resource.get('url')\n",
    "        \n",
    "        # Check if the resource type is \"preview\"\n",
    "        if resource_type == 'preview':\n",
    "            # Set the \"Image\" column with the corresponding URL\n",
    "            df.at[index, 'Image'] = url\n",
    "            break # Exit the loop once the preview image is found, assuming only one per record\n",
    "#This code iterates through the 'resources' field in each record, and if it finds a resource with \"resource_type\" set to \"preview\", it extracts the corresponding URL and adds it to the \"Image\" column in the DataFrame for that record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the resources, assign web service types, and determine the Resource Class\n",
    "\n",
    "def classify_urls(row):\n",
    "    for resource in row['resources']:\n",
    "        resource_type = resource['resource_type']\n",
    "        url = resource['url']\n",
    "        \n",
    "        if resource_type in ['ags_mapserver', 'ags_featureserver']:\n",
    "            for server_type in ['FeatureServer', 'MapServer', 'ImageServer']:\n",
    "                if url.endswith(server_type):\n",
    "                    row[server_type] = url + '/0'\n",
    "                    creator = row['Creator']\n",
    "                    row['Display Note'] = f'Tip: This page displays a preview of one layer from the {creator} ArcGIS Rest Service. However, if the web service is in a different project or contains multiple layers, it may not display. Try opening the layer in ArcGIS Online or click the \"Visit Source\" to browse for other layers in the service.'\n",
    "\n",
    "                    # Setting the Resource Class according to the conditions\n",
    "                    if server_type in ['FeatureServer', 'MapServer']:\n",
    "                        row['Resource Class'] = 'Datasets|Web services'\n",
    "                    elif server_type == 'ImageServer':\n",
    "                        row['Resource Class'] = 'Imagery|Web services'\n",
    "                        \n",
    "         # Handle the other resource types\n",
    "        dataset_types = ['fgdb', 'gpkg', 'shp', 'csv', 'xlsx', 'kmz', 'cad', 'aaigrid', 'geojson', 'json']\n",
    "        imagery_type = 'tif'\n",
    "\n",
    "        # Check if the \"Resource Class\" has not been set or it's set to 'Other'\n",
    "        if pd.isna(row.get('Resource Class')):\n",
    "            if resource_type in dataset_types:\n",
    "                row['Resource Class'] = 'Datasets'\n",
    "        elif resource_type == imagery_type:\n",
    "            row['Resource Class'] = 'Imagery'\n",
    "                \n",
    "    return row\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = df.apply(classify_urls, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a Resource Class to the remaining items\n",
    "\n",
    "# Define a pattern that matches 'image' or 'photo' anywhere in the string, without capturing groups\n",
    "pattern = r'(?:image|photo)'\n",
    "\n",
    "# Identify rows where the \"Title\" column contains the pattern and the \"Resource Class\" is NaN\n",
    "mask = df['Title'].str.contains(pattern, case=False, na=False) & pd.isna(df['Resource Class'])\n",
    "\n",
    "# Set the \"Resource Class\" for those rows to \"Imagery\"\n",
    "df.loc[mask, 'Resource Class'] = 'Imagery'\n",
    "\n",
    "# Fill missing values in the \"Resource Class\" column with the string \"Other\"\n",
    "df['Resource Class'] = df['Resource Class'].fillna('Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the multiple downloads file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of download formats\n",
    "download_formats = {\n",
    "    \"fgdb\": \"Geodatabase\",\n",
    "    \"gpkg\": \"GeoPackage\",\n",
    "    \"shp\": \"Shapefile\",\n",
    "    \"csv\": \"CSV\",\n",
    "    \"xlsx\": \"XLSX\",\n",
    "    \"tif\": \"TIF\",\n",
    "    \"cad\": \"CAD\",\n",
    "    \"kmz\": \"KMZ\",\n",
    "    \"geojson\": \"GeoJSON\",\n",
    "    \"json\": \"JSON\",\n",
    "    'aaigrid': \"ArcGRID\"\n",
    "}\n",
    "# List to collect the rows for the new DataFrame\n",
    "rows = []\n",
    "\n",
    "# Iterate through the original DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    friendlier_id = row['ID']\n",
    "    resources = row['resources']\n",
    "    for resource in resources:\n",
    "        resource_type = resource['resource_type']\n",
    "        url = resource['url']\n",
    "        if resource_type in download_formats and pd.notna(url):\n",
    "            # Append the row if the resource_type matches and url is not NaN\n",
    "            rows.append({\"friendlier_id\": friendlier_id, \"label\": resource_type, \"value\": url})\n",
    "\n",
    "# Create a new DataFrame using the collected rows\n",
    "multiple_downloads_df = pd.DataFrame(rows)\n",
    "\n",
    "# Replace the values in the 'label' column according to the mapping provided in download_formats\n",
    "multiple_downloads_df['label'] = multiple_downloads_df['label'].replace(download_formats)\n",
    "\n",
    "# Write the DataFrame to a CSV\n",
    "multiple_downloads_df.to_csv(\"05a-01_multiple-downloads.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add default and constructed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date Accessioned'] = action_date\n",
    "df['Code'] = \"05a-01\"\n",
    "df['Is Part Of'] = \"05a-01\"\n",
    "df['Member Of'] = \"ba5cc745-21c5-4ae9-954b-72dd8db6815a\"\n",
    "df['Accrual Method'] = \"CKAN\"\n",
    "df['Access Rights'] = \"Public\"\n",
    "df['Language'] = \"eng\"\n",
    "df['Provider'] = \"Minnesota Geospatial Commons\"\n",
    "df['Information'] = \"https://gisdata.mn.gov/dataset/\" + df['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired order of columns\n",
    "desired_order = [\n",
    "'Title',\n",
    "'Alternative Title',\n",
    "'Description',\n",
    "'Language',\n",
    "'Display Note',\n",
    "'Creator',\n",
    "'Provider',\n",
    "'Resource Class',\n",
    "'Theme',\n",
    "'Temporal Coverage',\n",
    "'Date Range',\n",
    "'Spatial Coverage',\n",
    "'Bounding Box',\n",
    "'Member Of',\n",
    "'Is Part Of',\n",
    "'FeatureServer',\n",
    "'HTML',\n",
    "'ImageServer',\n",
    "'Information',\n",
    "'MapServer',\n",
    "'ID',\n",
    "'Identifier',\n",
    "'Rights',\n",
    "'Access Rights',\n",
    "'Date Accessioned',\n",
    "'Code',\n",
    "'Accrual Method'\n",
    "\n",
    "# Add more columns as needed in the desired order\n",
    "]\n",
    "\n",
    "# Reindex the DataFrame based on the desired order of columns\n",
    "df = df.reindex(columns=desired_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply .str.strip() method to all string columns in the DataFrame and replace newline and tab characters\n",
    "df = df.apply(lambda x: x.str.replace('\\n', ' ').str.replace('\\t', ' ').str.replace('<br/>', ' ').str.replace('<br/><br/>', '|').str.strip() if x.dtype == \"object\" else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected DataFrame to a CSV file\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
