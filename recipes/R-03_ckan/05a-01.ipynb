{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import json \n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from html.parser import HTMLParser\n",
    "import re\n",
    "import ast\n",
    "import decimal\n",
    "import ssl\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-generate the current time in 'YYYYMM' format\n",
    "actionDate = time.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import ssl\n",
    "\n",
    "# Specify the CKAN portal URL you want to harvest from\n",
    "portalURL = \"https://gisdata.mn.gov/\"\n",
    "\n",
    "# Construct the API URL for package search\n",
    "packageURL = portalURL + 'api/3/action/package_search'\n",
    "\n",
    "# Specify the number of items per page\n",
    "items_per_page = 10\n",
    "\n",
    "# Initialize variables for pagination\n",
    "start = 0\n",
    "total_results = 0\n",
    "\n",
    "# List to store all metadata\n",
    "all_metadata = []\n",
    "\n",
    "# Request metadata in paginated manner\n",
    "while True:\n",
    "    # Construct the API request URL with pagination parameters\n",
    "    api_request_url = f\"{packageURL}?start={start}&rows={items_per_page}\"\n",
    "    \n",
    "    # Request metadata\n",
    "    context = ssl._create_unverified_context()\n",
    "    response = urllib.request.urlopen(api_request_url, context=context).read()\n",
    "    response_json = json.loads(response.decode('utf-8'))\n",
    "    \n",
    "    # Extract metadata from the response\n",
    "    metadata = response_json['result']['results']\n",
    "    all_metadata.extend(metadata)\n",
    "    \n",
    "    # Update pagination variables\n",
    "    start += items_per_page\n",
    "    total_results = response_json['result']['count']\n",
    "    \n",
    "    # Break the loop if we have collected all items\n",
    "    if start >= total_results:\n",
    "        break\n",
    "\n",
    "# Save the metadata to a local JSON file on your desktop\n",
    "desktop_path = \"\"  # Replace with your desktop path\n",
    "output_filename = \"ckan_metadata.json\"\n",
    "output_path = desktop_path + output_filename\n",
    "\n",
    "with open(output_path, \"w\") as json_file:\n",
    "    json.dump(all_metadata, json_file, indent=4)\n",
    "\n",
    "print(f\"Metadata for {total_results} items saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # Specify the path to the JSON file\n",
    "output_filename = \"ckan_metadata.json\"  # Update with the correct path\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = pd.read_json(output_filename)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns, flattened extras, group titles, tag titles, and resources have been written to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Function to process the resources\n",
    "def process_resources(resources):\n",
    "    result = {}\n",
    "    for resource in resources:\n",
    "        resource_type = resource[\"resource_type\"]\n",
    "        url = resource[\"url\"]\n",
    "        result[resource_type] = url\n",
    "    return pd.Series(result)\n",
    "\n",
    "# Apply the function to create the resource columns\n",
    "resource_columns = df['resources'].apply(process_resources)\n",
    "\n",
    "# Flatten the nested dictionaries within the \"extras\" array\n",
    "def flatten_extras(extras):\n",
    "    flattened = {}\n",
    "    for item in extras:\n",
    "        key = item[\"key\"]\n",
    "        value = item[\"value\"]\n",
    "        flattened[key] = value\n",
    "    return flattened\n",
    "\n",
    "df[\"flattened_extras\"] = df[\"extras\"].apply(flatten_extras)\n",
    "\n",
    "# Expand the flattened_extras dictionary columns into separate columns\n",
    "flattened_extras_df = df[\"flattened_extras\"].apply(pd.Series)\n",
    "\n",
    "# Flatten the \"groups\" dictionaries within the \"groups\" array\n",
    "def flatten_groups(groups):\n",
    "    if isinstance(groups, list):\n",
    "        return \"|\".join([group[\"display_name\"] for group in groups])\n",
    "    return \"\"\n",
    "\n",
    "df[\"group_titles\"] = df[\"groups\"].apply(flatten_groups)\n",
    "\n",
    "# Flatten the \"tags\" dictionaries within the \"tags\" array\n",
    "def flatten_tags(tags):\n",
    "    if isinstance(tags, list):\n",
    "        return \"|\".join([tag[\"display_name\"] for tag in tags])\n",
    "    return \"\"\n",
    "df[\"tag_titles\"] = df[\"tags\"].apply(flatten_tags)\n",
    "\n",
    "# Extract the \"title\" value from the \"organization\" dictionary\n",
    "df[\"organization_title\"] = df[\"organization\"].apply(lambda x: x[\"title\"] if isinstance(x, dict) else \"\")\n",
    "\n",
    "# Select the columns you want to keep from the original DataFrame\n",
    "selected_columns = [\n",
    "    \"title\",  \n",
    "    \"id\", \n",
    "    \"name\", \n",
    "    \"notes\"\n",
    "]\n",
    "\n",
    "# Select the columns you want to keep from the flattened_extras DataFrame\n",
    "selected_extras_columns = [\n",
    "    \"dsAccessConst\", \n",
    "    \"dsCurrentRef\", \n",
    "    \"dsMetadataUrl\", \n",
    "    \"dsModifiedDate\", \n",
    "    \"dsOriginator\", \n",
    "    \"dsPeriodOfContent\", \n",
    "    \"dsPurpose\", \n",
    "    \"gdrsDsGuid\", \n",
    "    \"spatial\"\n",
    "]\n",
    "\n",
    "# Combine the selected columns from the original, flattened_extras, and resource_columns DataFrames\n",
    "selected_df = pd.concat([df[selected_columns], flattened_extras_df[selected_extras_columns], df[\"group_titles\"], df[\"organization_title\"], df[\"tag_titles\"], resource_columns], axis=1)\n",
    "\n",
    "# Specify the path for the CSV file\n",
    "csv_file_path = actionDate + \"_05a-01.csv\"  # Update with the desired path\n",
    "\n",
    "# Write the selected DataFrame to a CSV file\n",
    "selected_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"Selected columns, flattened extras, group titles, tag titles, and resources have been written to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_in_minnesota = [\n",
    "    'Aitkin', 'Anoka', 'Becker', 'Beltrami', 'Benton', 'Big Stone',\n",
    "    'Blue Earth', 'Brown', 'Carlton', 'Carver', 'Cass', 'Chippewa',\n",
    "    'Chisago', 'Clay', 'Clearwater', 'Cook', 'Cottonwood', 'Crow Wing',\n",
    "    'Dakota', 'Dodge', 'Douglas', 'Faribault', 'Fillmore', 'Freeborn',\n",
    "    'Goodhue', 'Grant', 'Hennepin', 'Houston', 'Hubbard', 'Isanti',\n",
    "    'Itasca', 'Jackson', 'Kanabec', 'Kandiyohi', 'Kittson', 'Koochiching',\n",
    "    'Lac qui Parle', 'Lake', 'Lake of the Woods', 'Le Sueur', 'Lincoln',\n",
    "    'Lyon', 'Mahnomen', 'Marshall', 'Martin', 'McLeod', 'Meeker',\n",
    "    'Mille Lacs', 'Morrison', 'Mower', 'Murray', 'Nicollet', 'Nobles',\n",
    "    'Norman', 'Olmsted', 'Otter Tail', 'Pennington', 'Pine', 'Pipestone',\n",
    "    'Polk', 'Pope', 'Ramsey', 'Red Lake', 'Redwood', 'Renville',\n",
    "    'Rice', 'Rock', 'Roseau', 'St. Louis', 'Scott', 'Sherburne',\n",
    "    'Sibley', 'Stearns', 'Steele', 'Stevens', 'Swift', 'Todd',\n",
    "    'Traverse', 'Wabasha', 'Wadena', 'Waseca', 'Washington', 'Watonwan',\n",
    "    'Wilkin', 'Winona', 'Wright', 'Yellow Medicine'\n",
    "]\n",
    "\n",
    "cities_in_minnesota = [\n",
    "    'Minneapolis', 'St. Paul', 'Rochester', 'Duluth', 'Bloomington',\n",
    "    'Brooklyn Park', 'Plymouth', 'Woodbury', 'St. Cloud', 'Eagan',\n",
    "    'Maple Grove', 'Eden Prairie', 'Coon Rapids', 'Burnsville', 'Blaine',\n",
    "    'Lakeville', 'Minnetonka', 'Apple Valley', 'Edina', 'St. Louis Park','Twin Cities'\n",
    "]\n",
    "\n",
    "def transform_with_date(alt_title, date):\n",
    "    # Transform the title with the existing logic\n",
    "    transformed_title = transform_title(alt_title)\n",
    "\n",
    "    # Check if date is not missing and is a string\n",
    "    if isinstance(date, str) and '/' in date:\n",
    "        # Extract the year from the date string\n",
    "        year = date.split('/')[-1]\n",
    "\n",
    "        # Append the year in curly brackets\n",
    "        transformed_title += f\" {{{year}}}\"\n",
    "\n",
    "    return transformed_title\n",
    "\n",
    "\n",
    "\n",
    "def transform_title(alt_title):\n",
    "    bracket_content = None\n",
    "    \n",
    "    # 1. Look for \", {county} County, Minnesota\" and replace with \"[Minnesota--{county} County]\"\n",
    "    for county in counties_in_minnesota:\n",
    "        if f\", {county} County, Minnesota\" in alt_title:\n",
    "            bracket_content = f\"[Minnesota--{county} County]\"\n",
    "            alt_title = re.sub(f\", {county} County, Minnesota\", \"\", alt_title, 1)\n",
    "\n",
    "    # 2. Look for \"{city}, Minnesota\" and replace with \"[Minnesota--{city}]\"\n",
    "    if not bracket_content:\n",
    "        for city in cities_in_minnesota:\n",
    "            if f\"{city}, Minnesota\" in alt_title:\n",
    "                bracket_content = f\"[Minnesota--{city}]\"\n",
    "                alt_title = re.sub(f\"{city}, Minnesota\", \"\", alt_title, 1)\n",
    "\n",
    "    # 3. Look for variations of just the state of Minnesota\n",
    "    if not bracket_content:\n",
    "        variations = [r\", Minnesota\", r\"For Minnesota\", r\"Of Minnesota\", r\"In Minnesota\"]\n",
    "        for variation in variations:\n",
    "            if variation.lower() in alt_title.lower():\n",
    "                bracket_content = \"[Minnesota]\"\n",
    "                alt_title = re.sub(variation, \"\", alt_title, flags=re.IGNORECASE)\n",
    "\n",
    "    # 4. For everything else, add [\"Minnesota\"] to the end\n",
    "    if not bracket_content:\n",
    "        bracket_content = \"[Minnesota]\"\n",
    "\n",
    "    # Append bracketed content to the end of the title\n",
    "    alt_title = f\"{alt_title.strip()} {bracket_content}\"\n",
    "\n",
    "    return alt_title\n",
    "\n",
    "selected_df['New_Title'] = selected_df.apply(lambda row: transform_with_date(row['title'], row['dsPeriodOfContent']), axis=1)\n",
    "\n",
    "\n",
    "# selected_df['New_Title'] = selected_df['title'].apply(transform_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(date):\n",
    "    if pd.isnull(date) or '/' not in str(date):  # Check for NaN or incorrect format\n",
    "        return None\n",
    "    year = str(date).split('/')[-1]  # Extract the year from the date string\n",
    "    return f\"{year}-{year}\"  # Format the date range\n",
    "\n",
    "# Apply the function to the 'dsPeriodOfContent' column and create a new 'Date Range' column\n",
    "selected_df['Date Range'] = selected_df['dsPeriodOfContent'].apply(get_date_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_coverage(originator):\n",
    "    if pd.isnull(originator): # Check for NaN\n",
    "        return \"Minnesota\"\n",
    "\n",
    "    # Check for counties\n",
    "    for county in counties_in_minnesota:\n",
    "        if county in originator:\n",
    "            return f\"Minnesota--{county} County|Minnesota\"\n",
    "\n",
    "    # Check for specific organizations\n",
    "    if \"Metropolitan Council\" in originator or \"MetroGIS\" in originator:\n",
    "        return \"Minnesota--Anoka County|Minnesota--Carver County|Minnesota--Dakota County|Minnesota--Hennepin County|Minnesota--Ramsey County|Minnesota--Scott County|Minnesota--Washington County|Minnesota--Twin Cities Metropolitan Area|Minnesota\"\n",
    "    elif \"Metropolitan Emergency Services Board\" in originator:\n",
    "        return \"Minnesota--Anoka County|Minnesota--Carver County|Minnesota--Chisago County|Minnesota--Dakota County|Minnesota--Hennepin County|Minnesota--Isanti County|Minnesota--Ramsey County|Minnesota--Scott County|Minnesota--Sherburne County|Minnesota--Washington County|Minnesota--Twin Cities Metropolitan Area|Minnesota\"\n",
    "    \n",
    "    return \"Minnesota\"\n",
    "\n",
    "# Apply the function to the 'dsOriginator' column and create a new 'Spatial Coverage' column\n",
    "selected_df['Spatial Coverage'] = selected_df['dsOriginator'].apply(get_spatial_coverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(wkt_str):\n",
    "    # Default bounding box for Minnesota\n",
    "    default_bounding_box = \"-97.2392,43.4994,-89.4919,49.3845\"\n",
    "\n",
    "    # Check if the value is missing or not a string\n",
    "    if wkt_str is None or not isinstance(wkt_str, str):\n",
    "        return default_bounding_box\n",
    "\n",
    "    try:\n",
    "        # Parse the string as JSON\n",
    "        wkt_json = json.loads(wkt_str)\n",
    "\n",
    "        # Extract the coordinates\n",
    "        coordinates = wkt_json[\"coordinates\"][0]\n",
    "\n",
    "        # Calculate the bounding box\n",
    "        west = min(coord[0] for coord in coordinates)\n",
    "        south = min(coord[1] for coord in coordinates)\n",
    "        east = max(coord[0] for coord in coordinates)\n",
    "        north = max(coord[1] for coord in coordinates)\n",
    "\n",
    "        # Return the bounding box in the required format\n",
    "        return f\"{west},{south},{east},{north}\"\n",
    "\n",
    "    except (json.JSONDecodeError, KeyError, TypeError):\n",
    "        return default_bounding_box\n",
    "\n",
    "# Apply the function to the 'spatial' column and create a new 'Bounding Box' column\n",
    "selected_df['Bounding Box'] = selected_df['spatial'].apply(get_bounding_box)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversion mappings from old values to new values\n",
    "theme_mapping = {\n",
    "\n",
    "    \"Biota\": \"Biology\",\n",
    "    \"Geoscientific\": \"Geology\",\n",
    "    \"Imagery + Basemaps\": \"Imagery|Land Cover\",\n",
    "    \"Planning + Cadastre\": \"Property\",\n",
    "    \"Utilities + Communication\": \"Utilities\"\n",
    "    \n",
    "    # all the other values can be copied as is\n",
    "}\n",
    "\n",
    "\n",
    "# Function to apply the mapping and join the values back together\n",
    "def convert_and_join(row):\n",
    "    group_values = row['group_titles']\n",
    "    if pd.notna(group_values):  # Check for NaN before splitting\n",
    "        group_values = group_values.split('|')\n",
    "        converted_values = []\n",
    "        for value in group_values:\n",
    "            if value in theme_mapping:\n",
    "                converted_values.append(theme_mapping[value])\n",
    "            else:  # If the value is not in theme_mapping, append it as-is\n",
    "                converted_values.append(value)\n",
    "        return '|'.join(converted_values)\n",
    "    else:\n",
    "        return ''  # Return an empty string if the value is NaN\n",
    "\n",
    "# Apply the mapping and create the new \"Theme\" column\n",
    "selected_df['Theme'] = selected_df.apply(convert_and_join, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected DataFrame to a CSV file\n",
    "selected_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
