{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a446268",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the [DCAT 1.1 API](https://resources.data.gov/resources/dcat-us/) of ArcGIS Hubs and return the metadata for all suitable items as a CSV file in the GeoBTAA Metadata Application Profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7df424",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## STEP 1: Prepare the list of active ArcGIS Hubs\n",
    "\n",
    "We maintain a list of active ArcGIS Hub sites in GEOMG. (Access to GEOMG requires a login account. External users can create their own list or use one provided in this repository)\n",
    "\n",
    "1. Filter for items with these parameters:\n",
    "   - Resource Class: Websites\n",
    "   - Accrual Method: DCAT US 1.1\n",
    "   - [Shortcut query](https://geomg.lib.umn.edu/documents?f%5Bb1g_dct_accrualMethod_s%5D%5B%5D=DCAT+US+1.1&f%5Bgbl_resourceClass_sm%5D%5B%5D=Websites&rows=20&sort=score+desc)\n",
    "   \n",
    "2. Rename the downloaded file `arcHubs.csv` and move it into the same directory as this Notebook.\n",
    "\n",
    "\n",
    "    \n",
    "Exporting from GEOMG will produce a CSV containing all of the metadata associated with each Hub. For this script, the only fields used are:\n",
    "\n",
    "* **ID**: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.\n",
    "* **Title**: The name of the Hub. This is transferred to the \"Provider\" field for each dataset\n",
    "* **Publisher**: The place or administration associated with the portal. This is applied to the title in each dataset in brackets\n",
    "* **Spatial Coverage**: A list of place names. These are transferred to the Spatial Coverage for each dataset\n",
    "* **Member Of**: a larger collection level record. Most of the Hubs are either part of our [Government Open Geospatial Data Collection](https://geo.btaa.org/catalog/ba5cc745-21c5-4ae9-954b-72dd8db6815a) or the [Research Institutes Geospatial Data Collection](https://geo.btaa.org/catalog/b0153110-e455-4ced-9114-9b13250a7093)\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8a6b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## STEP 2a: Define the module-level code\n",
    "\n",
    "This section includes the necessary imports, configuration settings, and function/class definitions that will be used by the rest of the code in the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff38b476",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import csv # Provides functionality to read from and write to CSV files.\n",
    "import json # Provides functionality to work with JSON data.\n",
    "import os # Provides a way of using operating system dependent functionality, like reading or writing the file system.\n",
    "import re # Provides regular expression matching operations.\n",
    "import time # Provides functions for working with time, including time conversion, sleep function and timers.\n",
    "import urllib.request # provides functions for working with URLs, like opening URLs, reading data from URLs, etc.\n",
    "from html.parser import HTMLParser # provides an HTML parsing library that can be used to extract data from HTML docs.\n",
    "from urllib.parse import urlparse, parse_qs # provides a way to parse URLs into their components.\n",
    "\n",
    "import numpy as np # Provides numerical operations and array manipulation tools.\n",
    "import pandas as pd # Provides data manipulation and analysis functionality.\n",
    "import requests # Provides HTTP library for sending requests to servers and receiving responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274c4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Set up paths and output CSV field names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a38c9b27",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "directory = \".\"  # Set to directory containing arcHubs.csv\n",
    "hubFile = \"arcHubs.csv\"  # the name of the CSV file with the list of ArcGIS Hubs\n",
    "fieldnames = [  # DCAT schema fields to be included in report\n",
    "    \"Title\",\n",
    "    \"Alternative Title\",\n",
    "    \"Description\",\n",
    "    \"Language\",\n",
    "    \"Creator\",\n",
    "    \"Title Source\",\n",
    "    \"Resource Class\",\n",
    "    \"Resource Type\",\n",
    "    \"Keyword\",\n",
    "    \"Date Issued\",\n",
    "    \"Temporal Coverage\",\n",
    "    \"Date Range\",\n",
    "    \"Spatial Coverage\",\n",
    "    \"Bounding Box\",\n",
    "    \"Format\",\n",
    "    \"Information\",\n",
    "    \"Download\",\n",
    "    \"MapServer\",\n",
    "    \"FeatureServer\",\n",
    "    \"ImageServer\",\n",
    "    \"ID\",\n",
    "    \"Identifier\",\n",
    "    \"Provider\",\n",
    "    \"Code\",\n",
    "    \"Member Of\",\n",
    "    \"Is Part Of\",\n",
    "    \"Rights\",\n",
    "    \"Accrual Method\",\n",
    "    \"Date Accessioned\",\n",
    "    \"Access Rights\",\n",
    "]\n",
    "\n",
    "ActionDate = time.strftime('%Y%m%d') # Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ba35c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Function to remove HTML tags**\n",
    "\n",
    "Sometimes, the metadata fields we scrape contain HTML tags, such as links or formatting that do not work in the Geoportal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7616ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser): \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d): \n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self): # Returns a string of all the data in the list concatenated together.\n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html): # Defined by the MLS Stripper \n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value): # Calls strip_tags on the input value to remove any HTML tags present.\n",
    "    return strip_tags(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220f9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Function to generate an output CSV**\n",
    "\n",
    "iterate over the keys and writes the corresponding values to the CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6391c727",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for hub in dictionary:\n",
    "            for keys in hub:\n",
    "                allvalues = hub[keys]\n",
    "                csvout.writerow(allvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba373d6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Function to create a dictionary of metadata in the JSONs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f3391b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# use the len function to get the number of datasets and the range function to loop through each dataset\n",
    "        \n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}  # Dictionary List\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a3aea",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Function to generate the title as: alternativeTitle [place name] {year}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1908624",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The function uses regular expressions to extract the year from the alternative title, and replaces it with an empty string to remove it from the title.\n",
    "\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    # find if year exist in alternativeTitle\n",
    "    year = ''\n",
    "    try:  \n",
    "      year_range = re.findall(r'(\\d{4})-(\\d{4})', alternativeTitle)\n",
    "    except:\n",
    "      year_range = ''\n",
    "    try: \n",
    "      single_year = re.match(r'.*(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})', alternativeTitle)\n",
    "    except:\n",
    "      single_year = ''    \n",
    "    if year_range:   # if a 'yyyy-yyyy' exists\n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  # or if a 'yyyy' exists\n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "     \n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'       \n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1c72e",
   "metadata": {},
   "source": [
    "**Function to round the bounding box coordinates to two decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de38c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_spatial_coords(spatial):\n",
    "    try:\n",
    "        # Extract the four coordinate values from the 'Spatial' field\n",
    "        coords = [float(coord.strip()) for coord in spatial.split(',')]\n",
    "        # Round each coordinate value to two decimal places\n",
    "        coords = [round(coord, 2) for coord in coords]\n",
    "        # Construct the rounded 'Spatial' field value with the new coordinate values\n",
    "        new_spatial = ','.join([str(coord) for coord in coords])\n",
    "        return new_spatial\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9df9",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Function to create a dictionary of scanned metadata**\n",
    "\n",
    "This code defines a function called `metadataNewItems()` which takes two arguments \n",
    "\n",
    "* `newdata` (a dictionary containing metadata information about new items)\n",
    "* `newitem_ids` (a dictionary containing information about the new items such as the position and the landing page URLs).\n",
    "\n",
    "The function processes the metadata information for each new item and creates a dictionary containing the formatted metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6eb10c60",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This includes blank fields '' for some columns\n",
    "\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    # y = position of the dataset in the DCAT metadata json, v = landing page URLs\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "        \n",
    "\n",
    "#ALTERNATIVE TITLE\n",
    "       \n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = cleanData(newdata[\"dataset\"][y]['title'])\n",
    "        except:\n",
    "            alternativeTitle = newdata[\"dataset\"][y]['title']\n",
    "            \n",
    "# TITLE\n",
    "            \n",
    "        # call the format_title function\n",
    "        title = format_title(alternativeTitle, titleSource)\n",
    "\n",
    "\n",
    "\n",
    "# BOUNDING BOX\n",
    "    \n",
    "        # call the round_spatial_coords function\n",
    "        spatial = newdata[\"dataset\"][y][\"spatial\"] \n",
    "        bbox = round_spatial_coords(spatial)\n",
    "\n",
    "            \n",
    "#DESCRIPTION\n",
    "\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        description = description.replace(\"{{default.description}}\", \"\").replace(\"{{description}}\", \"\")\n",
    "        description = re.sub(r'[\\n]+|[\\r\\n]+', ' ', description, flags=re.S)\n",
    "        description = re.sub(r'\\s{2,}', ' ', description)\n",
    "        description = description.translate({8217: \"'\", 8220: '\"', 8221: '\"', 160: \"\", 183: \"\", 8226: \"\", 8211: \"-\", 8203: \"\"})\n",
    "\n",
    "\n",
    "# RESOURCE TYPE\n",
    "\n",
    "        # if 'LiDAR' exists in Title or Description, add it to Resource Type\n",
    "        if 'LiDAR' in title or 'LiDAR' in description:\n",
    "            resourceType = 'LiDAR'\n",
    "                            \n",
    "#CREATOR\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "\n",
    "\n",
    "# DISTRIBUTION\n",
    "\n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "\n",
    "        format_types = []\n",
    "        resourceClass = \"\"\n",
    "        formatElement = \"\"\n",
    "        downloadURL = \"\"\n",
    "        resourceType = \"\"\n",
    "        webService = \"\"\n",
    "        featureServer = \"\"\n",
    "        mapServer = \"\"\n",
    "        imageServer = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                # If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    resourceClass = \"Datasets|Web services\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"].split('?')[0]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"].split('?')[0]\n",
    "\n",
    "                    resourceType = \"Vector data\"\n",
    "\n",
    "                # If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"ArcGIS GeoService\":\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "\n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            resourceClass = \"Imagery|Web services\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            resourceType = \"Satellite imagery\"\n",
    "                    else:\n",
    "                        resourceClass = \"\"\n",
    "                        formatElement = \"\"\n",
    "                        downloadURL = \"\"\n",
    "\n",
    "            # If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                resourceClass = \"\"\n",
    "                formatElement = \"\"\n",
    "                downloadURL = \"\"\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            if \"FeatureServer\" in webService:\n",
    "                featureServer = webService\n",
    "            if \"MapServer\" in webService:\n",
    "                mapServer = webService\n",
    "            if \"ImageServer\" in webService:\n",
    "                imageServer = webService\n",
    "        except:\n",
    "            print(identifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "# KEYWORDS\n",
    "\n",
    "        keyword = newdata[\"dataset\"][y][\"keyword\"]\n",
    "        keyword_list = []\n",
    "        keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "\n",
    "        \n",
    "# DATES\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "\n",
    "        # auto-generate Temporal Coverage and Date Range\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     # if title has {YYYY} or {YYYY-YYYY}\n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            dateRange = temporalCoverage[:4] + '-' + temporalCoverage[-4:]\n",
    "        else:\n",
    "            temporalCoverage = 'Continually updated resource'\n",
    "        \n",
    "#RIGHTS\n",
    "\n",
    "        rights = cleanData(newdata[\"dataset\"][y]['license']) if 'license' in newdata[\"dataset\"][y] else \"\"\n",
    "\n",
    "\n",
    "# IDENTIFIER\n",
    "        slug = identifier.split('=', 1)[-1].replace(\"&sublayer=\", \"_\")\n",
    "        querystring = parse_qs(urlparse(identifier).query)\n",
    "        identifier_new = \"https://hub.arcgis.com/datasets/\" + \"\" + querystring[\"id\"][0]\n",
    "\n",
    "            \n",
    "# Define full metadata list\n",
    "\n",
    "        metadataList = [\n",
    "            title, \n",
    "            alternativeTitle, \n",
    "            description, \n",
    "            language, \n",
    "            creator,\n",
    "            titleSource,\n",
    "            resourceClass, \n",
    "            resourceType, \n",
    "            keyword_list, \n",
    "            dateIssued, \n",
    "            temporalCoverage,\n",
    "            dateRange, \n",
    "            spatialCoverage, \n",
    "            bbox,\n",
    "            formatElement, \n",
    "            information, \n",
    "            downloadURL, \n",
    "            mapServer, \n",
    "            featureServer,\n",
    "            imageServer, \n",
    "            slug, \n",
    "            identifier_new, \n",
    "            provider, \n",
    "            hubCode, \n",
    "            memberOf, \n",
    "            isPartOf, \n",
    "            rights,\n",
    "            accrualMethod,\n",
    "            dateAccessioned, \n",
    "            accessRights\n",
    "        ]     \n",
    "\n",
    "        # deletes items where the resourceClass is empty\n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[5] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[slug] = metadata\n",
    "\n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef627b2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2b: Run the executable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9c0be",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Declare a list to hold the scanned metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2ba8d6d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "allRecords = []\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1069f4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Scan the metadata for each Hub**\n",
    "\n",
    "This code reads data from `hubFile.csv` using the `csv.DictReader` function. It then iterates over each row in the file and extracts values from specific columns to be used later in the script.\n",
    "\n",
    "For each row, the script also defines default values for a set of metadata fields. It then checks if the URL provided in the CSV file exists and is a valid JSON response. If the response is not valid, the script prints an error message and continues to the next row. Otherwise, it extracts dataset identifiers from the JSON response and passes the response along with the identifiers to a function called metadataNewItems. The metadata for each row is then appended to a list called `allRecords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "908a93c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning  05b-27137 https://open-data-slcgis.hub.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  10b-55109 https://gis-scccdd.opendata.arcgis.com/api/feed/dcat-us/1.1.json\n",
      "scanning  11b-39151 http://opendata.starkcountyohio.gov/api/feed/dcat-us/1.1.json\n"
     ]
    }
   ],
   "source": [
    "with open(hubFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Read in values from arcHubs.csv to be used within the script or as part of the metadata report\n",
    "        hubCode = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        isPartOf = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        \n",
    "        # Define default values for each record\n",
    "        accrualMethod = \"ArcGIS Hub\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        language = \"eng\"\n",
    "\n",
    "        print(\"scanning \", hubCode, url)\n",
    "        \n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        # check if the Hub's URL is broken\n",
    "        if response.headers['content-type'] != 'application/json; charset=utf-8':\n",
    "            print(\"\\n--------------------- Data hub URL does not exist --------------------\\n\",\n",
    "                  hubCode, url,  \"\\n--------------------------------------------------------------------------\\n\")\n",
    "            continue\n",
    "        else:\n",
    "            newdata = json.load(response)\n",
    "\n",
    "\n",
    "        # Makes a list of dataset identifiers\n",
    "        newjson_ids = getIdentifiers(newdata)\n",
    "\n",
    "\n",
    "        allRecords.append(metadataNewItems(newdata, newjson_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae82f15b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Write the scanned metadata to a CSV in the GeoBTAA Metadata Profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d787b12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "newItemsReport = f\"{directory}/{ActionDate}_scannedRecords.csv\"\n",
    "printItemReport(newItemsReport, fieldnames, allRecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffd82a",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Drop duplicate items**\n",
    "\n",
    "ArcGIS Hub administrators can include datasets from other Hubs in their own site. As a result, some datasets are duplicated in other Hubs. However, they always have the same Identifier, so we can use pandas to detect and remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8959e16e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Reopen the new CSV and drop duplicate items with the same ID\n",
    "\n",
    "df_newitems = pd.read_csv(newItemsReport)\n",
    "df_finalItems = df_newitems.drop_duplicates(subset=['ID'])\n",
    "df_finalItems.to_csv(newItemsReport, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07c00a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Troubleshooting (optional)\n",
    "\n",
    "The Hub sites are fairly unstable and it is likely that one or more of them will fail and interrupt the script. Check and see if the site is down, moved, etc. Make any updates to GEOMG directly. For tracking problems, the Status field in GEOMG is plain text and can be used for admin notes.\n",
    "\n",
    "- If a site is missing, Unpublish it from GEOMG and indicate the Date Retired, and make a note in the Status field.  \n",
    "- If a site just isn't working, Remove the value \"DCAT US 1.1\" from the Accrual Method field and make a note in the Status field.\n",
    "\n",
    "Edit the arcHubs.csv (or re-download it) and keep running this Notebook until it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911c45c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Upload to GEOMG\n",
    "\n",
    "### Review the previous upload\n",
    "\n",
    "1. Check the Date Accessioned field of the last harvest and copy it. \n",
    "\n",
    "\n",
    "### Upload everything that you just harvested.\n",
    "\n",
    "2. Upload the new CSV file. This will overwrite the Date Accessioned value for any items that were already present.\n",
    "\n",
    "### Delete items that were retired from ArcGIS Hubs\n",
    "3. Use the old Date Accessioned value to search for the previous harvest date. This example uses 2023-03-07: (https://geomg.lib.umn.edu/documents?f%5Bb1g_dct_accrualMethod_s%5D%5B%5D=ArcGIS+Hub&q=%222023-03-07%22&rows=20&sort=score+desc)\n",
    "4. Unpublished the ones that have the old date in the Date Accessioned field - record this number in the ticket under Number Deleted\n",
    "\n",
    "### Publish items that are new as of the latest harvest\n",
    "5. Look for records in the uploaded batch that are still \"Draft\" - these are new records. \n",
    "6. Publish them and record this number in the GitHub issue ticked under Number Added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
