{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566a209c",
   "metadata": {},
   "source": [
    "# PASDA Harvest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c043e",
   "metadata": {},
   "source": [
    "[Pennsylvania Spatial Data Access (PASDA](https://www.pasda.psu.edu) is the stateâ€™s comprehensive GIS clearinghouse. Most Pennsylvania statewide agencies and regional organizations provide their data through this site. Many counties and cities do as well.\n",
    "\n",
    "\n",
    "Part 1: Parse the PASDA portal\n",
    "1. Use the script `getURLs.py` to obtain a list of all of the records currently in PASDA. The resulting CSV will be called `URLS_{today's date}.csv` which is just a list of the landing pages for the datasets in the PASDA portal.\n",
    "\n",
    "\n",
    "2. Use the `pasdaURLS_{today's date}.csv` and the `html2csv.py`script to scrape the metadata from the PASDA landing pages. This resulting CSV will be called `output_{today's date}.csv`.\n",
    "\n",
    "\n",
    "Part 2: Extract the bounding boxes\n",
    "Context: most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'Metadata\" column.\n",
    "Although these files are created as XMLs, the link is a rendered HTML.\n",
    "\n",
    "1. Create a CSV file that is a list of just the metadata file pages. See sample-inputMetadataUrls.csv for an example.\n",
    "\n",
    "2. Use the getBbox.py script to parse the files and extract the bounding boxes.\n",
    "3. Merge these values back into the output CSV from Part 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3720d7",
   "metadata": {},
   "source": [
    "## Part 1: Get URLs\n",
    "\n",
    "Purpose: This script will crawl the PASDA website and return all of the published dataset landing pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5c447",
   "metadata": {},
   "source": [
    "### Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # The csv module implements classes to read and write tabular data in CSV format.\n",
    "import time # This module provides various time-related functions.\n",
    "import urllib.request # The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP)\n",
    "from bs4 import BeautifulSoup # For pulling data out of HTML and XML files\n",
    "import re\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddda94",
   "metadata": {},
   "source": [
    "### Search for the ID of all of the published items.\n",
    "\n",
    "PASDA's website layout does not have a page that lists all of the published datasets. Therefore, we will perform an  empty search using the keyword value '+' to find all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e09cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "resURL = 'https://www.pasda.psu.edu/uci/SearchResults.aspx?Keyword=+' # Setting URL\n",
    "page = urllib.request.urlopen(resURL).read() # Reading content of webpage\n",
    "soup = BeautifulSoup(page, 'html.parser') # Parse the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table contains all dataset links that start with 'a' html tag\n",
    "table = soup.find('table', id=\"DataGrid1\")    \n",
    "hrefs = table.findAll('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [] # Empty List\n",
    "for href in hrefs:\n",
    "    url = 'https://www.pasda.psu.edu/uci/' + href['href'] # Loop with concatenating base URLS and href\n",
    "    urls.append([url]) # Append URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6c203",
   "metadata": {},
   "source": [
    "### Write a CSV file that contains a list of the landing page URLs for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all dataset urls in csv file with actiondate\n",
    "actionDate = time.strftime('%Y%m%d') # Action Date\n",
    "with open(f'URLs_{actionDate}.csv', 'w') as fw: \n",
    "    writer = csv.writer(fw) # Write Rows \n",
    "    writer.writerows(urls) # URLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f8f61",
   "metadata": {},
   "source": [
    "## Part 2: Parse the discovery metadata on each page\n",
    "\n",
    "Next, this script will scan each page listed in the URLs_{actionDate}.csv file to look for the following information:\n",
    "\n",
    "- Title\n",
    "- Date\n",
    "- Publisher\n",
    "- Description\n",
    "- Metadata Link (a linked supplemental file - more details on this in Part 3)\n",
    "\n",
    "The script will then write a new CSV file in the GeoBTAA template format that contains all of this parsed information as well as default values for other fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a796a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '08a-01'  # Portal Hub Header\n",
    "accessRights = \"Public\" # Access Rights Header\n",
    "accrualMethod = \"HTML\" # Accrual Method Header\n",
    "dateAccessioned = time.strftime('%Y-%m-%d') # Data Accessioned Header\n",
    "language = \"eng\" # Language Header\n",
    "isPartOf = \"08a-01\" # Part of Portal Header\n",
    "memberOf = \"ba5cc745-21c5-4ae9-954b-72dd8db6815a\" # Member of Link Header\n",
    "provider = \"Pennsylvania Spatial Data Access (PASDA)\" # Provider Agency Header\n",
    "resourceClass = \"\" # Resource Class Header\n",
    "resourceType = '' # Resource Type Header\n",
    "dateRange = '' # Date Range Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0da7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract exising urls from local csv file\n",
    "\n",
    "urls = [] # Creates a container for URLs to be generated\n",
    "\n",
    "## Read the rows from the output CSV from datasetURL.py\n",
    "\n",
    "with open('URLs_20230218.csv') as fr: # Opens CSV file\n",
    "    reader = csv.reader(fr)  # Reader object assigned\n",
    "    for row in reader: # Scans each URL to be appended \n",
    "        urls.append(row) # Appended to each row\n",
    "\n",
    "\n",
    "# Store parsed elements for all urls\n",
    "\n",
    "parseElements = [] # Generates parsed elements for URLs to be stored (similar to above)\n",
    "\n",
    "\n",
    "# Find values for the rest\n",
    "# Loop through the list of URLs\n",
    "for url in urls:\n",
    "    # Open the URL and read the page content\n",
    "    page = urllib.request.urlopen(url[0]).read()\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    landingPage = str(url).strip(\"[']\")\n",
    "    iden = 'pasda-' + landingPage.rsplit(\"=\",1)[1]\n",
    "\n",
    "    # Find the values for the title, date, publisher, and description fields\n",
    "    title = soup.find(attrs={'id': 'Label1'}).text.strip()\n",
    "    date = soup.find(attrs={'id': 'Label2'}).text.strip()\n",
    "    publisher = soup.find(attrs={'id': 'Label3'}).text.strip()\n",
    "    description = soup.find(attrs={'id': 'Label14'}).text.strip()\n",
    "    \n",
    "    # Find the links for the metadata files and dataset downloads\n",
    "    \n",
    "    metadataLink = soup.find('a', href=True, text='Metadata') \n",
    "    downloadLink = soup.find('a', href=True, text='Download')\n",
    "    \n",
    "    metadata = \"https://www.pasda.psu.edu/uci/\" + metadataLink['href'] # Stripped if BeautifulSoup module fails or slips through\n",
    "    try:\n",
    "        download = downloadLink['href']\n",
    "    except:\n",
    "        download = ''\n",
    "            \n",
    "    parseElements.append([landingPage,iden,title,date,dateRange,publisher,provider,language,description,resourceClass,resourceType,metadata,download,code,isPartOf,memberOf,accessRights,accrualMethod,dateAccessioned])\n",
    "    \n",
    "    \n",
    "# Generate action date with format YYYYMMDD\n",
    "\n",
    "actionDate = time.strftime('%Y%m%d')\n",
    "\n",
    "# Write outputs to local CSV file\n",
    "\n",
    "with open(f'output_{actionDate}.csv', 'w') as fw: # Concatenates Fields\n",
    "    fields = ['Information','ID','Title','Temporal Coverage','Date Range','Publisher','Provider','Language','Description','Resource Class','Resource Type','HTML','Download','Code','Is Part Of','Member Of','Access Rights','Accrual Method','Date Accessioned']\n",
    "\n",
    "    writer = csv.writer(fw)           # Writes \n",
    "    writer.writerow(fields)           # Field Names\n",
    "    writer.writerows(parseElements)   # Elements\n",
    "\n",
    "    print('#### Job done ####')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1d1b4",
   "metadata": {},
   "source": [
    "## Part 3: Extract the bounding boxes\n",
    "\n",
    "Context: The bounding box coordinates are not part of the discovery metadata on each item's landing page. However, most of the records have supplemental metadata in ISO 19139 or FGDC format. The link to this document is found in the 'Metadata\" column. Although these files are created as XMLs, the link is a rendered HTML, so we can parse them using a similar method as shown in part 2.\n",
    "\n",
    "1. Create a CSV file that is a list of just the metadata file pages. See sample-inputMetadataUrls.csv for an example.\n",
    "2. Use the getBbox.py script to parse the files and extract the bounding boxes.\n",
    "3. Merge these values back into the output CSV from Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "portalMetadata = [] # The first three lines initialize an empty list called portalMetadata\n",
    "# and create a new CSV file named \"bbox-output.csv\" with headers 'url' and 'bbox' using the csv module.\n",
    "f = csv.writer(open('bbox-output.csv', 'w'))\n",
    "f.writerow(['url','bbox'])\n",
    "\n",
    "with open('metaLinks.csv','r') as harvest: # The with block opens the file and reads each as strings\n",
    "     urls = csv.reader(harvest)\n",
    "     for url in urls:\n",
    "        portalMetadata.append(url)\n",
    "\n",
    "for url in portalMetadata: # The for loop iterates over each url in portalMetadata\n",
    "      try:\n",
    "            page = urlopen(url[0]).read()\n",
    "            soup = BeautifulSoup(page, \"html.parser\")\n",
    "            pageLink =str(url)[1:-1].strip(\"\\'\")\n",
    "      \n",
    "            try: # The try-except blocks search the HTML of the web page for specific strings \n",
    "                  west = soup.find('i',string='West_Bounding_Coordinate:').next_sibling.strip()   \n",
    "            except:\n",
    "                  west = ''\n",
    "            \n",
    "            try:\n",
    "                  south = soup.find('i',string='South_Bounding_Coordinate:').next_sibling.strip()   \n",
    "            except:\n",
    "                  south = ''\n",
    "            \n",
    "            try:\n",
    "                  east = soup.find('i',string='East_Bounding_Coordinate:').next_sibling.strip()   \n",
    "            except:\n",
    "                  east = ''\n",
    "            \n",
    "            try:\n",
    "                  north = soup.find('i',string='North_Bounding_Coordinate:').next_sibling.strip()   \n",
    "            except:\n",
    "                  north = ''\n",
    "            \n",
    "            bbox = west + ',' + south + ',' +east+','+north\n",
    "      except:\n",
    "            bbox = \"missing\" # Omits errors if none found to missing\n",
    "       \n",
    "      f.writerow([pageLink,bbox])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
