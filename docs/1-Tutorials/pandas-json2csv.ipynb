{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ee7bd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transform a batch of JSON files into a single CSV file\n",
    "\n",
    "This tutorial uses the Python module [pandas (Python Data Analysis Library)](https://pandas.pydata.org) to open a batch of JSON files and transform the contents into a single CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1c2aa",
   "metadata": {},
   "source": [
    "## About pandas\n",
    "\n",
    "Pandas is a Python library that contains many functions for analyzing data. For the GeoBTAA workflows, we are most interested in how it eases transformations between JSON and CSV files:\n",
    "\n",
    "*CSV files*: Pandas can easily read and write CSV files using its `read_csv()` and `to_csv()` methods, respectively. These methods can handle many CSV formats, including different delimiter characters, header options, and data types. Once the CSV data is loaded into a Pandas DataFrame, it can be easily manipulated and analyzed using Pandas' powerful data manipulation tools, such as filtering, grouping, and aggregation.\n",
    "\n",
    "*JSON data*: Pandas can also read and write JSON data using its `read_json()` and `to_json()` methods. These methods can handle various JSON formats, such as normal JSON objects, JSON arrays, and JSON lines. Once the JSON data is loaded into a Pandas DataFrame, it can be easily manipulated and analyzed using the same data manipulation tools used for CSV data.\n",
    "\n",
    "*pandas DataFrame* A DataFrame is similar to a Python list or dictionary, but it has rows and columns, similar to a spreadsheet. This makes it a simpler task to convert between JSON and CSV. To review these Python terms, refer to the glossary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68baf22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Install pandas\n",
    "\n",
    "If you do not have pandas installed yet, choose ONE of the follow commands. (Uncomment one of these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00bf02c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ! conda install pandas --yes\n",
    "\n",
    "# OR\n",
    "\n",
    "# ! pip install pandas --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6daf23",
   "metadata": {},
   "source": [
    "## 2. Import modules\n",
    "\n",
    "Next, we will import a few Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5033f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "print(\"modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed2d9f",
   "metadata": {},
   "source": [
    "## 3. Declare the file paths and names\n",
    "\n",
    "Next, declare your file paths and names. For this tutorial, we are going to open 3 JSON files that are in the local folder called `sample-jsons`.\n",
    "\n",
    "Then, we enter the name `pandas-output` for the CSV file that will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ce0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = r\"sample-jsons\" # point to the folder path\n",
    "csv_name = \"pandas-output\" # name for the csv to be created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca8e52",
   "metadata": {},
   "source": [
    "## 4 .Create an empty list\n",
    "\n",
    "Before we run a Python loop, we need to create an empty list that will store the information. We give it a name of `jsonMetadata` and set it as equal to empty (`= []`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce667375",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonMetadata = [] # empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2433925d",
   "metadata": {},
   "source": [
    "## 5. Open the JSON files and add them to a Python List\n",
    "\n",
    "The code uses `os.walk`. This will open each JSON file, read the metadata,, and add it to a list called `jsonMetadata` (the one we created in the last step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, dir, files in os.walk(json_path):\n",
    "    for filename in files:\n",
    "    \tif filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            json_file_open = open(file_path, 'rb')\n",
    "            data = json_file_open.read().decode('utf-8', errors='ignore')\n",
    "            loaded = json.loads(data)\n",
    "            jsonMetadata.append(loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826fe81e",
   "metadata": {},
   "source": [
    "## 6. Convert the List into a pandas DataFrame\n",
    "\n",
    "Here is where **pandas** finally comes in. We convert the list (jsonMetadata) into a special object called a *pandas DataFrame*. Here, we use the convention of calling the DataFrame `df`. We will print out the DataFrame so you can see how it is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(jsonMetadata)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6cfc3",
   "metadata": {},
   "source": [
    "## 7. Drop one of the columns\n",
    "\n",
    "Now that all of the metadata from the JSON files is loaded into a pandas DataFrame, we can manipulate it in various ways. For example, let's say we do not want to include the column called `geoblacklight_version` in our final output. We can call the `.drop` method. When the DataFrame is printed out again, the first column is gone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e574575",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['geoblacklight_version'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19117c7b",
   "metadata": {},
   "source": [
    "## 8. Write the DataFrame to a CSV file\n",
    "\n",
    "We can perform other data conversions or analysis at this step as well, such as changing the column names, rearranging them, or other manipulations. For now, we will write the DataFrame to a CSV to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54462637",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"{}.csv\".format(csv_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570469f2",
   "metadata": {},
   "source": [
    "## 9. Inspect the new CSV file\n",
    "\n",
    "In practice, you will likely open a generated CSV file in a spreadsheet editor to prepare the metadata for publishing. However, let's take a look a it within this Notebook using the pandas `.read_csv` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv = pd.read_csv(\"pandas-output.csv\")\n",
    "new_csv.head(5) #displays the first 5 rows for us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a027c9",
   "metadata": {},
   "source": [
    "*For a more complex version of this script, see the Recipes section.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
